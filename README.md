![alttext](https://github.com/UrbsLab/STREAMLINE/blob/main/Pictures/STREAMLINE_LOGO.jpg?raw=true)
# Overview
STREAMLINE is an end-to-end automated machine learning (AutoML) pipeline that empowers anyone 
to easily run, interpret, and apply a rigorous and customizable analysis for data mining 
or predictive modeling. Notably, this tool is currently limited to supervised learning on 
tabular, binary classification data but will be expanded as our development continues. The 
development of this pipeline focused on 
1) overall automation
2) avoiding and detecting sources of bias 
3) optimizing modeling performance
4) ensuring complete reproducibility (under certain STREAMLINE parameter settings)
5) capturing complex associations in data (e.g. feature interactions)
6) enhancing interpretability of output. 

Overall, the goal of this pipeline is to provide a transparent framework to learn from data as well as identify the strengths and weaknesses of ML modeling algorithms or other AutoML algorithms.

A preprint introducing and applying STREAMLINE is now available https://arxiv.org/abs/2206.12002?fbclid=IwAR1toW5AtDJQcna0_9Sj73T9kJvuB-x-swnQETBGQ8lSwBB0z2N1TByEwlw.

See [below](#citation) for how to cite this preprint and/or the codebase prior to the availability of the final peer-reviewed publication.

***
## Quick Start
Click below for the easiest way for anyone to run an analysis using STREAMLINE (on Google Colaboratory):

[Setting Up Your First Run](#setting-up-your-first-run)

Click below for a quick look at an example pre-run notebook (with no installation steps), and/or example output files.

[View pipeline and output before running](#view-pipeline-and-output-before-running)

And click below for a summary of the figures automatically generated using STREAMLINE.

[Figures generated by STREAMLINE](#figures-generated-by-streamline)

***

## STREAMLINE Schematic

This schematic breaks the overall pipeline down into 4 basic components: (1) preprocessing and feature transformation, (2) feature importance evaluation and selection, (3) modeling, and (4) postprocessing.

![alttext](https://github.com/UrbsLab/STREAMLINE/blob/main/Pictures/ML_pipe_schematic.png?raw=true)

***
## Table of Contents
* [Overview](#overview)
    * [Quick start](#quick-start)
    * [STREAMLINE schematic](#streamline-schematic)    
    * [Table of contents](#table-of-contents)
    * [What level of computing skill is required for use?](#what-level-of-computing-skill-is-required-for-use)
    * [What can it be used for?](#what-can-it-be-used-for)
    * [What does STREAMLINE include?](#what-does-streamline-include)
    * [How is STREAMLINE different from other AutoML tools?](#how-is-streamline-different-from-other-automl-tools)
    * [STREAMLINE run modes](#streamline-run-modes)
    * [View pipeline and output before running](#view-pipeline-and-output-before-running)
    * [Implementation](#implementation)
    * [Disclaimer](#disclaimer)
* [Installation and Use](#installation-and-use)
    * [Use Mode 1: Google Colaboratory](#use-mode-1-google-colaboratory)
        * [Setting up your first run](#setting-up-your-first-run)
        * [Inspecting your first run](#inspecting-your-first-run)
        * [Running STREAMLINE on your own dataset(s)](#running-streamline-on-your-own-datasets)
        * [Tips for reducing STREAMLINE runtime](#tips-for-reducing-streamline-runtime)
        * [Tips for improving STREAMLINE modeling performance](#tips-for-improving-streamline-modeling-performance)
    * [Use Modes 2-4: Standard Installation and Use](#use-modes-2-4-standard-installation-and-use)
        * [Prerequisites](#prerequisites)
            * [Anaconda3](#anaconda3)
            * [Additional Python Packages](#additional-python-packages)
        * [Download STREAMLINE](#download-streamline)
        * [Code orientation](#code-orientation)
        * [Run from jupyter notebook](#run-from-jupyter-notebook)
            * [Running STREAMLINE jupyter notebook on your own dataset(s)](#running-streamline-jupyter-notebook-on-your-own-datasets)
        * [Run from command line (local or cluster parallelization)](#run-from-command-line-local-or-cluster-parallelization)
            * [Local run example](#local-run-example)
            * [Computing cluster run (parallelized) example](#computing-cluster-run-parallelized-example)
        * [Checking phase completion](#checking-phase-completion)
        * [Phase details (run parameters and additional examples)](#phase-details-run-parameters-and-additional-examples)
            * [Phase 1: Exploratory Analysis](#phase-1-exploratory-analysis)
                * [Example: Data with instances matched by one or more covariates](#example-data-with-instances-matched-by-one-or-more-covariates)
                * [Example: Ignore specified feature columns in data](#example-ignore-specified-feature-columns-in-data)
                * [Example: Specify features to treat as categorical](#example-specify-features-to-treat-as-categorical)
            * [Phase 2: Data Preprocessing](#phase-2-data-preprocessing)
            * [Phase 3: Feature Importance Evaluation](#phase-3-feature-importance-evaluation)
            * [Phase 4: Feature Selection](#phase-4-feature-selection)
            * [Phase 5: Machine Learning Modeling](#phase-5-machine-learning-modeling)
                * [Example: Run only one ML modeling algorithm](#example-run-only-one-ml-modeling-algorithm)
                * [Example: Utilize built-in algorithm feature importance estimates when available](#example-utilize-built-in-algorithm-feature-importance-estimates-when-available)
                * [Example: Specify an alternative primary evaluation metric](#example-specify-an-alternative-primary-evaluation-metric)
                * [Example: Reduce computational burden of algorithms that run slow in large instance spaces](#example-reduce-computational-burden-of-algorithms-that-run-slow-in-large-instance-spaces)
            * [Phase 6: Statistics Summary](#phase-6-statistics-summary)
            * [Phase 7: Compare Datasets](#phase-7-optional-compare-datasets)
            * [Phase 8: Generate PDF Training Summary Report](#phase-8-optional-generate-pdf-training-summary-report)
            * [Phase 9: Apply Models to Replication Data](#phase-9-optional-apply-models-to-replication-data)
            * [Phase 10: Generate PDF 'Apply Replication' Summary Report](#phase-10-optional-generate-pdf-apply-replication-summary-report)
            * [Phase 11: File Cleanup](#phase-11-optional-file-cleanup)
* [Other guidelines for STREAMLINE use](#other-guidelines-for-streamline-use)
* [Unique characteristics of STREAMLINE](#unique-characteristics-of-streamline)
* [Do even more with STREAMLINE](#do-even-more-with-streamline)      
* [Demonstration data](#demonstration-data)  
* [Troubleshooting](#troubleshooting)
    * [STREAMLINE fails to run to completion](#streamline-fails-to-run-to-completion)
    * [Rerunning a failed modeling job](#rerunning-a-failed-modeling-job)
    * [Unending modeling jobs](#unending-modeling-jobs)
* [Figures generated by STREAMLINE](#figures-generated-by-streamline)
* [Development notes](#development-notes)
    * [History](#history)
    * [Planned extensions/improvements](#planned-extensionsimprovements)
        * [Known issues](#known-issues)
        * [Logistical extensions](#logistical-extensions)
        * [Capabilities extensions](#capabilities-extensions)
        * [Algorithmic extensions](#algorithmic-extensions)
* [Acknowledgements](#acknowledgements)
* [Citation](#citation)
    * [URBS-lab related research](#urbs-lab-related-research)
        * [A rigorous ML pipeline for binary classification](#a-rigorous-ml-pipeline-for-binary-classification)
        * [Relief-based feature importance estimation](#relief-based-feature-importance-estimation)
        * [Collective feature selection](#collective-feature-selection)
        * [Learning classifier systems](#learning-classifier-systems)

***
## What level of computing skill is required for use?
STREAMLINE offers a variety of use options making it accessible to those with little or no coding experience as well as 
the seasoned programmer/data scientist. While there is currently no graphical user interface (GUI), 
the most naive user need only know how to navigate their PC file system, specify folder/file paths, un-zip a folder, 
and have or set up a Google Drive account. 
Those with a very basic knowledge of python and computer environments can apply 
STREAMLINE on Google Colab within the included jupyter notebook, and those 
with a bit more experience can run it serially by command line or in parallel 
(as is) on a compatible Linux computing clusters.

***
## What can it be used for?
STREAMLINE can be used as:
1. A tool to quickly run a rigorous ML data analysis over one or more datasets using one or more of the well-known or in-development modeling algorithms included
2. A framework to compare established scikit-learn compatible ML modeling algorithms to each other or to new algorithms
3. A baseline standard of comparison (i.e. positive control) with which to evaluate other AutoML tools that seek to optimize ML pipeline assembly as part of their methodology
4. A framework to quickly run an exploratory analysis and/or feature importance estimation/feature selection prior to using some other methodology for ML modeling
5. An educational example of how to integrate some of the many amazing Python-based data science tools currently available (in particular pandas, scipy, optuna, and scikit-learn).
6. A framework from which to create a new, expanded, adapted, or modified ML analysis pipeline
7. A framework to test new algorithms and identify less obvious bugs (i.e. those that don't prevent the algorithm from running to completion)

***
## What does STREAMLINE include?
The automated elements of STREAMLINE includes 
1) exploratory analysis
2) basic data cleaning
3) cross validation (CV) partitioning, scaling, imputation
4) filter-based feature importance estimation
5) collective feature selection
6) modeling with 'Optuna' hyperparameter optimization across 15 implemented ML algorithms 
7) testing evaluations with 15 classification metrics, and model feature importance estimation
8) automatic saving of all results, models, and publication-ready plots (including proposed composite feature importance plots)
9) non-parametric statistical comparisons across ML algorithms and analyzed datasets
10) automatically generated PDF summary reports.

The following 15 scikit-learn compatible ML modeling algorithms are currently included as options: 
Naive Bayes (NB), Logistic Regression (LR), Decision Tree (DT), Random Forest (RF), 
Gradient Boosting (GB), XGBoost (XGB), LGBoost (LGB), CatBoost (CGB), 
Support Vector Machine (SVM), Artificial Neural Network (ANN), 
K-Nearest Neighbors (k-NN), Genetic Programming (GP), 
Eductional Learning Classifier System (eLCS), 
'X' Classifier System (XCS), and 
Extended Supervised Tracking and Classifying System (ExSTraCS). 

Classification-relevant hyperparameter values and ranges have carefully 
selected for each and are pre-specified for the automated (Optuna-driven) 
automated hyperparameter sweep.

The automatically formatted PDF reports generated by STREAMLINE are intended 
to give a brief summary of pipeline settings and key results. 
A folder containing all results, statistical analyses publication-ready plots/figures, 
models, and other outputs is saved allowing users to carefully examine all aspects of 
analysis performance.  We have also included a variety of useful Jupyter Notebooks 
designed to operate on this output folder giving users quick paths to do even more 
with the pipeline output. 

Examples include: (1) Accessing prediction probabilities, 
(2) regenerating figures with custom tweaks, 
(3) trying out the effect of different prediction thresholds on selected 
models with an interactive slider, 
(4) re-evaluating models using a new prediction threshold, and 
(5) generating an interactive model feature importance ranking visualization across 
all ML algorithms. 
STREAMLINE is completely reproducible when the `timeout` parameter is set to `None`, 
ensuring training of the same models with the same performance whenever the same datasets, 
pipeline settings, and random seed are used. 
STREAMLINE output can vary somewhat (particularly when parallelized) 
since Optuna (for hyperparameter optimization) may not complete the same 
number of optimization trials within the user specified time limit on different 
computing resources. However, Optuna helps ensure STREAMLINE run completion 
within a reasonable time frame. STREAMLINE also outputs all CV training/testing 
datasets generated, along with relevant scaling and imputation objects so that 
users can easily run their own comparable ML analyses outside of STREAMLINE.

This pipeline does NOT automate the following elements, as they are still best 
completed by human experts: (1) feature engineering, or feature construction, 
(2) feature encoding (e.g. apply one-hot-encoding to categorical features, or 
numerically encode text-based feature values), (3) account for bias in data 
collection, or (4) anything beyond simple data cleaning (i.e. the pipeline 
only removes instances with no class label, or where all feature values are missing). 
We recommend users consider conducting these items, as needed, prior to applying STREAMLINE.

***
## How is STREAMLINE different from other AutoML tools?
Unlike most other AutoML tools, STREAMLINE was designed as a framework to rigorously apply 
and compare a variety of ML modeling algorithms and collectively learn from them as opposed 
to identifying a best performing model and/or attempting to optimize the analysis pipeline 
configuration itself. STREAMLINE adopts a fixed series of purposefully selected steps/phases 
in line with data science best practices. It seeks to automate all domain generalizable 
elements of an ML analysis pipeline with a specific focus on biomedical data mining challenges. 
This tool can be run or utilized in a number of ways to suite a variety experience levels and
levels of problem/data complexity.

***
## STREAMLINE run modes
This multixphase pipeline has been set up to run in one of four ways:

1. A 'Notebook' within Google Colaboratory [Almost Anyone]:
    * Advantages: (1) No coding or PC environment experience needed, (2) computing can be performed directly on Google Cloud, (3) one-click run of whole pipeline
    * Disadvantages: (1) Can only run pipeline serially, (2) slowest of the run options, (3) limited by google cloud computing allowances
    * Notes: Requires a Google and Google Drive account (free)

2. A Jupyter Notebook (included) [Basic Experience]:
    * Advantages: (1) Allows easy customizability of nearly all aspects of the pipeline with minimal coding/environment experience, (2) offers in-notebook viewing of results, (3) offers in-notebook documentation of the run phases, (4) one-click run of whole pipeline
    * Disadvantages: (1) Can only run pipeline serially, (2) slower runtime than from command-line
    * Notes: Requires Anaconda3, Python3, and several other minor Python package installations

3. Locally from the command line [Command-line Users]:
    * Advantages: (1) Typically runs faster than within Jupyter Notebook, (2) an easier more versatile option for those with command-line experience
    * Disadvantages: (1) Can only run pipeline serially, (2) command-line experience recommended
    * Notes: Requires Anaconda3, Python3, and several other minor Python package installations

4. Run in parallel from the command line using a computing cluster (only Linux-based cluster currently tested) [Computing Cluster Users]:
    * Advantages: (1) By far the fastest, most efficient way to run STREAMLINE, (2) offers parallelization within pipeline phases over separate datasets, cross-validation partitions, and ML algorithms.
    * Disadvantages: (1) Experience with command-line recommended (2) access to a computing cluster required
    * Notes: Requires Anaconda3, Python3, and several other minor Python package installations. Parallelization occurs within phases. Individual phases must be run in sequence.

Parallelized runs of STREAMLINE were set up using ____
***
## View pipeline and output before running
* To quickly pre-view the pipeline (pre-run on included [demonstration datasets](#demonstration-data) without any installation whatsoever, open the following link:

https://github.com/raptor419/STREAMLINE_Dev/blob/main/STREAMLINE-Notebook.ipynb 

Note, that with this link, you can only view the pre-run STREAMLINE Jupyter Notebook and will not be able to run or permanently edit the code. This is an easy way to get a feel for what the pipeline is and does.

* To quickly pre-view the folder of output files generated when running STREAMLINE on the [demonstration datasets](#demonstration-data), open the following link:

https://drive.google.com/drive/folders/1dgaXnJnzdthTxP914ALdrB4IBHjJdm1a?usp=sharing

***
## Implementation
STREAMLINE is coded in Python 3 relying heavily on pandas and scikit-learn as well as a variety of other python packages.

***
## Disclaimer
We make no claim that this is the best or only viable way to assemble an ML analysis pipeline for a given classification problem, nor that the included ML modeling algorithms will yield the best performance possible. We intend many expansions/improvements to this pipeline in the future to make it easier to use and hopefully more effective in application.  We welcome feedback, suggestions, and contributions for improvement.

***
## Installation and Use

*In near future*

```
pip install streamline
```

***

***
# Other Guidelines for STREAMLINE Use
* SVM and ANN modeling should only be applied when data scaling is applied by the pipeline.
* Logistic Regression' baseline model feature importance estimation is determined by the exponential of the feature's coefficient. This should only be used if data scaling is applied by the pipeline.  Otherwise `use_uniform_FI` should be True.
* While the STREAMLINE includes `impute_data` as an option that can be turned off in `DataPreprocessing`, most algorithm implementations (all those standard in scikit-learn) cannot handle missing data values with the exception of eLCS, XCS, and ExSTraCS. In general, STREAMLINE is expected to fail with an errors if run on data with missing values, while `impute_data` is set to 'False'.

***
# Unique Characteristics of STREAMLINE
* Pipeline includes reliable default run parameters that can be adjusted for further customization.
* Easily compare ML performance between multiple target datasets (e.g. with different feature subsets)
* Easily conduct an exploratory analysis including: (1) basic dataset characteristics: data dimensions, feature stats, missing value counts, and class balance, (2) detection of categorical vs. quantiative features, (3) feature correlation (with heatmap), and (4) univariate analyses with Chi-Square (categorical features), or Mann-Whitney U-Test (quantitative features).
* Option to manually specify which features to treat as categorical vs. quantitative.
* Option to manually specify features in loaded dataset to ignore in analysis.
* Option to utilize 'matched' cross validation partitioning: Case/control pairs or groups that have been matched based on one or more covariates will be kept together within CV data partitions.
* Imputation is completed using mode imputation for categorical variables first, followed by MICE-based iterative imputation for quantitaive features. There is an option to use mean imputation for quantitative features when imputation computing cost is prohibitive in large datasets.
* Data scaling, imputation, and feature selection are all conducted within respective CV partitions to prevent data leakage (i.e. testing data is not seen for any aspect of learning until final model evaluation).
* The scaling, imputation, and feature selection data transformations (based only on the training data) are saved (i.e. 'pickled') so that they can be applied in the same way to testing partitions, and in the future to any replication data.
* Collective feature selection is used: Both mutual information (proficient at detecting univariate associations) and MultiSURF (a Relief-based algorithm proficient at detecting both univariate and epistatic interactions) are run, and features are only removed from consideration if both algorithms fail to detect an informative signal (i.e. score > 0). This ensures that interacting features that may have no univariate association with class are not removed from the data prior to modeling.
* Automatically outputs average feature importance bar-plots from feature importance/feature selection phase.
* Since MultiSURF scales linearly with # of features and quadratically with # of instances, there is an option to select a random instance subset for MultiSURF scoring to reduce computational burden.
* Includes 3 rule-based machine learning algorithms: ExSTraCS, XCS, and eLCS (to run optionally). These 'learning classifier systems' have been demonstrated to be able to detect complex associations while providing human interpretable models in the form of IF:THEN rule-sets. The ExSTraCS algorithm was developed by our research group to specifically handle the challenges of scalability, noise, and detection of epistasis and genetic heterogeneity in biomedical data mining.  
* Utilizes the 'optuna' package to conduct automated Bayesian hyperparameter optimization during modeling (and optionally outputs plots summarizing the sweep).
* We have sought to specify a comprehensive range of relevant hyperparameter options for all included ML algorithms.
* Some ML algorithms that have a build in strategy to gather model feature importance estimates (i.e. LR,DT,RF,XGB,LGB,GB,eLCS,XCS,ExSTraCS) These can be used in place of permutation feature importance estimates by setting the parameter `use_uniform_FI` to 'False'.
* All other algorithms rely on estimating feature importance using permutation feature importance.
* All models are evaluated, reporting 16 classification metrics: Accuracy, Balanced Accuracy, F1 Score, Sensitivity(Recall), Specificity, Precision (PPV), True Positives (TP), True Negatives (TN), False Positives (FP), False Negatives (FN), Negative Predictive Value (NPV), Likeliehood Ratio + (LR+), Likeliehood Ratio - (LR-), ROC AUC, PRC AUC, and PRC APS.
* All models are saved as 'pickle' files so that they can be loaded and reapplied in the future.
* Outputs ROC and PRC plots for each ML modeling algorithm displaying individual n-fold CV runs and average the average curve.
* Outputs boxplots for each classification metric comparing ML modeling performance (across n-fold CV).
* Outputs boxplots of feature importance estimation for each ML modeling algorithm (across n-fold CV).
* Outputs our proposed 'composite feature importance plots' to examine feature importance estimate consistency (or lack of consistency) across all ML models (i.e. all algorithms)
* Outputs summary ROC and PRC plots comparing average curves across all ML algorithms.
* Collects run-time information on each phase of the pipeline and for the training of each ML algorithm model.
* For each dataset, Kruskall-Wallis and subsequent pairwise Mann-Whitney U-Tests evaluates statistical significance of ML algorithm modeling performance differences for all metrics.
* The same statistical tests (Kruskall-Wallis and Mann-Whitney U-Test) are conducted comparing datasets using the best performing modeling algorithm (for a given metric and dataset).
* Outputs boxplots comparing performance of multiple datasets analyzed either across CV runs of a single algorithm and metric, or across average for each algorithm for a single metric.
* A formatted PDF report is automatically generated giving a snapshot of all key pipeline results.
* A script is included to apply all trained (and 'pickled') models to an external replication dataset to further evaluate model generalizability. This script (1) conducts an exploratory analysis of the new dataset, (2) uses the same scaling, imputation, and feature subsets determined from n-fold cv training, yielding 'n' versions of the replication dataset to be applied to the respective models, (3) applies and evaluates all models with these respective versions of the replication data, (4) outputs the same set of aforementioned boxplots, ROC, and PRC plots, and (5) automatically generates a new, formatted PDF report summarizing these applied results.

***

# Demonstration data
Included with this pipeline is a folder named `DemoData` including two small datasets used as a demonstration of pipeline efficacy. New users can easily run the included jupyter notebook 'as-is', and it will be run automatically on these datasets. The first dataset `hcc-data_example.csv` is the Hepatocellular Carcinoma (HCC) dataset taken from the UCI Machine Learning repository. It includes 165 instances, 49 fetaures, and a binary class label. It also includes a mix of categorical and numeric features, about 10% missing values, and class imbalance, i.e. 63 deceased (class = 1), and 102 surived (class 0).  To illustrate how STREAMLINE can be applied to more than one dataset at once, we created a second dataset from this HCC dataset called `hcc-data_example_no_covariates.csv`, which is the same as the first but we have removed two covariates, i.e. `Age at Diagnosis`, and `Gender`.

Furthermore, to demonstrate how STREAMLINE-trained models may be applied to new data in the future through the phase 9 `ApplyModel.py` we have simply added a copy of `hcc-data_example.csv`, renamed as `hcc-data_example_rep.csv` to the folder `DemoRepData`. While this is not a true replication dataset (as none was available for this example) it does illustrate the functionality of `ApplyModel`. Since the cross validation (CV)-trained models are being applied to all of the original target data, the `ApplyModel.py` results in this demonstration are predictably overfit.  When applying trained models to a true replication dataset model prediction performance is generally expected to be as good or less well performing than the individual testing evaluations completed for each CV model.

***
# Troubleshooting

## STREAMLINE fails to run to completion

***
# Figures generated by STREAMLINE
![alttext](https://github.com/UrbsLab/STREAMLINE/blob/main/Pictures/STREAMLINE_Figures.png?raw=true)

***
# Development notes
Have ideas on how to improve this pipeline? We welcome suggestions, contributions, and collaborations.

## History
STREAMLINE is based on our initial development repository https://github.com/UrbsLab/STREAMLINE. STREAMLINE's codebase and functionalities have been reorganized and extended, along with the name rebranding. This STREAMLINE repository will be developed further in the future while AutoMLPipe-BC will remain as is.

## Planned extensions/improvements

### Known issues
* Repair probable bugs in eLCS and XCS ML modeling algorithms (outside of STREAMLINE). Currently, we have intentionally set both to 'False' by default, so they will not run unless user explicitly turns them on)
* Set up STREAMLINE to be able to run (as an option) through all phases even if some CV model training runs have failed (as an option)
* Optuna currently prevents a guarantee of reproducibility of STREAMLINE when run in parallel, unless the user specifies `None` for the `timeout` parameter. This is explained in the Optuna documentation as an inherent result of running Optuna in parallel, since it is possible for a different optimal configuration to be found if a greater number of optimization trials are completed from one run to the next. We will consider alternative strategies for running STREAMLINE hyperparameter optimization as options in the future.
* Optuna generated visualization of hyperparameter sweep results fails to operate correctly under certain situations (i.e. for GP most often, and for LR when using a version of Optuna other than 2.0.0)  It looks like Optuna developers intend to fix these issues in the future, and we will update STREAMLINE accordingly when they do.

### Logistical extensions
* Simplify documentation using github.io :heavy_check_mark:
* Improved modularization of code for adding new ML modeling algorithms :heavy_check_mark: - Planned as a pypi package
* Set up code to be run easily on cloud computing options such as AWS, Azure, or Google Cloud
* Set up option to use STREAMLINE within Docker - In Progress/TODO
* Set up STREAMLINE parallelization to be able to automatically run with one command rather than require phases to be run in sequence (subsequent phases only being run when the prior one completes) :heavy_check_mark: - In Progress

### Capabilities extensions
* Support multiclass and quantitative endpoints
    * Will require significant extensions to most phases of the pipeline including exploratory analysis, CV partitioning, feature importance/selection, modeling, statistics analysis, and visualizations
* Shapley value calculation and visualizations
* Create ensemble model from all trained models which can then be evaluated on hold out replication data
* Expand available model visualization opportunities for model interpretation (i.e. Logistic Regression)
* Improve Catboost integration:
    * Allow it to use internal feature importance estimates as an option
    * Give it the list of features to be treated as categorical
* New code providing even more post-run data visualizations and customizations
* Clearly identify which algorithms can be run with missing values present, when user does not wish to apply `impute_data` (not yet fully tested)
* Create a smarter approach to hyperparameter optimization: (1) avoid hyperparameter combinations that are invalid (i.e. as seen when using Logistic Regression), (2) intelligently exclude key hyperparameters known to improve overall performance as they get larger, and apply a user defined value for these in the final model training after all other hyperparameters have been optimized (i.e. evolutionary algorithms such as genetic programming and ExSTraCS almost always benefit from larger population sizes and learning cycles. Given that we know these parameters improve performance, including them in hyperparameter optimization only slows down the process with little informational gain)

### Algorithmic extensions
* Addition of other ML modeling algorithm options
* Refinement of pre-configured ML algorithm hyperparameter options considered using Optuna
* Expanded feature importance estimation algorithm options and improved, more flexible feature selection strategy improving high-order feature interaction detection
* New rule-based machine learning algorithm (in development)

***
# Acknowledgements
STREAMLINE is the result of 3 years of on-and-off development gaining feedback from multiple biomedical research collaborators at the University of Pennsylvania, 
Fox Chase Cancer Center, Cedars Sinai Medical Center, and the University of Kansas Medical Center. 
The bulk of the coding was completed by Ryan Urbanowicz, Robert Zhang and Harsh Bandhey. Special thanks to 
Yuhan Cui, Pranshu Suri, Patryk Orzechowski, Trang Le, Sy Hwang, Richard Zhang, Wilson Zhang, 
and Pedro Ribeiro for their code contributions and feedback.  

We also thank the following collaborators for their feedback on application 
of the pipeline during development: Shannon Lynch, Rachael Stolzenberg-Solomon, 
Ulysses Magalang, Allan Pack, Brendan Keenan, Danielle Mowery, Jason Moore, and Diego Mazzotti.

***
# Citations
To cite the STREAMLINE preprint on arXiv, please use:
```
@article{urbanowicz2022streamline,
  title={STREAMLINE: A Simple, Transparent, End-To-End Automated Machine Learning Pipeline Facilitating Data Analysis and Algorithm Comparison},
  author={Urbanowicz, Ryan J and Zhang, Robert and Cui, Yuhan and Suri, Pranshu},
  journal={arXiv preprint arXiv:2206.12002v1},
  year={2022}
}
```
If you wish to cite the STREAMLINE codebase, please use:
```
@misc{streamline2022,
  author = {Urbanowicz, Ryan and Zhang, Robert},
  title = {STREAMLINE: A Simple, Transparent, End-To-End Automated Machine Learning Pipeline},
  year = {2022},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/UrbsLab/STREAMLINE/} }
}
```
## URBS-lab related research
In developing STREAMLINE we integrated a number of methods and lessons learned from our lab's previous research. We briefly summarize and provide citations for each.

### A rigorous ML pipeline for binary classification
A preprint describing an early version of what would become STREAMLINE applied to pancreatic cancer.
```
@article{urbanowicz2020rigorous,
  title={A Rigorous Machine Learning Analysis Pipeline for Biomedical Binary Classification: Application in Pancreatic Cancer Nested Case-control Studies with Implications for Bias Assessments},
  author={Urbanowicz, Ryan J and Suri, Pranshu and Cui, Yuhan and Moore, Jason H and Ruth, Karen and Stolzenberg-Solomon, Rachael and Lynch, Shannon M},
  journal={arXiv preprint arXiv:2008.12829v2},
  year={2020}
}
```

### Relief-based feature importance estimation
One of the two feature importance algorithms used by STREAMLINE is MultiSURF, a Relief-based filter feature importance algorithm that can prioritize features involved in either univariate or multivariate feature interactions associated with outcome. We believe that it is important to have at least one 'interaction-sensitive' feature importance algorithm involved in feature selection prior such that relevant features involved in complex associations are not filtered out prior to modeling. The paper below is an introduction and review of Relief-based algorithms.  
```
@article{urbanowicz2018relief,
  title={Relief-based feature selection: Introduction and review},
  author={Urbanowicz, Ryan J and Meeker, Melissa and La Cava, William and Olson, Randal S and Moore, Jason H},
  journal={Journal of biomedical informatics},
  volume={85},
  pages={189--203},
  year={2018},
  publisher={Elsevier}
}
```
This next published research paper compared a number of Relief-based algorithms and demonstrated best overall performance with MultiSURF out of all evaluated. This second paper also introduced 'ReBATE', a scikit-learn package of Releif-based feature importance/selection algorithms (used by STREAMLINE).
```
@article{urbanowicz2018benchmarking,
  title={Benchmarking relief-based feature selection methods for bioinformatics data mining},
  author={Urbanowicz, Ryan J and Olson, Randal S and Schmitt, Peter and Meeker, Melissa and Moore, Jason H},
  journal={Journal of biomedical informatics},
  volume={85},
  pages={168--188},
  year={2018},
  publisher={Elsevier}
}
```

### Collective feature selection
Following feature importance estimation, STREAMLINE adopts an ensemble approach to determining which features to select. The utility of this kind of 'collective' feature selection, was introduced in the next publication.
```
@article{verma2018collective,
  title={Collective feature selection to identify crucial epistatic variants},
  author={Verma, Shefali S and Lucas, Anastasia and Zhang, Xinyuan and Veturi, Yogasudha and Dudek, Scott and Li, Binglan and Li, Ruowang and Urbanowicz, Ryan and Moore, Jason H and Kim, Dokyoon and others},
  journal={BioData mining},
  volume={11},
  number={1},
  pages={1--22},
  year={2018},
  publisher={Springer}
}
```

### Learning classifier systems
STREAMLINE currently incorporates 15 ML classification modeling algorithms that can be run. Our own research has closely followed a subfield of evolutionary algorithms that discover a set of rules that collectively constitute a trained model. The appeal of such 'rule-based machine learning algorithms' (e.g. learning classifier systems) is that they can model complex associations while also offering human interpretable models. In the first paper below we introduced 'ExSTraCS', a learning classifier system geared towards bioinformatics data analysis. ExSTraCS was the first ML algorithm demonstrated to be able to tackle the long-standing 135-bit multiplexer problem directly, largely due to it's ability to use prior feature importance estimates from a Relief algorithm to guide the evolutionary rule search.
```
@article{urbanowicz2015exstracs,
  title={ExSTraCS 2.0: description and evaluation of a scalable learning classifier system},
  author={Urbanowicz, Ryan J and Moore, Jason H},
  journal={Evolutionary intelligence},
  volume={8},
  number={2},
  pages={89--116},
  year={2015},
  publisher={Springer}
}
```
In the next published pre-print we introduced a scikit-learn implementation of ExSTraCS (used by STREAMLINE) as well as a pipeline (LCS-DIVE) to take ExSTraCS output and characterize different patterns association between features and outcome. Future work will demonstrate how STREAMLINE can be linked with LCS-DIVE to better understand the relationship between features and outcome captured by rule-based modeling.
```
@article{zhang2021lcs,
  title={LCS-DIVE: An Automated Rule-based Machine Learning Visualization Pipeline for Characterizing Complex Associations in Classification},
  author={Zhang, Robert and Stolzenberg-Solomon, Rachael and Lynch, Shannon M and Urbanowicz, Ryan J},
  journal={arXiv preprint arXiv:2104.12844},
  year={2021}
}
```
In the next publication we introduced the first scikit-learn compatible implementation of an LCS algorithm. Specifically this paper implemented eLCS, an educational learning classifier system. This eLCS algorithm is a direct descendant of the UCS algorithm.
```
@inproceedings{zhang2020scikit,
  title={A scikit-learn compatible learning classifier system},
  author={Zhang, Robert F and Urbanowicz, Ryan J},
  booktitle={Proceedings of the 2020 Genetic and Evolutionary Computation Conference Companion},
  pages={1816--1823},
  year={2020}
}
```
eLCS was originally developed as a very simple supervised learning LCS implementation primarily as an educational resource pairing with the following published textbook.
```
@book{urbanowicz2017introduction,
  title={Introduction to learning classifier systems},
  author={Urbanowicz, Ryan J and Browne, Will N},
  year={2017},
  publisher={Springer}
}
```