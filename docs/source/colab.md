# Running on Google Colab

In this section we first provide [instructions for users with little to no coding experience](#setting-up-your-first-run). 
Users with some coding experience can jump to other sections 
like how to run it [locally](local.md), on [jupyter notebook](jupyter.md) or on
an [HPC Cluster](cluster.md). 

However, all users may benefit from reviewing the [tips](tips.md) section.

As STREAMLINE is currently in its 'beta' release, we recommend users first check that they have downloaded the 
most recent release of STREAMLINE before applying any of the run modes described below, 
as we are actively updating this software as feedback is received.

This is the easiest but most limited way to run STREAMLINE. These instructions are geared towards those with little 
to no computing experience. All other users can skip to the next more advanced sections 
but may wish to revisit later parts of this section for helpful details.

* To learn more about Google Colaboratory prior to setup please visit the following link: 
https://research.google.com/colaboratory/

## Running on Demo Dataset
Follow the steps below to get STREAMLINE running on the [demonstration datasets](sample.md#demonstration-data). 
In summary, they detail the process of opening the STREAMLINE Colab Notebook to your Google Drive, 
and running the notebook called `STREAMLINE-GoogleColab.ipynb` with Google Colaboratory (the link is provided below), running it 
and downloading the output files.

1. Set up a Google account (if for some reason you don't already have one).
    * Click here for help: https://support.google.com/accounts/answer/27441?hl=en

2. Open the following Google Colab Notebook using this link: 
[https://colab.research.google.com/drive/17s55GajtN5WCEV-DfegtiFhvGp73Haj4?usp=sharing](https://colab.research.google.com/drive/17s55GajtN5WCEV-DfegtiFhvGp73Haj4?usp=sharing)

3. [Optional] At the top of the notebook open the `Runtime` menu and select `Disconnect and delete runtime`. This clears the memory of the previous notebook run. This is only necessary when the underlying base code is modified, but it may be useful to troubleshoot if modifications to the notebook do not seem to have an effect.

4. At the top of the notebook open the `Runtime` menu and select `Run all`.  This directs the notebook to run all code cells of the notebook, i.e. all phases of STREAMLINE.  Here we have preconfigured STREAMLINE to automatically run on two [demonstration datasets](sample.md#demonstration-data) found in the `DemoData` folder.

5. Note: At this point the notebook will do the following automatically:
   1. Reserve a limited amount of free memory (RAM) and disk space on Google Cloud.
       * Note: it is also possible to set up this Notebook to run using the resources of your local PC (not covered here).
   2. Load the individual STREAMLINE run files into memory from the STREAMLINE Github.
   3. Install all other necessary python packages on the Google Colaboratory Environment.
   4. Run the entirety of STREAMLINE on the [demonstration datasets](sample.md#demonstration-data) folder (i.e. `DemoData`).
       * Note: all 5 steps should take approximately 3-5 minutes to run.
   5. Download the Main Report and the replication report automatically.
   6. If you have the last cell uncommented (by default) it will also download the complete experiment folder as zip file on your local computer.

## Inspecting your first run
During or after the notebook runs, users can inspect the individual code and text (i.e. markdown) ce
lls of the notebook. Individual cells can be collapsed or expanded by clicking on the small arrowhead 
on the left side of each cell. The first set of cells set up the coding environment automatically. 
Later cells are used to set the pipeline run parameters and then run the 11 phases of the pipeline in sequence. 

Some cells will display output figures generated by STREAMLINE. 
For example, scroll down to 'Phase 6: Statistics' and open the cell below the text 'Run Statistics 
Summary and Figure Generation'. Scrolling down this cell will first reveal the run commands calling 
relevant STREAMLINE code, then the figures generated by this phase. Note that to save runtime, 
this demonstration run is only applying three ML modeling algorithms: Naive Bayes, Logistic Regression, 
and Decision Trees.  These are typically the three fastest algorithms available in STREAMLINE.

The Colab notebook automatically downloads the General and Replication Report, 
in addition to the complete experiment folder.

The user open the zip on their local system, or alternatively you can explore the files on Google Colab 
by opening the file-explorer pane on the left.

How to analyze this output can be found in the [Analyzing Outputs](analysis.md) section.

## Running on your own datasets
This section explains how to update the Google Colaboratory Notebook to run on one or more user specified 
datasets rather than the [demonstration datasets](sample.md#demonstration-data). This instructions are 
effectively the same for running STREAMLINE from Jupyter Notebook. Note that, for brevity, 
the parameter names given below are slightly different from the argument identifiers when using STREAMLINE 
from the command-line (a guide for commandline parameters is given [here](parameters.md)).

1. Open the same notebook as the above section for DemoData
2. Uncomment cell of code which says "Uncomment to load custom dataset"
3. Upload files as and input `class_label`, `instance_label` and `match_label` asked with these requirements:
    * Files are in comma-separated format with extension '.txt' or '.csv' format.
    * Missing data values should be empty or indicated with an 'NA'.
    * Dataset(s) include a header giving column labels.
    * Data columns should only include features (i.e. independant variables), a class label, and [optionally] instance (i.e. row) labels, and/or match labels (if matched cross validation will be used).
    * Binary class values are encoded as 0 (e.g. negative), and 1 (positive) with respect to true positive, true negative, false positive, false negative metrics. PRC plots focus on classification of 'positives'.
    * All feature values (both categorical and quantitative) are numerically encoded (i.e. no letters or words). Scikit-learn does not accept text-based values.
        * However, both `instance_label` and `match_label` values may be either numeric or text.
    * If multiple datasets are being analyzed they must each have the same `class_label` (e.g. 'Class'), and (if present), the same `instance_label` (e.g. 'ID') and `match_label` (e.g. 'Match_ID').
4. Update the first 6 pipeline run parameters as such:
    * `demo_run`: Change from True to False (Note, this parameter is only used by the notebooks for the demonstration analysis, and is one of the few parameters that use a Boolean rather than string value).
    * `data_path`: Change the end of the path from DemoData to the name of your new dataset folder (e.g. "/content/drive/MyDrive/STREAMLINE-main/my_data").
    * `output_path`: This can be left 'as-is' or modified to some other folder on your google drive within which to store all STREAMLINE experiments.
    * `experiment_name`: Change this to some new unique experiment name (do this each time you want to run a new experiment, either on the same or different dataset(s)), e.g. 'my_first_experiment'.
    * `class_label`: Change to the column header indicating the class label in each dataset, e.g. 'Class'.
    * `instance_label`: Change to the column header indicating unique instance ID's for each row in the dataset(s), or change to the string 'None' if your dataset does not include instance IDs.
5. Specifying replication data run parameters:
    * Scroll down to the code block with the text 'Run Parameters for Phase 10'.
    * If you don't have a replication dataset simply change `applyToReplication` to False (boolean value) and ignore the other two run parameters in this code block.
6. [Optional] Update other STREAMLINE run parameters to suit your analysis needs within code blocks 6-14. We will cover some common run parameters to consider here:
    * `cv_partitions`: The number of CV training/testing partitions created, and consequently the number of models trained for each ML algorithm. We recommend setting this between 3-10. A larger value will take longer to run but produce more accurate results.
    * `categorical_cutoff`: STREAMLINE uses this parameter to automatically determine which features to treat as categorical vs. numeric. If a feature has more than this many unique values, it is considered to be numeric.
        * Note: Currently, STREAMLINE does NOT automatically apply one-hot-encoding to categorical features meaning that all features will still be treated as numerical during ML modeling. Its currently up to the users decide whether to pre-encode features.  However STREAMLINE does take feature type into account during both the exploratory analysis, data preprocessing, and feature importance phases.
        * Note: Users can also manually specify which features to treat as categorical or even to point to features in the dataset that should be ignored in the analysis with the parameters `ignore_features_path` and `categorical_feature_path`, respectively. For either, instead of the default string 'None' setting the user specifies the path to a .csv file including a row of feature names from the dataset that should either be treated as categorical or ignored, respectively.
    * `algorithms`: A list of modeling algorithms to run, setting it to None will run all the algorithms. Must be from the set of the full or abbreviated name of models found in `streamline/models` folder.
    * `exlude`: A list of modeling algorithms to exclude from the pipeline. Must be from the set of the full or abbreviated name of models found in `streamline/models` folder.
    * * `n_trials`: Set to a higher value to give Optuna more attempts to optimize hyperparameter settings.
    * `timeout`: Set higher to increase the maximum time allowed for Optuna to run the specified `n_trials` (useful for algorithms that take more time to run)
* Note: There are a number of other run parameter options, and we encourage users to read descriptions of each to see what other options are available.
