# Running STREAMLINE
This section details how to run STREAMLINE in any of its run modes. These include:
1. **Google Colab Notebook:** (run remotely on free google cloud resources)
    * *Both an 'easy' and 'manual' run mode is available for users run their own data*
2. **Jupyter Notebook:** (run locally on your PC)
3. **Command Line Interface:** (locally or on a 'dask-compatable' CPU Computing Cluster)

While the notebooks only allow STREAMLINE to be run serially, it can be '[embarrassingly](https://en.wikipedia.org/wiki/Embarrassingly_parallel)' parallelized when run from the command line in one of two ways:
* **Local command line:** basic CPU core parallelization 
* **CPU Computing Cluster:** job submission parallelization

When run from the command line, STREAMLINE can be run in one of two ways:
* **Using a Configuration File:** run all, or any number of phases using a single command that points to a 'configuration file' with all necessary run parameters
* **Using Command-Line Arguments:** run all, or any number of phases using command line arguments

For more details and guidelines on selecting a run mode, see '[Picking a Run Mode](#picking-a-run-mode)'.

All users may benefit from reviewing the '[Guidelines for Setting Run Parameters](tips.md)' section for tips on (1) ensuring reproducibility (2) reducing runtime, and (3) improving modeling performance. Details on the variety of outputs generated by STREAMLINE can be found in '[Navigating STREAMLINE Output](output.md)'.

Once you've completed the [installation](install.md) instructions for the run mode desired, follow the mode-specific directions below for running STREAMLINE.

***
## Google Colab Notebook
This run mode is best for (1) easily trying out STREAMLINE on demonstration data, (2) running analyses on small datasets, or (3) educational purposes. Check out this [tutorial](https://www.tutorialspoint.com/google_colab/index.htm) to lean the basics of a [Google Colab Notebook](https://research.google.com/colaboratory/).

Below we first detail how to run the Colab Notebook on the included [demonstration datasets](data.md#demonstration-data), then how to adapt this notebook to run on your own dataset(s) as well as change STREAMLINE run parameters if desired. 

### Running the Demo (Colab)
The STREAMLINE Google Colab Notebook is set up to run a limited analysis applying all 9 phases of the pipeline. This includes 3-fold cross validation, and applying only three of the faster ML modeling algorithms to 2 example 'target datasets', and a 'replication dataset' relevant to only one of the target datasets. These datasets are detailed in [Demonstration Data](data.md#demonstration-data). This demo should take 6-7 minutes to run on Google Cloud, with results viewable in the notebook. The notebook will also automatically download the PDF summary reports, and the zipped 'experiment folder' with all output files, with the user's permission. 

To run this demo, do the following:
1. Set up a Google account (if you don't already have one). Click [here]( https://support.google.com/accounts/answer/27441?hl=en) for help.
2. Open the STREAMLINE Google Colab Notebook by clicking the link below:
[https://colab.research.google.com/drive/14AEfQ5hUPihm9JB2g730Fu3LiQ15Hhj2?usp=sharing](https://colab.research.google.com/drive/14AEfQ5hUPihm9JB2g730Fu3LiQ15Hhj2?usp=sharing)
3. \[Optional] Open the `Runtime` menu and select `Disconnect and delete runtime`. *This clears the memory of the previous notebook run. This is only necessary when the underlying base code is modified, but it may be useful to troubleshoot if modifications to the notebook do not seem to have an effect.*
4. Open the `Runtime` menu and select `Run all`. *This will run all code cells of the notebook, i.e. all phases of STREAMLINE.* 

At this point the notebook will do the following automatically:
* Reserve a limited amount of free memory (RAM) and disk space on Google Cloud.
* Load the most recent STREAMLINE repository into memory from Github. *The STREAMLINE release version is automatically indicated in the summary PDF reports.*
* Install other necessary python packages in the Google Colab Environment.
* Run the entirety of STREAMLINE on the [demonstration datasets](sample.md#demonstration-data).
* Download the PDF 'testing evaluation' and the 'replication evaluation' summary reports automatically. *Google will ask for user permission the first time*
* Download the zipped 'experiment folder' with all output files to your local computer.

See '[Notebook Output](output.md#notebooks)' for more on examining output within the notebook and '[Output Files](output.md#output-files)'

### Running Your Own Datasets (Colab)
Before running STREAMLINE on new data, make sure it adheres to '[Input Data Requirements](data.md#input-data-requirements)'. To update the STREAMLINE Colab Notebook to run on one or more user specified 'target datasets', users can chose between an 'easy' and 'manual mode. 

As above, begin by opening the STREAMLINE Google Colab Notebook by clicking the link below:
[https://colab.research.google.com/drive/14AEfQ5hUPihm9JB2g730Fu3LiQ15Hhj2?usp=sharing](https://colab.research.google.com/drive/14AEfQ5hUPihm9JB2g730Fu3LiQ15Hhj2?usp=sharing)

Before running, update the run parameters within the 'STREAMLINE RUN PARAMETERS' section of the notebook as indicated below.

*Note that, for brevity, some parameter names used in the notebook (used below) are slightly different from those used in the command line. Details on STREAMLINE run parameters are given [here](parameters.md).*

#### Easy Mode
This mode is most convenient if you want to run the notebook on other data, but want to be prompted to enter/select essential parameter information instead of adjusting parameters within run parameter code cells. This mode will prompt the user for essential 'experiment' and dataset-specific run parameter values. This mode is also convenient as it allows you to select datasets directly from your local computer rather than creating new folders within the temporary Colab Notebook workspace. All other non-essential run parmameters need to be updated within respective code cells.

1. In the first code cell, set (`demo_run = False`) and (`use_data_prompt = True`)
    * *This tells the notebook that you don't want to run the demo datasets, and you want to be 'prompted' to enter/select essential run parameters rather than edit the respective code cells*
2. \[Optional] Update non-essential run parameters (within respective code cells) to the users specifications
    * *Most commonly, this would include `n_splits`, `categorical_cutoff`, and `algorithms`
    * *We also strongly recommend specifying `categorical_feature_headers` and/or `quantitiative_feature_headers` as lists of feature names in the dataset(s) headers that should be treated as either categorical or quanatiative*
    * *If only one of these lists is specified, all features not specified in that list will be treated as the other feature type by default*
3. Open the `Runtime` menu and select `Run all`.
4. Reply to the prompts requesting the following essential parameter values: 
    * `experiment_name` - a unique name for the ouput folder for the current STREAMLINE 'experiment'
    * `data_path` - use file navigation window to select the folder containing one or more 'target datasets' to be analyzed
        * *These datasets must adhere to the formatting detailed in '[Input Data Requirements](data.md#input-data-requirements)'*
    * `class_label` - specify the header name for the outcome column in the dataset(s), e.g. 'Class'
    * `instance_label` - specify the header name for the unique instance IDs in the dataset(s) or specify `None` if not relevant
    * `match_label` - specify the header name for the match/group column in the dataset(s) or specify `None` if not relevant
    * `applyToReplication` - indicate `True` or `False` as to whether 'replication data' is available for the replication phase
    * `rep_data_path` - use file navigation window to select the folder containing one or more 'replication datasets' to be analyzed
        * *All datasets in this folder should be replicates for a single 'target dataset', and similarly adhere to [formatting](data.md#input-data-requirements) requirments*
    * `dataset_for_rep` - specify the filename (with extension) of the original 'target dataset' to indicate which models the replication data will be applied to
* *After providing valid entries for these prompts, all phases of STREAMLINE will run in sequence within the notebook.*
* *STREAMLINE outputfiles are automatically saved to the output 'experiment folder' named `UserOutput` within the temporary notebook workspace, as well as optionally downloaded to the users computer after completion* 

#### Manual Mode
1. In the first code cell, set (`demo_run = False`) and (`use_data_prompt = False`)
    * *This tells the notebook that you don't want to run the demo datasets, but you want update all run parameters (essential and non-essential) within respective code cells*
2. Click on the 'Files' tab on the left side of the notebook (pictured as a blank folder), and right-click on 'content' folder (i.e. the temporary google colab workspace) and create a 'New Folder' to contain your target dataset(s), called `UserData` (or some other name if you also update the `data_path` parameter)
3. Save your [formatted](data.md#input-data-requirements) target dataset(s) within this folder
    * *Note: We recommend making sure datasets run within Google Colab do not contain any sensitive or protected health information (PHI)*
4. \[Optional] Repeat steps 2-3 for any replication dataset(s) you wish to apply to the models trained for a specific 'target dataset'
    * *If you have no replication data, make sure to update the `applyToReplication` parameter to `False`*
5. Update (essential and non-essential) run parameter code cells to the dataset and users specifications
    * *Note: You can leave `output_path` as `/content/UserOutput` and all output will be saved to this automatically created folder*
    * *If you run more than one STREAMLINE 'experiments' in a single session, make sure to update `experiment_name` each time to avoid overwriting a prior experiment*
6. Open the `Runtime` menu and select `Run all`
* *Note: Common errors preventing the notebook from running to completion include issues with file/path names, dataset formatting, or other incorrect changes to other run parameter settings*
* *The notebook includes comments after each run parameter, indicating the format and value options for each*

***
## Jupyter Notebook
This run mode is best for (1) confirming successful STREAMLINE installation for local computer use, (2) running STREAMLINE in a notebook on your own computer's resources (generally faster than Colab Notebook), (3) running analyses on small to moderately sized datasets, (4) viewing output directly within a notebook, or (5) educational purposes. Click [here](https://jupyter.readthedocs.io/en/latest/running.html) to learn the basics of Jupyter Notebook.

Running STREAMLINE in Jupyter Notebook is largely the same as for running it in Google Colab. Below we specify how to run the Jupyter Notebook on the included [demonstration datasets](data.md#demonstration-data), then how to adapt it to run on your own dataset(s). 

### Running the Demo (Jupyter)
The STREAMLINE Jupyter Notebook is also set up to run a limited analysis applying all 9 phases of the pipeline. This includes 3-fold cross validation, and applying only three of the faster ML modeling algorithms to 2 example 'target datasets', and a 'replication dataset' relevant to only one of the target datasets. These datasets are detailed in [Demonstration Data](data.md#demonstration-data). This demo should take about 2-5 minutes to run (depending on your computer hardware), with results viewable in the notebook. The notebook will automatically save the 'experiment folder' (named `DemoOutput`) with all output files (including PDF reports).

1. From your command line, open Jupyter Notebook by typing `jupyter notebook`.
2. Within the Jupyter local file browser that opens, navigate into the previously saved/installed `STREAMLINE` directory where you find the file named `STREAMLINE-Notebook.ipypnb`.
3. Click to open `STREAMLINE-Notebook.ipypnb` as a Jupyter Notebook in a new page open in your web browser. 
4. Open the `Kernel` menu and select `Restart & Run All`. *This will run all code cells of the notebook, i.e. all phases of STREAMLINE.* 

At this point the notebook will do the following automatically:
* Run the entirety of STREAMLINE on the [demonstration datasets](sample.md#demonstration-data).
* Save all output files (including PDF reports) as an 'experiment folder' named `DemoOutput` within the `STREAMLINE` directory.

See '[Notebook Output](output.md#notebooks)' for more on examining output within the notebook and '[Output Files](output.md#output-files)'

### Running Your Own Datasets (Jupyter)
Begin by opening `STREAMLINE-Notebook.ipypnb` as a Jupyter Notebook (steps 1-3 above). Before running, update the run parameters within the 'STREAMLINE RUN PARAMETERS' section of the notebook as indicated below.

*Note that, for brevity, some parameter names used in the notebook (used below) are slightly different from those used in the command line. Details on STREAMLINE run parameters are given [here](parameters.md).*

1. In the first code cell, set (`demo_run = False`)
    * *This tells the notebook that you don't want to run the demo datasets, and you want to be 'prompted' to enter/select essential run parameters rather than edit the respective code cells*
2. Update essential run parameters (within respective code cells) to the user/dataset's specifications
    * `experiment_name` - a unique name for the ouput folder for the current STREAMLINE 'experiment'
    * `data_path` -  path to the folder containing one or more 'target datasets' to be analyzed
        * *These datasets must adhere to the formatting detailed in '[Input Data Requirements](data.md#input-data-requirements)'*
    * `output_path` - path to the folder (that will be automatically created if it doesn't yet exist) in which the 'experiment folder' including all STREAMLINE output will be saved
        * *Note: You can leave `output_path` as `./UserOutput` if you've named this folder `UserOutput`*
    * `class_label` - the header name for the outcome column in the dataset(s), e.g. 'Class'
    * `instance_label` - the header name for the unique instance IDs in the dataset(s) or specify `None` if not relevant
    * `match_label` - the header name for the match/group column in the dataset(s) or specify `None` if not relevant
    * `ignore_features` - list of text-valued feature names in target datasets that you want STREAMLINE to drop from the analysis, or specify `None` if not relevant
    * `categorical_feature_headers`  - list of text-valued feature names in the dataset(s) headers that should be treated as categorical or specify `None` if `quantitative_feature_headers` were specified and you want all other features to be treated as categorical, or you want feature types to be automatically decided using `categorical_cutoff`
    * `quantitiative_feature_headers` - list of text-valued feature names in the dataset(s) headers that should be treated as quantitative or specify `None` if `categorical_feature_headers` were specified and you want all other features to be treated as quantitative, or you want feature types to be automatically decided using `categorical_cutoff`
    * `applyToReplication` - indicate `True` or `False` as to whether 'replication data' is available for the replication phase
    * `rep_data_path` - path to folder containing one or more 'replication datasets' to be analyzed
        * *All datasets in this folder should be replicates for a single 'target dataset', and similarly adhere to [formatting](data.md#input-data-requirements) requirments*
    * `dataset_for_rep` - path to the file (with extension) of the original 'target dataset' to indicate which models the replication data will be applied to
3. \[Optional] Update non-essential run parameters (within respective code cells) to the users specifications
    * *Most commonly, this would include `n_splits`, `categorical_cutoff`, and `algorithms`
4. Open the `Kernel` menu and select `Restart & Run All`. *This will run all code cells of the notebook, i.e. all phases of STREAMLINE.* 
* *Note: It can take multiple hours or longer to run this notebook on larger datasets and/or using all machine learning modeling algorithms. We recommend using a computing cluster for such tasks if possible.*

***
## Command Line Interface
This run mode is best for (1) most efficiently running STREAMLINE with parallelization options, (2) users comfortable with command lines, or (3) running moderate to large datasets and/or more exhaustive run parameter configurations. 

Running STREAMLINE from the command line can be done locally (with or without CPU core parallelization), or on a dask-compatable CPU computing cluster. Any of these scenarios can also be run from a single command (i.e. all phases at once) using a 'configuration file', or separately one phase at a time. Below we indicate how to run all of these possible command line run configurations using the [demonstration datasets](data.md#demonstration-data) as an example. As for Google Colab and Jupyter Notebook run modes, to run STREAMLINE on datasets other than the [demonstration datasets](data.md#demonstration-data), essential run parameters should be specified/updated accordingly. STREAMLINE command line run parameters are detailed within the [run parameters section](parameters.md).

### Locally
This section explains running STREAMLINE locally using the command line interface.

#### Using a Configuration File
All phases of STREAMLINE can be run (in sequence) with a single simple command by editing and calling an associated configuration file (`run_configs/local.cfg`) as indicated below.
* *Note: This approach also allows users to run any subset of sequential STREAMLINE phases (e.g. Phase 1 alone for EDA, or Phases 1-4 for EDA, data processing, and feature selection) using the different 'phases to run' flags within the configuration file.*

1. Open your command line interface and navigate to the installed `STREAMLINE` directory.
2. To run the [demonstration datasets](data.md#demonstration-data), skip to step 5. To view the pre-specified configuration file, click [here](https://github.com/UrbsLab/STREAMLINE/blob/main/run_configs/local.cfg).
3. Assuming you want to run your own dataset(s), further navigate into the `run_configs` folder and open `local.cfg` in a text editor to update the essential and non-essential run parameters accordingly (see [run parameters](parameters.md) and [terminal text editors](install.md#terminal-text-editors) for help).
    * *Under 'phases to run' the `do_till_report` parameter (when set to `True`) will automatically run all phases up until `do_replicate` by default. `do_replicate`, `do_rep_report` and `do_cleanup` must all be specified individually*
    * *To run a subset of phases (e.g. phases 1-4), set `do_till_report = False`, and `do_eda`, `do_dataprep`, `do_feat_imp`, and `do_feat_sel` each to `True` and the other 'do' phases to `False`*
    * *Make sure to keep `run_cluster = False`, which tells STREAMLINE to be run locally rather than on a CPU computing cluster*
    * *Optionally set `run_parallel = False`, which will turn off local multi-core CPU parallelization*
4. Navigate back to the `STREAMLINE` base directory.
5. Run the following command within the `STREAMLINE` base directory:

```
python run.py -c run_configs/local.cfg
```
* *Note: You can save your own `.cfg` files to call with this command. We recommend copying renaming, and editing `local.cfg` and then calling this new configuration file as an argument to `run.py`*

#### Using Command-Line Arguments
STREAMLINE phases can also be called individually from the command line without a configuration file (instead specifying run parameters as arguments). This can be helpful, in particular, if you want to run a big analysis, and would like to look at the output of phases along the way without committing to running the whole pipeline upfront. The example commands below are set up to run the [demonstration datasets](data.md#demonstration-data), however users can adjust these arguments for their own data. Similar to any other run mode, make sure to specify arguments for all 'essential' run parameters for a given dataset. 
* *Note: Any unspecified non-essential run parameters will be assigned their default values for a given STREAMLINE run*
* *Warning: This run approach should not be used if you need/want to specify a list of ignored, categorical, or quanatiative feature names*

As before, begin by opening your command line interface and navigate to the installed `STREAMLINE` directory. 

The subsections below provide different example scenarios running `run.py` on the [demonstration datasets](data.md#demonstration-data). These scenarios run STREAMLINE similarly to our other demo run mode examples above, however we are not specifying categorical or quanatiative feature names, thus STREAMLINE is automatically attempting to detect feature types based on the `categorical_cutoff` parameter (with a default of 10).

##### All Phases (Replication Data Included)
This command will run all phases (1-9) 
* *Note: 
```
python run.py --data-path ./data/DemoData --out-path DemoOutputLocal --exp-name demo_experiment --do-till-report --class-label Class --inst-label InstanceID --algorithms=NB,LR,DT --do-replicate --rep-path ./data/DemoRepData --dataset ./data/DemoData/hcc-data_example_custom.csv --do-rep-report --do-clean --run-cluster False --run-parallel True
```



unspecified (non-essential run parameters will use default values)

### CPU Computing Cluster




#### Using command-line parameters

`run.py` can also be used with command line parameters 
as defined in the [parameters section](parameters.md)

Similarly, the following additional parameters need to be given

```
python run.py <other commands> --run-cluster False --run-parallel True<or Flase, accordingly>
```

As example case to all phases till report generation is given below:

```
python run.py --data-path ./data/DemoData --out-path demo --exp-name demo --do-till-report --class-label Class --inst-label InstanceID --algorithms=NB,LR,DT --run-cluster False --run-parallel True
```

A user can also run phases of STREAMLINE individually, 
however the user must have run all the phases before the phase he wants to run, i.e. the user must run this
pipeline sequentially in the given order.

To just run Exploratory Phase (Phase 1):
```
python run.py --data-path ./data/DemoData --out-path demo --exp-name demo --do-eda --class-label Class --inst-label InstanceID --algorithms NB,LR,DT --run-cluster False --run-parallel True
```

To just run Data Preparation Phase (Phase 2):
```
python run.py --out-path demo --exp-name demo --do-dataprep --run-cluster False --run-parallel True
```


To just run Feature Importance Phase (Phase 3):
```
python run.py --out-path demo --exp-name demo --do-feat-imp --run-cluster False --run-parallel True
```

To just run Feature Selection Phase (Phase 4):
```
python run.py --out-path demo --exp-name demo --do-feat-sel --run-cluster False --run-parallel True
```

To just run Modeling Phase (Phase 5):
```
python run.py --out-path demo --exp-name demo --do-model --algorithms NB,LR,DT --run-cluster False --run-parallel True
```

To just run Statistical Analysis Phase (Phase 6):
```
python run.py --out-path demo --exp-name demo --do-stats --run-cluster False --run-parallel True
```

To just run Dataset Compare Phase (Phase 7):
```
python run.py --out-path demo --exp-name demo --class-label Class --inst-label InstanceID --do-till-report False --do-compare-dataset True --algorithms NB,LR,DT --do-all False --run-cluster False --run-parallel True
```

To just run (Reporting Phase) Phase 8:
```
python run.py --out-path demo --exp-name demo --class-label Class --inst-label InstanceID --do-till-report False --do-report True --algorithms NB,LR,DT --do-all False --run-cluster False --run-parallel True
```


To just run Replication Phase (Phase 9):
```
python run.py --rep-path ./data/DemoRepData --dataset ./data/DemoData/hcc-data_example_custom.csv --out-path demo --exp-name demo --do-replicate --run-cluster False --run-parallel True
```

To just run Replication Report Phase (Phase 10):
```
python run.py --rep-path ./data/DemoRepData --dataset ./data/DemoData/hcc-data_example_custom.csv --out-path demo --exp-name demo --do-rep-report --run-cluster False --run-parallel True
```

To just run Cleaning Phase (Phase 11):
```
python run.py --out-path demo --exp-name demo --do-clean --del-time --del-old-cv --run-cluster False --run-parallel True
```




## Running on HPC Clusters

The easiest way to run STREAMLINE on HPC is through the CLI interface.
The runtime parameters can easily be set up using either the config file 
of command line parameters. A few tools may be helpful in doings so and are described in
the [helpful tools](#helpful-tools) section.

You only need to additionally define 4 additional parameters to run the models
using a cluster setup.

Rest is handled similarly by `run.py` as defined in the local section.

### Helpful Tools

#### nano
GNU nano is a text editor for Unix-like computing 
systems or operating environments using a command line interface. 

This would be incredibly handy in changing opening and changing the config file through ssh terminal
for using STREAMLINE through config file.

A detailed guide can be found [here](https://www.hostinger.com/tutorials/how-to-install-and-use-nano-text-editor)

A gist of the application is that you can edit the `run_configs/cedars.cfg` config file by the following steps
1. Go to the root streamline folder.
2. Type `nano run_configs/cedars.cfg` in the terminal to open the file in tmux.
3. Make the necessary in the changes in the config file.
4. Press `Ctrl + X` to close the file and `Y` to save the changes.


#### tmux
tmux is a terminal multiplexer/emulator. It lets you switch easily between several programs in one terminal, 
detach them (they keep running in the background) and reattach them to a different terminal. 

Terminal emulators programs allow you to create several "pseudo terminals" from a single terminal.
They decouple your programs from the main terminal, 
protecting them from accidentally disconnecting. 
You can detach tmux or screen from the login terminal, 
and all your programs will continue to run safely in the background. 
Later, we can reattach them to the same or a different terminal to 
monitor the process. These are also very useful for running multiple programs with a single connection, 
such as when you're remotely connecting to a machine using Secure Shell (SSH).

A detailed guide on using it can be found [here](https://www.redhat.com/sysadmin/introduction-tmux-linux)

A gist of the application is that you can open a new terminal 
that will stay open even if you disconnect and close your terminal.

The steps to take it is as follows:
1. Go to the root streamline folder.
2. Type and run `tmux new -s mysession`
3. Open the required config file using nano (e.g. `run_configs/cedars.cfg`) 
4. Make the necessary in the changes in the config file.
5. Press `Ctrl + X` to close the file and `Y` to save the changes.
6. Run required commands.
7. Press `Ctrl + b` and then the `d` key to close the terminal.



### Running using command line interface

#### Using config file

Edit the multiprocessing section of the config file according to your needs.

The multiprocessing section has four parameters that need to be defined.
1. `run-parallel`: Flag to run parallel processing in local job, overridden if `run-cluster` is defined. 
2. `res-mem`: memory reserved per job
3. `run-cluster`: flag for type of cluster, by far the most important parameter discussed in detail below.
4. `queue`: the partition queue used for job submissions.

The `run_cluster` parameter is the most important parameter here.
It is set to False when running locally, to use a cluster implementation, specify as a 
string type of cluster. Currently, clusters supported by `dask-jobqueue` can be supported.

Additionally, the old method of manual submission can be done using the flags
`"SLURMOld"` and `"LSFOld"` instead. This will generate and submit jobs using shell files 
similar to the legacy version of STREAMLINE.

As example config setup to run all steps till report generations using SLURM dask-jobqueue on Cedars HPC Cluster Setup.
is given in the config 
file [here](https://github.com/UrbsLab/STREAMLINE/blob/main/run_configs/cedars.cfg)

We specifically focus on the multiprocessing section of the 
config file 
[here](https://github.com/UrbsLab/STREAMLINE/blob/main/run_configs/cedars.cfg#L8-L12).


Now you can run the pipeline using the following command (considering the config file is `upenn.cfg`): 
```
python run.py -c run_configs/cedars.cfg
```


#### Using command-line parameters

`run.py` can also be used with command line parameters 
as defined in the [parameters section](parameters.md)

As discussed above you need only specify 3 additional parameters in the 
CLI parameters way of running STREAMLINE

```
python run.py <other commands> --run-cluster SLURM --res-mem 4 --queue defq
```

We give examples to run all phases separately and together 
on the example DemoData on the Cedars SLURM HPC.

As example case to all phases till report generation is given below:

```
python run.py --data-path ./data/DemoData --out-path demo --exp-name demo --do-till-report --class-label Class --inst-label InstanceID --algorithms=NB,LR,DT --run-cluster SLURM --res-mem 4 --queue defq
```

A user can also run phases of STREAMLINE individually, 
however the user must have run all the phases before the phase he wants to run, i.e. the user must run this
pipeline sequentially in the given order.

To just run Exploratory Phase (Phase 1):
```
python run.py --data-path ./data/DemoData --out-path demo --exp-name demo --do-eda --class-label Class --inst-label InstanceID --run-cluster SLURM --res-mem 4 --queue defq
```

To just run Data Preparation Phase (Phase 2):
```
python run.py --out-path demo --exp-name demo --do-dataprep --run-cluster SLURM --res-mem 4 --queue defq
```


To just run Feature Importance Phase (Phase 3):
```
python run.py --out-path demo --exp-name demo --do-feat-imp --run-cluster SLURM --res-mem 4 --queue defq
```

To just run Feature Selection Phase (Phase 4):
```
python run.py --out-path demo --exp-name demo --do-feat-sel --run-cluster SLURM --res-mem 4 --queue defq
```

To just run Modeling Phase (Phase 5):
```
python run.py --out-path demo --exp-name demo --do-model --algorithms NB,LR,DT --run-cluster SLURM --res-mem 4 --queue defq
```

To just run Statistical Analysis Phase (Phase 6):
```
python run.py --out-path demo --exp-name demo --do-stats --run-cluster SLURM --res-mem 4 --queue defq
```

To just run Dataset Compare Phase (Phase 7):
```
python run.py --out-path demo --exp-name demo --do-compare-dataset --run-cluster SLURM --res-mem 4 --queue defq
```

To just run (Reporting Phase) Phase 8:
```
python run.py --out-path demo --exp-name demo --do-report --run-cluster SLURM --res-mem 4 --queue defq
```


To just run Replication Phase (Phase 9):
```
python run.py --rep-path ./data/DemoRepData --dataset ./data/DemoData/hcc-data_example_custom.csv --out-path demo --exp-name demo --do-replicate --run-cluster SLURM --res-mem 4 --queue defq
```

To just run Replication Report Phase (Phase 10):
```
python run.py --rep-path ./data/DemoRepData --dataset ./data/DemoData/hcc-data_example_custom.csv --out-path demo --exp-name demo --do-rep-report --run-cluster SLURM --res-mem 4 --queue defq
```

To just run Cleaning Phase (Phase 11):
```
python run.py --out-path demo --exp-name demo --do-clean --del-time --del-old-cv --run-cluster SLURM --res-mem 4 --queue defq
```


## Picking a Run Mode

### Why run STREAMLINE on Google Colab?
Running STREAMLINE on Google Colab is best for:
1. Running the STREAMLINE demonstration on the included demo data
2. Users with little to no coding experience
3. Users that want the quickest/easiest approach to running STREAMLINE
4. Users that do not have access to a very powerful computer or compute cluster.
5. Applying STREAMLINE to smaller-scale analyses (in particular when only using free/limited Google Cloud resources):
    * Smaller datasets (e.g. < 500 instances and features)
    * A small number of total datasets (e.g. 1 or 2)
    * Only using the simplest/quickest modeling algorithms (e.g. Naive Bayes, Decision Trees, Logistic Regression)
    * Only using 1 or 2 modeling algorithms

1. **Google Colab Notebook** - on free Google Cloud resources [Anyone can run]:
    * Advantages
      * No coding or PC environment experience needed
      * Automatically installs and uses the most recent version of STREAMLINE
      * Computing can performed directly on Google Cloud from anywhere
      * One-click run of whole pipeline (all phases)
      * Offers in-notebook viewing of results and ability to save notebook as documentation of analysis
      * Allows easy customizability of nearly all aspects of the pipeline with minimal coding/environment experience
    * Disadvantages:
      * Can only run pipeline serially
      * Slowest of the run options
      * Limited by google cloud computing allowances (may only work for smaller datasets)
    * Notes: Requires a Google account (free)

2. **Jupyter Notebook** - locally [Basic experience]:
    * Advantages:
      * Does not rely on free computing limitations of Google Cloud (but rather your own computer's limitations)
      * One-click run of whole pipeline (all phases)
      * Offers in-notebook viewing of results and ability to save notebook as documentation of analysis
      * Allows easy customizability of all aspects of the pipeline with minimal coding/environment experience
    * Disadvantages:
      * Can only run pipeline serially
      * Slower runtime than from command-line
      * Beginners have to set up their computing environment
    * Notes: Requires Anaconda3, Python3, and several other minor Python package installations

3. **Command Line (Local)** [Command-line Users]:
    * Advantages:
      * Typically runs faster than within Jupyter Notebook
      * A more versatile option for those with command-line experience
      * One-command run of whole pipeline available when using a configuration file to run
      * Can optionally run the pipeline one phase at a time
    * Disadvantages:
      * Can only run pipeline serially or with limited local cpu core parallelization
      * Command-line experience recommended
    * Notes: Requires Anaconda3, Python3, and several other minor Python package installations

4. **Command Line (HPC Cluster)** [Computing Cluster Users]:
    * Advantages:
      * By far the fastest, most efficient way to run STREAMLINE
      * Offers ability to run STREAMLINE over 7 types of HPC systems
      * One-command run of whole pipeline available when using a configuration file to run
      * Can optionally run the pipeline one phase at a time
    * Disadvantages:
      * Experience with command-line and dask-compatible clusters recommended
      * Access to a computing cluster required
    * Notes: Requires Anaconda3, Python3, and several other minor Python package installations. Cluster runs of STREAMLINE were set up using `dask-jobqueue` and thus should support 7 types of clusters as described in the [dask documentation](https://jobqueue.dask.org/en/latest/api.html). Currently we have only directly tested STREAMLINE on SLURM and LSF clusters. Further codebase adaptation may be needed for clusters types not on the above link.

