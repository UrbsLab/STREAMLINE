# Running STREAMLINE
This section details how to run STREAMLINE in any of its run modes. These include:
1. **Google Colab Notebook:** (run remotely on free google cloud resources)
    * *Both an 'easy' and 'manual' run mode is available for users run their own data*
2. **Jupyter Notebook:** (run locally on your PC)
3. **Command Line:** (locally or on a 'dask-compatable' CPU Computing Cluster)

While the notebooks only allow STREAMLINE to be run serially, it can be '[embarrassingly](https://en.wikipedia.org/wiki/Embarrassingly_parallel)' parallelized when run from the command line in one of two ways:
* **Local command line:** basic CPU core parallelization 
* **CPU Computing Cluster:** job submission parallelization

Lastly, when run from the command line, STREAMLINE can be run in one of two ways:
* **All phases at once:** with a single command pointing to a 'configuration file' that includes all necessary run parameters
* **One phase at a time:** using either phase-specific commands with command-line arguments, or again using the 'configuration file'

For more details and guidelines on selecting a run mode, see '[Picking a Run Mode](#picking-a-run-mode)'.

All users may benefit from reviewing the '[Guidelines for Setting Run Parameters](tips.md)' section for tips on (1) ensuring reproducibility (2) reducing runtime, and (3) improving modeling performance. Details on the variety of outputs generated by STREAMLINE can be found in '[Navigating STREAMLINE Output](output.md)'.

Once you've completed the [installation](install.md) instructions for the run mode desired, follow the mode-specific directions below for running STREAMLINE.

***
## Google Colab Notebook
This run mode is best for (1) easily trying out STREAMLINE on demonstration data, (2) running analyses on small datasets, or (3) educational purposes. Check out this [tutorial](https://www.tutorialspoint.com/google_colab/index.htm) to lean the basics of a [Google Colab Notebook](https://research.google.com/colaboratory/).

Below we first detail how to run the Colab Notebook on the included [demonstration datasets](data.md#demonstration-data), then how to adapt this notebook to run on your own dataset(s) as well as change STREAMLINE run parameters if desired. 

### Running the Demo (Colab)
The STREAMLINE Google Colab Notebook is set up to run a limited analysis applying all 9 phases of the pipeline. This includes 3-fold cross validation, and applying only three of the faster ML modeling algorithms to 2 example 'target datasets', and a 'replication dataset' relevant to only one of the target datasets. These datasets are detailed in [Demonstration Data](data.md#demonstration-data). This demo should take 6-7 minutes to run on Google Cloud, with results viewable in the notebook. The notebook will also automatically download the PDF summary reports, and the zipped 'experiment folder' with all output files, with the user's permission. 

To run this demo, do the following:
1. Set up a Google account (if you don't already have one). Click [here]( https://support.google.com/accounts/answer/27441?hl=en) for help.
2. Open the STREAMLINE Google Colab Notebook by clicking the link below:
[https://colab.research.google.com/drive/14AEfQ5hUPihm9JB2g730Fu3LiQ15Hhj2?usp=sharing](https://colab.research.google.com/drive/14AEfQ5hUPihm9JB2g730Fu3LiQ15Hhj2?usp=sharing)
3. [Optional] Open the `Runtime` menu and select `Disconnect and delete runtime`. *This clears the memory of the previous notebook run. This is only necessary when the underlying base code is modified, but it may be useful to troubleshoot if modifications to the notebook do not seem to have an effect.*
4. Open the `Runtime` menu and select `Run all`. *This will run all code cells of the notebook, i.e. all phases of STREAMLINE.* 

At this point the notebook will do the following automatically:
* Reserve a limited amount of free memory (RAM) and disk space on Google Cloud.
* Load the most recent STREAMLINE repository into memory from Github. *The STREAMLINE release version is automatically indicated in the summary PDF reports.*
* Install other necessary python packages in the Google Colab Environment.
* Run the entirety of STREAMLINE on the [demonstration datasets](sample.md#demonstration-data).
* Download the PDF 'testing evaluation' and the 'replication evaluation' summary reports automatically. *Google will ask for user permission the first time*
* Download the zipped 'experiment folder' with all output files to your local computer.

See '[Notebook Output](output.md#notebooks)' for more on examining output within the notebook and '[Output Files](output.md#output-files)'

### Running Your Own Datasets (Colab)
Before running STREAMLINE on new data, make sure it adheres to '[Input Data Requirements](data.md#input-data-requirements)'. To update the STREAMLINE Colab Notebook to run on one or more user specified 'target datasets', users can chose between an 'easy' and 'manual mode. 

As above, begin by opening the STREAMLINE Google Colab Notebook by clicking the link below:
[https://colab.research.google.com/drive/14AEfQ5hUPihm9JB2g730Fu3LiQ15Hhj2?usp=sharing](https://colab.research.google.com/drive/14AEfQ5hUPihm9JB2g730Fu3LiQ15Hhj2?usp=sharing)

Before running, update the run parameters within the 'STREAMLINE RUN PARAMETERS' section of the notebook as indicated below.

*Note that, for brevity, some parameter names used in the notebook (used below) are slightly different from those used in the command line. Details on STREAMLINE run parameters are given [here](parameters.md).*

#### Easy Mode
This mode is most convenient if you want to run the notebook on other data, but want to be prompted to enter/select essential parameter information instead of adjusting parameters within run parameter code cells. This mode will prompt the user for essential 'experiment' and dataset-specific run parameter values. This mode is also convenient as it allows you to select datasets directly from your local computer rather than creating new folders within the temporary Colab Notebook workspace. All other non-essential run parmameters need to be updated within respective code cells.

1. In the first code cell, set (`demo_run = False`) and (`use_data_prompt = True`)
    * *This tells the notebook that you don't want to run the demo datasets, and you want to be 'prompted' to enter/select essential run parameters rather than edit the respective code cells*
2. Update non-essential run parameters (within respective code cells) to the users specifications
    * *Most commonly, this would include `n_splits`, `categorical_cutoff`, and `algorithms`
    * *We also strongly recommend specifying `categorical_feature_headers` and/or `quantitiative_feature_headers` as lists of feature names in the dataset(s) headers that should be treated as either categorical or quanatiative*
    * *If only one of these lists is specified, all features not specified in that list will be treated as the other feature type by default*
3. Open the `Runtime` menu and select `Run all`.
4. Reply to the prompts requesting the following essential parameter values: 
    * `experiment_name` - a unique name for the ouput folder for the current STREAMLINE 'experiment'
    * `data_path` - use file navigation window to select the folder containing one or more 'target datasets' to be analyzed
        * *These datasets must adhere to the formatting detailed in '[Input Data Requirements](data.md#input-data-requirements)'*
    * `class_label` - specify the header name for the outcome column in the dataset(s), e.g. 'Class'
    * `instance_label` - specify the header name for the unique instance IDs in the dataset(s) or specify `None` if not relevant
    * `match_label` - specify the header name for the match/group column in the dataset(s) or specify `None` if not relevant
    * Indicate `True` or `False` as to whether 'replication data' is available for the replication phase
    * `rep_data_path` - use file navigation window to select the folder containing one or more 'replication datasets' to be analyzed
        * *All datasets in this folder should be replicates for a single 'target dataset', and similarly adhere to formatting requirments*
    * `dataset_for_rep` - specify the filename (with extension) of the original 'target dataset' to indicate which models the replication data will be applied to
* *After providing valid entries for these prompts, all phases of STREAMLINE will run in sequence within the notebook.*
* *STREAMLINE outputfiles are automatically saved to the output 'experiment folder' named `UserOutput` within the temporary notebook workspace, as well as optionally downloaded to the users computer after completion* 

#### Manual Mode
1. In the first code cell, set (`demo_run = False`) and (`use_data_prompt = False`)
    * *This tells the notebook that you don't want to run the demo datasets, but you want update all run parameters (essential and non-essential) within respective code cells*
2. Click on the 'Files' tab on the left side of the notebook (pictured as a blank folder), and right-click on 'content' folder (i.e. the temporary google colab workspace) and create a 'New Folder' to contain your target dataset(s), called `UserData` (or some other name if you also update the `data_path` parameter)
3. Save your [formatted](data.md#input-data-requirements) target dataset(s) within this folder
    * *Note: We recommend making sure datasets run within Google Colab do not contain any sensitive or protected health information (PHI)*
4. \[Optional] Repeat steps 2-3 for any replication dataset(s) you wish to apply to the models trained for a specific 'target dataset'
    * *If you have no replication data, make sure to update the `applyToReplication` parameter to `False`*
5. Update (essential and non-essential) run parameter code cells to the dataset and users specifications
    * *Note: You can leave `output_path` as `/content/UserOutput` if you've named this folder `UserOutput`*
    * *If you run more than one STREAMLINE 'experiments' in a single session, make sure to update `experiment_name` each time to avoid overwriting a prior experiment*
6. Open the `Runtime` menu and select `Run all`
* *Note: Common errors preventing the notebook from running to completion include issues with file/path names, dataset formatting, or other incorrect changes to other run parameter settings*
* *The notebook includes comments after each run parameter, indicating the format and value options for each*

***
## Jupyter Notebook
This run mode is best for (1) confirming successful STREAMLINE installation for local computer use, (2) running STREAMLINE in a notebook on your own computer's resources (generally faster than Colab Notebook), (2) running analyses on small to moderately sized datasets, or (3) educational purposes. Click [here](https://jupyter.readthedocs.io/en/latest/running.html) to learn the basics of Jupyter Notebook.

Running STREAMLINE in Jupyter Notebook is largely the same as for running it in Google Colab. Below we specify how to run the Jupyter Notebook on the included [demonstration datasets](data.md#demonstration-data), then how to adapt it to run on your own dataset(s). 

### Running the Demo (Jupyter)
The STREAMLINE Jupyter Notebook is also set up to run a limited analysis applying all 9 phases of the pipeline. This includes 3-fold cross validation, and applying only three of the faster ML modeling algorithms to 2 example 'target datasets', and a 'replication dataset' relevant to only one of the target datasets. These datasets are detailed in [Demonstration Data](data.md#demonstration-data). This demo should take about 2-5 minutes to run (depending on your computer hardware), with results viewable in the notebook. The notebook will automatically save the 'experiment folder' (named `DemoOutput`) with all output files (including PDF reports).

1. From your command line, open Jupyter Notebook by typing `jupyter notebook`.
2. Within the Jupyter local file browser that opens, navigate into the previously saved/installed `STREAMLINE` directory where you find the file named `STREAMLINE-Notebook.ipypnb`.
3. Click to open `STREAMLINE-Notebook.ipypnb` as a Jupyter Notebook in a new page open in your web browser. 
4. Open the `Kernel` menu and select `Restart & Run All`. *This will run all code cells of the notebook, i.e. all phases of STREAMLINE.* 

At this point the notebook will do the following automatically:
* Run the entirety of STREAMLINE on the [demonstration datasets](sample.md#demonstration-data).
* Save all output files (including PDF reports) as an 'experiment folder' named `DemoOutput` within the `STREAMLINE` directory.
* Download the zipped 'experiment folder' with all output files to your local computer.

See '[Notebook Output](output.md#notebooks)' for more on examining output within the notebook and '[Output Files](output.md#output-files)'


### Running Your Own Datasets (Jupyter)



#### Running on Demo Dataset
Here we detail how to run STREAMLINE within the provided Jupyter Notebook named `STREAMLINE-Notebook.ipypnb`. 
This included notebook is set up to run on the included [demonstration datasets](sample.md#demonstration-data).

1. First, ensure all local installation is done as per the [guide](install.md#jupyter-notebook) in your environment 
   and dataset assumptions are satisfied.

2. Open Jupyter Notebook (info about Jupyter Notebooks here, https://jupyter.readthedocs.io/en/latest/running.html) by 
   going to the STREAMLINE Root folder, typing `jupyter notebook` and then opening the `STREAMLINE-Notebook.ipypnb` that
   shows up in the new page open on your web browser.

3. Scroll down to the second code block of the notebook below the header 'Mandatory Parameters to Update' and update the following run parameters to reflect paths on your PC.
    * `data_path`: Change the path, so it reflects the location of the `DemoData` folder (within the STREAMLINE folder) on your PC, e.g. `C:/Users/ryanu/Documents/GitHub/STREAMLINE/DemoData`.
    * `output_path`: Change the path to specify the desired location for the STREAMLINE output experiment folder.

4. Click `Kernel` on the Jupyter notebook GUI, and select `Restart & Run All` to run the script.  

5. Running the included [demonstration datasets](sample.md#demonstration-data) with the pre-specified notebook run parameters, 
   should only take a 3-5 minutes depending on your PC hardware.
    * Note: It may take several hours or more to run this notebook in other contexts. Parameters that impact runtimes are discussed in [this section](tips.md#reducing-runtime) above. We recommend users with larger analyses to use a computing cluster if possible.

#### Running on Your Own Datasets
Move your custom dataset to the STREAMLINE root directory,
follow the same steps as in the [Colab Notebook](colab.md#running-on-your-own-datasets-tbd)

### Running on command line interface

The most efficient way of running STREAMLINE is through command line.
There's two ways to run STREAMLINE thorough a CLI interface.

1. Through picking up run parameters through a config file.
2. Through manually inputting run parameters

There is a runner file called run.py which runs the whole or part of STREAMLINE
pipeline as defined. A few examples are given below.

#### Using config file

`run.py` can also be used with config parameters 
as defined in the [parameters section](parameters.md)

Then it can be run with the command defined below.
```
python run.py -c <config_file>
```

For running the Demo Dataset locally a config file is already provided and 
the user doesn't need to do any edits.

The example config setup to run all steps till report generations locally on the Demo Dataset
is given in the config 
file [here](https://github.com/UrbsLab/STREAMLINE/blob/dev/run.cfg)

The user can simply run the following command to run the whole pipeline:
```
python run.py -c ./run_configs/local.cfg
```

Specifically the `run_cluster` parameter in the `multiprocessing` section has to been defined as
`run_cluster = "False"` which runs it locally. This specific parameter in 
the file is located [here](https://github.com/UrbsLab/STREAMLINE/blob/5c66b3286056bbd9b514c202aa0a22758a76f62c/run.cfg#L11)


#### Using command-line parameters

`run.py` can also be used with command line parameters 
as defined in the [parameters section](parameters.md)

Similarly, the following additional parameters need to be given

```
python run.py <other commands> --run-cluster False --run-parallel True<or Flase, accordingly>
```

As example case to all phases till report generation is given below:

```
python run.py --data-path ./data/DemoData --out-path demo --exp-name demo --do-till-report --class-label Class --inst-label InstanceID --algorithms=NB,LR,DT --run-cluster False --run-parallel True
```

A user can also run phases of STREAMLINE individually, 
however the user must have run all the phases before the phase he wants to run, i.e. the user must run this
pipeline sequentially in the given order.

To just run Exploratory Phase (Phase 1):
```
python run.py --data-path ./data/DemoData --out-path demo --exp-name demo --do-eda --class-label Class --inst-label InstanceID --algorithms NB,LR,DT --run-cluster False --run-parallel True
```

To just run Data Preparation Phase (Phase 2):
```
python run.py --out-path demo --exp-name demo --do-dataprep --run-cluster False --run-parallel True
```


To just run Feature Importance Phase (Phase 3):
```
python run.py --out-path demo --exp-name demo --do-feat-imp --run-cluster False --run-parallel True
```

To just run Feature Selection Phase (Phase 4):
```
python run.py --out-path demo --exp-name demo --do-feat-sel --run-cluster False --run-parallel True
```

To just run Modeling Phase (Phase 5):
```
python run.py --out-path demo --exp-name demo --do-model --algorithms NB,LR,DT --run-cluster False --run-parallel True
```

To just run Statistical Analysis Phase (Phase 6):
```
python run.py --out-path demo --exp-name demo --do-stats --run-cluster False --run-parallel True
```

To just run Dataset Compare Phase (Phase 7):
```
python run.py --out-path demo --exp-name demo --class-label Class --inst-label InstanceID --do-till-report False --do-compare-dataset True --algorithms NB,LR,DT --do-all False --run-cluster False --run-parallel True
```

To just run (Reporting Phase) Phase 8:
```
python run.py --out-path demo --exp-name demo --class-label Class --inst-label InstanceID --do-till-report False --do-report True --algorithms NB,LR,DT --do-all False --run-cluster False --run-parallel True
```


To just run Replication Phase (Phase 9):
```
python run.py --rep-path ./data/DemoRepData --dataset ./data/DemoData/hcc-data_example_custom.csv --out-path demo --exp-name demo --do-replicate --run-cluster False --run-parallel True
```

To just run Replication Report Phase (Phase 10):
```
python run.py --rep-path ./data/DemoRepData --dataset ./data/DemoData/hcc-data_example_custom.csv --out-path demo --exp-name demo --do-rep-report --run-cluster False --run-parallel True
```

To just run Cleaning Phase (Phase 11):
```
python run.py --out-path demo --exp-name demo --do-clean --del-time --del-old-cv --run-cluster False --run-parallel True
```




## Running on HPC Clusters

The easiest way to run STREAMLINE on HPC is through the CLI interface.
The runtime parameters can easily be set up using either the config file 
of command line parameters. A few tools may be helpful in doings so and are described in
the [helpful tools](#helpful-tools) section.

You only need to additionally define 4 additional parameters to run the models
using a cluster setup.

Rest is handled similarly by `run.py` as defined in the local section.

### Helpful Tools

#### nano
GNU nano is a text editor for Unix-like computing 
systems or operating environments using a command line interface. 

This would be incredibly handy in changing opening and changing the config file through ssh terminal
for using STREAMLINE through config file.

A detailed guide can be found [here](https://www.hostinger.com/tutorials/how-to-install-and-use-nano-text-editor)

A gist of the application is that you can edit the `run_configs/cedars.cfg` config file by the following steps
1. Go to the root streamline folder.
2. Type `nano run_configs/cedars.cfg` in the terminal to open the file in tmux.
3. Make the necessary in the changes in the config file.
4. Press `Ctrl + X` to close the file and `Y` to save the changes.


#### tmux
tmux is a terminal multiplexer/emulator. It lets you switch easily between several programs in one terminal, 
detach them (they keep running in the background) and reattach them to a different terminal. 

Terminal emulators programs allow you to create several "pseudo terminals" from a single terminal.
They decouple your programs from the main terminal, 
protecting them from accidentally disconnecting. 
You can detach tmux or screen from the login terminal, 
and all your programs will continue to run safely in the background. 
Later, we can reattach them to the same or a different terminal to 
monitor the process. These are also very useful for running multiple programs with a single connection, 
such as when you're remotely connecting to a machine using Secure Shell (SSH).

A detailed guide on using it can be found [here](https://www.redhat.com/sysadmin/introduction-tmux-linux)

A gist of the application is that you can open a new terminal 
that will stay open even if you disconnect and close your terminal.

The steps to take it is as follows:
1. Go to the root streamline folder.
2. Type and run `tmux new -s mysession`
3. Open the required config file using nano (e.g. `run_configs/cedars.cfg`) 
4. Make the necessary in the changes in the config file.
5. Press `Ctrl + X` to close the file and `Y` to save the changes.
6. Run required commands.
7. Press `Ctrl + b` and then the `d` key to close the terminal.



### Running using command line interface

#### Using config file

Edit the multiprocessing section of the config file according to your needs.

The multiprocessing section has four parameters that need to be defined.
1. `run-parallel`: Flag to run parallel processing in local job, overridden if `run-cluster` is defined. 
2. `res-mem`: memory reserved per job
3. `run-cluster`: flag for type of cluster, by far the most important parameter discussed in detail below.
4. `queue`: the partition queue used for job submissions.

The `run_cluster` parameter is the most important parameter here.
It is set to False when running locally, to use a cluster implementation, specify as a 
string type of cluster. Currently, clusters supported by `dask-jobqueue` can be supported.

Additionally, the old method of manual submission can be done using the flags
`"SLURMOld"` and `"LSFOld"` instead. This will generate and submit jobs using shell files 
similar to the legacy version of STREAMLINE.

As example config setup to run all steps till report generations using SLURM dask-jobqueue on Cedars HPC Cluster Setup.
is given in the config 
file [here](https://github.com/UrbsLab/STREAMLINE/blob/main/run_configs/cedars.cfg)

We specifically focus on the multiprocessing section of the 
config file 
[here](https://github.com/UrbsLab/STREAMLINE/blob/main/run_configs/cedars.cfg#L8-L12).


Now you can run the pipeline using the following command (considering the config file is `upenn.cfg`): 
```
python run.py -c run_configs/cedars.cfg
```


#### Using command-line parameters

`run.py` can also be used with command line parameters 
as defined in the [parameters section](parameters.md)

As discussed above you need only specify 3 additional parameters in the 
CLI parameters way of running STREAMLINE

```
python run.py <other commands> --run-cluster SLURM --res-mem 4 --queue defq
```

We give examples to run all phases separately and together 
on the example DemoData on the Cedars SLURM HPC.

As example case to all phases till report generation is given below:

```
python run.py --data-path ./data/DemoData --out-path demo --exp-name demo --do-till-report --class-label Class --inst-label InstanceID --algorithms=NB,LR,DT --run-cluster SLURM --res-mem 4 --queue defq
```

A user can also run phases of STREAMLINE individually, 
however the user must have run all the phases before the phase he wants to run, i.e. the user must run this
pipeline sequentially in the given order.

To just run Exploratory Phase (Phase 1):
```
python run.py --data-path ./data/DemoData --out-path demo --exp-name demo --do-eda --class-label Class --inst-label InstanceID --run-cluster SLURM --res-mem 4 --queue defq
```

To just run Data Preparation Phase (Phase 2):
```
python run.py --out-path demo --exp-name demo --do-dataprep --run-cluster SLURM --res-mem 4 --queue defq
```


To just run Feature Importance Phase (Phase 3):
```
python run.py --out-path demo --exp-name demo --do-feat-imp --run-cluster SLURM --res-mem 4 --queue defq
```

To just run Feature Selection Phase (Phase 4):
```
python run.py --out-path demo --exp-name demo --do-feat-sel --run-cluster SLURM --res-mem 4 --queue defq
```

To just run Modeling Phase (Phase 5):
```
python run.py --out-path demo --exp-name demo --do-model --algorithms NB,LR,DT --run-cluster SLURM --res-mem 4 --queue defq
```

To just run Statistical Analysis Phase (Phase 6):
```
python run.py --out-path demo --exp-name demo --do-stats --run-cluster SLURM --res-mem 4 --queue defq
```

To just run Dataset Compare Phase (Phase 7):
```
python run.py --out-path demo --exp-name demo --do-compare-dataset --run-cluster SLURM --res-mem 4 --queue defq
```

To just run (Reporting Phase) Phase 8:
```
python run.py --out-path demo --exp-name demo --do-report --run-cluster SLURM --res-mem 4 --queue defq
```


To just run Replication Phase (Phase 9):
```
python run.py --rep-path ./data/DemoRepData --dataset ./data/DemoData/hcc-data_example_custom.csv --out-path demo --exp-name demo --do-replicate --run-cluster SLURM --res-mem 4 --queue defq
```

To just run Replication Report Phase (Phase 10):
```
python run.py --rep-path ./data/DemoRepData --dataset ./data/DemoData/hcc-data_example_custom.csv --out-path demo --exp-name demo --do-rep-report --run-cluster SLURM --res-mem 4 --queue defq
```

To just run Cleaning Phase (Phase 11):
```
python run.py --out-path demo --exp-name demo --do-clean --del-time --del-old-cv --run-cluster SLURM --res-mem 4 --queue defq
```


## Picking a Run Mode

### Why run STREAMLINE on Google Colab?
Running STREAMLINE on Google Colab is best for:
1. Running the STREAMLINE demonstration on the included demo data
2. Users with little to no coding experience
3. Users that want the quickest/easiest approach to running STREAMLINE
4. Users that do not have access to a very powerful computer or compute cluster.
5. Applying STREAMLINE to smaller-scale analyses (in particular when only using free/limited Google Cloud resources):
    * Smaller datasets (e.g. < 500 instances and features)
    * A small number of total datasets (e.g. 1 or 2)
    * Only using the simplest/quickest modeling algorithms (e.g. Naive Bayes, Decision Trees, Logistic Regression)
    * Only using 1 or 2 modeling algorithms

1. **Google Colab Notebook** - on free Google Cloud resources [Anyone can run]:
    * Advantages
      * No coding or PC environment experience needed
      * Automatically installs and uses the most recent version of STREAMLINE
      * Computing can performed directly on Google Cloud from anywhere
      * One-click run of whole pipeline (all phases)
      * Offers in-notebook viewing of results and ability to save notebook as documentation of analysis
      * Allows easy customizability of nearly all aspects of the pipeline with minimal coding/environment experience
    * Disadvantages:
      * Can only run pipeline serially
      * Slowest of the run options
      * Limited by google cloud computing allowances (may only work for smaller datasets)
    * Notes: Requires a Google account (free)

2. **Jupyter Notebook** - locally [Basic experience]:
    * Advantages:
      * Does not rely on free computing limitations of Google Cloud (but rather your own computer's limitations)
      * One-click run of whole pipeline (all phases)
      * Offers in-notebook viewing of results and ability to save notebook as documentation of analysis
      * Allows easy customizability of all aspects of the pipeline with minimal coding/environment experience
    * Disadvantages:
      * Can only run pipeline serially
      * Slower runtime than from command-line
      * Beginners have to set up their computing environment
    * Notes: Requires Anaconda3, Python3, and several other minor Python package installations

3. **Command Line (Local)** [Command-line Users]:
    * Advantages:
      * Typically runs faster than within Jupyter Notebook
      * A more versatile option for those with command-line experience
      * One-command run of whole pipeline available when using a configuration file to run
      * Can optionally run the pipeline one phase at a time
    * Disadvantages:
      * Can only run pipeline serially or with limited local cpu core parallelization
      * Command-line experience recommended
    * Notes: Requires Anaconda3, Python3, and several other minor Python package installations

4. **Command Line (HPC Cluster)** [Computing Cluster Users]:
    * Advantages:
      * By far the fastest, most efficient way to run STREAMLINE
      * Offers ability to run STREAMLINE over 7 types of HPC systems
      * One-command run of whole pipeline available when using a configuration file to run
      * Can optionally run the pipeline one phase at a time
    * Disadvantages:
      * Experience with command-line and dask-compatible clusters recommended
      * Access to a computing cluster required
    * Notes: Requires Anaconda3, Python3, and several other minor Python package installations. Cluster runs of STREAMLINE were set up using `dask-jobqueue` and thus should support 7 types of clusters as described in the [dask documentation](https://jobqueue.dask.org/en/latest/api.html). Currently we have only directly tested STREAMLINE on SLURM and LSF clusters. Further codebase adaptation may be needed for clusters types not on the above link.

