# Navigating STREAMLINE Output
This section covers the different outputs generated by STREAMLINE. The sections below will use the demo run of STREAMLINE on two [demonstration datasets](data.md#demonstration-data) as a specific example for navigating the output files generated.

***
## Notebooks
During or after the notebook runs, users can inspect the individual code and text (i.e. markdown) cells of the notebook. Individual cells can be collapsed or expanded by clicking on the small arrowhead on the left side of each cell. The first set of cells include basic notebook instructions and then specify all run parameters (which a user can edit direclty within the notebook). Later cells run the underlying STREAMLINE code for up to 9 phases, plus output folder cleaning (and downloading output files in the case of the Colab Notebook).

These later code cells will automatically display many of the notifications, results, and output figures generated by STREAMLINE. 

As mentioned, the Google Colab notebook will automatically download the output folder as a zipped folder, as well as automatically open the testing and replication PDF reports on your computer. Users can then extract this downloaded output folder and view all individual output files arranged into analysis subdirectories. You can also view output files in Google Colab by opening the file-explorer pane on the left side of the notebook.

***
## Experiment Folder (Hierarchy)
1. After running STREAMLINE you will find the 'experiment folder' (named by the [experiment_name](parameters.md#experiment-name) parameter) saved to folder specified by [output_path](parameters.md#output-path). In the Colab Notebook demo, this would be `/content/DemoOutput/demo_experiment/`. 
2. Opening the above experiment folder you will find the following folder/file hierarchy:
    * `DatasetComparisons` - all statistical significance results and plots for comparing modeling performance across multiple 'target datasets' run
        * `dataCompBoxplots` - all data comparison boxplots
    * `hcc-data_example` - all output specific to the first 'target dataset' analyzed
        * `CVDatasets` - copies of all training and testing datasets in .csv format (as well as intermediate files if [overwrite_cv](parameters.md#overwrite-cv) = `False`)
        * `exploratory` - all phase 1 exploratory data analysis (EDA) output, files at this level are post-processed EDA output
            * `initial` - all pre-processed EDA output
            * `univariate_analyses` - all univariate analysis results and plots
        * `feature_selection` - all phase 3 & 4 output (feature importance estimation and feature selection)
            * `multisurf` - MultiSURF scores and a summary figure
            * `mutual_information` - mutual information scores and a summary figure
        * `model_evaluation` - all model evaluation output (phase 6)
            * `feature_importance` - all model feature importance estimation scores and figures
            * `metricBoxplots` - all evaluation metric boxplots comparing algorithm performance
            * `pickled_metrics` - all evaluation metrics pickled separately for each algorithm and CV dataset combo
            * `statistical_comparisons` - all statistical significance results comparing algorithm performance
        * `models` - all model output (phase 5), including pickled model objects and selected hyperparameter settings for each algorithm and CV dataset combo
            * `pickledModels` - all models saved as pickled objects
        * `scale_impute` - all trained imputation and scaling maps saved as pickled objects
    * `hcc-data_example_custom` - contains all output specific to the second 'target dataset' analyzed
        * *Has the same folder hierarchy as `hcc-data_example` above with the addition of a `replication` folder*
        * `replication` - all phase 8 (i.e. replication) output for the second 'target dataset' analyzed
            * `hcc-data_example_custom_rep` - all replication output for the this specific 'replication dataset' (in this demo there was only one)
                * `exploratory` - all exploratory data analysis (EDA) output for this 'replication dataset', files at this level are post-processed EDA output
                    * `initial` - all pre-processed EDA output for this 'replication dataset'
                * `model_evaluation` - all model evaluation output for this 'replication dataset'
                    * `metricBoxplots` - all evaluation metric boxplots comparing algorithm performance for this 'replication dataset'
                    * `pickled_metrics` - all evaluation metrics pickled separately for each algorithm and CV dataset combo (for this 'replication dataset')
                    * `statistical_comparisons` - all statistical significance results comparing algorithm performance (for this 'replication dataset')
    * `jobs` - contains cluster job submission files *(empty if output 'cleaning' applied)*
    * `jobsCompleted` - contains cluster checks for job completion *(empty if output 'cleaning' applied)*
    * `logs` - contains cluster job output and error logs *(empty if output 'cleaning' applied)*

***
## Output File Details

### PDF Report(s)
#### Testing Evaluation Report
When you first open the experiment folder, you will find the file `demo_experiment_ML_Pipeline_Report.pdf`. This is an automatically formatted PDF summarizing key findings during the model training and evaluation. It conveniently documents all STREAMLINE run parameters, and summarizes key results for data processing, processed data EDA, model evaluation, feature importance, algorithm comparisons, dataset comparisons, and runtime.

#### Replication Evaluation Report
A simpler 'replication report' is generated for each 'replication dataset' applied to the models trained by a single 'target dataset'. You can find the demo replication report at the following path: `/demo_experiment/hcc-data_example_custom/replication/hcc-data_example_custom_rep/demo_experiment_ML_Pipeline_Replication_Report.pdf`. This report differs from the testing evaluation report in that it excludes the following irrelevant elements: (1) univariate analysis summary, (2) feature importance summary, (3) dataset comparison summary, and (4) runtime summary.

### Experiment Meta Info
When you first open the experiment folder, you will also find `algInfo.pickle` and `metadata.pickle` which are used internally by STREAMLINE across all phases, as well as by the 'Useful Notebooks', covered in [Doing More with STREAMLINE](more.md#doing-more-with-streamline).

### DatasetComparisons
At the beginning of the [testing evaluation report](#testing-evaluation-report), each dataset is assigned an abbreviated designation of 'D#' (e.g. D1, D2, etc) based on the alphabetical order of each dataset name. These designations are used in some of the files included within this folder.

#### Statistical Significance Comparisons
When you first open this folder you will find `.csv` files containing all statistical significance results comparing modeling performance across two or more 'target datasets' run at once with STREAMLINE. 

STREAMLINE applies three non-parametric tests of significance: 
1. [Kruskal Wallis one-way analysis of variance](https://en.wikipedia.org/wiki/Kruskal%E2%80%93Wallis_one-way_analysis_of_variance) - used for comparing two or more independent samples of equal or different sample sizes
2. [Mann-Whitney U test](https://en.wikipedia.org/wiki/Mann%E2%80%93Whitney_U_test) (aka Wicoxon rank-sum test) - used for pair-wise independent sample comparisons
3. [Wilcoxon signed-rank test](https://en.wikipedia.org/wiki/Wilcoxon_signed-rank_test) - used for pair-wise dependent sample comparisons.

The `.csv` files in this folder include the above significance tests' results comparing model performance between 'target datasets':
* `BestCompare` files: (1) apply the given test to each evalution metric, (2) only compares the models from the 'top-performing-algorithm' for a given metric/dataset - determined by which had the best median metric value for in a 'sample' (3)  where a 'sample' is the set of *k* trained CV models for a given algorithm
* `KruskalWallis` files: (1) applies Kruskal Wallis to each evalution metric for a given algorithm across dataset 'samples', (2) where a 'sample' is the set of *k* trained CV models for that algorithm
* `MannWhitney` files: (1) applies Mann-Whitney to each evalution metric for a given algorithm examining pairs of dataset 'samples', (2) where a 'sample' is the set of *k* trained CV models for that algorithm
* `WilcoxonRank` files: (1) applies Wilcoxon to each evalution metric for a given algorithm examining pairs of dataset 'samples', (2) where a 'sample' is the set of *k* trained CV models for that algorithm

#### dataCompBoxplots
This folder contains two different types of box plots comparing dataset performance:
1. `DataCompare`: (1) one plot for each combination of algorithm + evaluation metric (only ROC-AUC, and PRC-AUC metrics), (2) the 'sample' making up each individual box-and-whisker is the set of *k* trained CV models for that algorithm
2. `DataCompareAllModels`: (1) one plot for each evaluation metric (all 16 classification metrics), (2) the 'sample' making up each individual box-and-whisker is the set of median algorithm performances, (3) lines are overlaid on the boxplot to illustrate differences in median performance between datasets for all algorithms.

[Under Construction Below]

all statistical significance results and plots for comparing modeling performance across multiple 'target datasets' run
Within this folder you should find the following:

* `<experiment_name>_ML_Pipeline_Report.pdf` [File]: This is an automatically formatted PDF summarizing key findings during the model training and evaluation. A great place to start!
* `metadata.csv` [File]: Another way to view the STREAMLINE parameters used by the pipeline.  These are also organized on the first page of the PDF report.
* `metadata.pickle` [File]: A binary 'pickle' file of the metadata for easy loading by the 11 pipeline phases. (For more experienced users)
* `algInfo.pickle` [File]: A binary 'pickle' file including a dictionary indicating which ML algorithms were used, along with abbreviations of names for figures/filenames, and colors to use for each algorithm in figures. (For more experienced users)
* `DatasetComparisons` [Folder]: Containing figures and statistical significance comparisons between the two datasets that were analyzed with STREAMLINE. (This folder only appears if more than one dataset was included in the user specified data folder, i.e. `data_path`, and phase 7 of STREAMLINE was run). Within the PDF summary, each dataset is assigned an abbreviated designation of 'D#' (e.g. D1, D2, etc) based on the alphabetical order of each dataset name. These designations are used in some of the files included within this folder.
* [Folders] - A folder for each of the two datasets analyzed (in this demo there were two: `demodata` and `hcc-data_example_no_covariates`). These folders include all results and models respective to each dataset. We summarize the contents of each folder below (feel free to skip this for now and revisit it as needed)...
    * `exploratory` [Folder]: Includes all exploratory analysis summaries and figures after basic transformation.
      * `initial` [Folder]: Includes all exploratory analysis before data transformation.
    * `CVDatasets` [Folder]: Includes all individual training and testing datasets (as .csv files) generated.
        * Note: These are the datasets passed to modeling so if imputation and scaling was conducted, these datasets will have been partitioned, imputed, and scaled.
    * `scale_impute` [Folder]: Includes all pickled files preserving how scaling and/or imputation was conducted based on respective training datasets.
    * `feature_selection` [Folder]: Includes feature importance and selection summaries and figures.
    * `models` [Folder]: Includes the ML algorithm hyperparameters selected by Optuna for each CV partition and modeling algorithm, as well as pickled files storing all models for future use.
    * `model_evaluation` [Folder]: Includes all model evaluation results, summaries, figures, and statistical comparisons.
    * `applymodel` [Folder]: Includes all model evaluation results when applied to a hold out replication datasets. This includes a new PDF summary of models when applied to this further hold-out dataset.
        * Note: In the demonstration analysis we only created and applied a replication dataset for `hcc-data_example`. Therefore, this folder only appears in output folder for `hcc-data_example`.
    * `runtimes.csv` [File]: Summary file giving the total runtimes spent on each phase or ML modeling algorithm in STREAMLINE.


## Figures Summary
Below is an example overview of the different figures generated by STREAMLINE for binary classification data. Note that these current images were generated from the Beta 0.2.5 release, and have been improved, updated and expanded since.

![alttext](pictures/STREAMLINE_Figures.png)


