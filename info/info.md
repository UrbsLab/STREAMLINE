#  About (FAQs)

## What level of computing skill is required for use?
STREAMLINE offers a variety of use options making it accessible to those with little or no coding experience as well as 
the seasoned programmer/data scientist. While there is currently no graphical user interface (GUI), 
the most naive user need only know how to navigate their PC file system, specify folder/file paths, un-zip a folder, 
and have or set up a Google Drive account. 
Those with a very basic knowledge of python and computer environments can apply 
STREAMLINE on Google Colab within the included jupyter notebook, and those 
with a bit more experience can run it serially by command line or in parallel 
(as is) on a compatible Linux computing clusters.

***
## What can it be used for?
STREAMLINE can be used as:
1. A tool to quickly run a rigorous ML data analysis over one or more datasets using one or more of the well-known or in-development modeling algorithms included
2. A framework to compare established scikit-learn compatible ML modeling algorithms to each other or to new algorithms
3. A baseline standard of comparison (i.e. positive control) with which to evaluate other AutoML tools that seek to optimize ML pipeline assembly as part of their methodology
4. A framework to quickly run an exploratory analysis and/or feature importance estimation/feature selection prior to using some other methodology for ML modeling
5. An educational example of how to integrate some of the many amazing Python-based data science tools currently available (in particular pandas, scipy, optuna, and scikit-learn).
6. A framework from which to create a new, expanded, adapted, or modified ML analysis pipeline
7. A framework to test new algorithms and identify less obvious bugs (i.e. those that don't prevent the algorithm from running to completion)

***
## What does STREAMLINE include?
The automated elements of STREAMLINE includes 
1) exploratory analysis
2) basic data cleaning
3) cross validation (CV) partitioning, scaling, imputation
4) filter-based feature importance estimation
5) collective feature selection
6) modeling with 'Optuna' hyperparameter optimization across 15 implemented ML algorithms 
7) testing evaluations with 15 classification metrics, and model feature importance estimation
8) automatic saving of all results, models, and publication-ready plots (including proposed composite feature importance plots)
9) non-parametric statistical comparisons across ML algorithms and analyzed datasets
10) automatically generated PDF summary reports.

The following 15 scikit-learn compatible ML modeling algorithms are currently included as options: 
Naive Bayes (NB), Logistic Regression (LR), Decision Tree (DT), Random Forest (RF), 
Gradient Boosting (GB), XGBoost (XGB), LGBoost (LGB), CatBoost (CGB), 
Support Vector Machine (SVM), Artificial Neural Network (ANN), 
K-Nearest Neighbors (k-NN), Genetic Programming (GP), 
Eductional Learning Classifier System (eLCS), 
'X' Classifier System (XCS), and 
Extended Supervised Tracking and Classifying System (ExSTraCS). 

Classification-relevant hyperparameter values and ranges have carefully 
selected for each and are pre-specified for the automated (Optuna-driven) 
automated hyperparameter sweep.

The automatically formatted PDF reports generated by STREAMLINE are intended 
to give a brief summary of pipeline settings and key results. 
A folder containing all results, statistical analyses publication-ready plots/figures, 
models, and other outputs is saved allowing users to carefully examine all aspects of 
analysis performance.  We have also included a variety of useful Jupyter Notebooks 
designed to operate on this output folder giving users quick paths to do even more 
with the pipeline output. 

Examples include: (1) Accessing prediction probabilities, 
(2) regenerating figures with custom tweaks, 
(3) trying out the effect of different prediction thresholds on selected 
models with an interactive slider, 
(4) re-evaluating models using a new prediction threshold, and 
(5) generating an interactive model feature importance ranking visualization across 
all ML algorithms. 
STREAMLINE is completely reproducible when the `timeout` parameter is set to `None`, 
ensuring training of the same models with the same performance whenever the same datasets, 
pipeline settings, and random seed are used. 
STREAMLINE output can vary somewhat (particularly when parallelized) 
since Optuna (for hyperparameter optimization) may not complete the same 
number of optimization trials within the user specified time limit on different 
computing resources. However, Optuna helps ensure STREAMLINE run completion 
within a reasonable time frame. STREAMLINE also outputs all CV training/testing 
datasets generated, along with relevant scaling and imputation objects so that 
users can easily run their own comparable ML analyses outside of STREAMLINE.

This pipeline does NOT automate the following elements, as they are still best 
completed by human experts: (1) feature engineering, or feature construction, 
(2) feature encoding (e.g. apply one-hot-encoding to categorical features, or 
numerically encode text-based feature values), (3) account for bias in data 
collection, or (4) anything beyond simple data cleaning (i.e. the pipeline 
only removes instances with no class label, or where all feature values are missing). 
We recommend users consider conducting these items, as needed, prior to applying STREAMLINE.

***
## How is STREAMLINE different from other AutoML tools?
Unlike most other AutoML tools, STREAMLINE was designed as a framework to rigorously apply 
and compare a variety of ML modeling algorithms and collectively learn from them as opposed 
to identifying a best performing model and/or attempting to optimize the analysis pipeline 
configuration itself. STREAMLINE adopts a fixed series of purposefully selected steps/phases 
in line with data science best practices. It seeks to automate all domain generalizable 
elements of an ML analysis pipeline with a specific focus on biomedical data mining challenges. 
This tool can be run or utilized in a number of ways to suite a variety experience levels and
levels of problem/data complexity.

***
## STREAMLINE Run Modes 
This multi-phase pipeline has been set up to run in one of four ways:

1. A 'Notebook' within Google Colaboratory [Almost Anyone]:
    * Advantages
      * No coding or PC environment experience needed
      * Computing can be performed directly on Google Cloud
      * One-click run of whole pipeline
    * Disadvantages:
      * Can only run pipeline serially
      * slowest of the run options
      * limited by google cloud computing allowances
    * Notes: Requires a Google and Google Drive account (free)

2. A Jupyter Notebook (included) [Basic Experience]:
    * Advantages:
      * Allows easy customizability of nearly all aspects of the pipeline with minimal coding/environment experience, 
      * offers in-notebook viewing of results
      * offers in-notebook documentation of the run phases
      * one-click run of whole pipeline
    * Disadvantages:
      * Can only run pipeline serially
      * slower runtime than from command-line
    * Notes: Requires Anaconda3, Python3, and several other minor Python package installations

3. Locally from the command line [Command-line Users]:
    * Advantages: 
      * Typically runs faster than within Jupyter Notebook
      * an easier more versatile option for those with command-line experience
    * Disadvantages: 
      * Can only run pipeline serially
      * command-line experience recommended
    * Notes: Requires Python3, and several other minor Python package installations

4. On HPC Clusters from command line (Currently only tested on SLURM Clusters) [Computing Cluster Users]:
    * Advantages: 
      * By far the fastest, most efficient way to run STREAMLINE
      * offers ability to run STREAMLINE over 7 types of HPC systems
    * Disadvantages: 
      * Experience with command-line recommended
      * access to a computing cluster required
    * Notes: Requires Python3, and several other minor Python package installations. Parallelization occurs within phases. Individual phases must be run in sequence.

Cluster runs of STREAMLINE were set up using `dask-jobqueue` and can support 7 types of clusters as described in the [dask documentation](https://jobqueue.dask.org/en/latest/api.html)
More setting may need to be done for non SLURM clusters.

## Implementation Highlights
* Overview
* * Pipeline includes reliable default run parameters that can be adjusted for further customization.
* * Easily compare ML performance between multiple target datasets (e.g. with different feature subsets)
* * Easily conduct an exploratory analysis including: (1) basic dataset characteristics: data dimensions, feature stats, missing value counts, and class balance, (2) detection of categorical vs. quantiative features, (3) feature correlation (with heatmap), and (4) univariate analyses with Chi-Square (categorical features), or Mann-Whitney U-Test (quantitative features).
* Preprocessing
* * Option to manually specify which features to treat as categorical vs. quantitative.
* * Option to manually specify features in loaded dataset to ignore in analysis.
* * Option to utilize 'group' cross validation partitioning: Case/control pairs or groups that have been matched based on one or more covariates will be kept together within CV data partitions.
* * Imputation is completed using mode imputation for categorical variables first, followed by MICE-based iterative imputation for quantitaive features. There is an option to use mean imputation for quantitative features when imputation computing cost is prohibitive in large datasets.
* Feature Importance and Selection
* * Data scaling, imputation, and feature selection are all conducted within respective CV partitions to prevent data leakage (i.e. testing data is not seen for any aspect of learning until final model evaluation).
* * The scaling, imputation, and feature selection data transformations (based only on the training data) are saved (i.e. 'pickled') so that they can be applied in the same way to testing partitions, and in the future to any replication data.
* * Collective feature selection is used: Both mutual information (proficient at detecting univariate associations) and MultiSURF (a Relief-based algorithm proficient at detecting both univariate and epistatic interactions) are run, and features are only removed from consideration if both algorithms fail to detect an informative signal (i.e. score > 0). This ensures that interacting features that may have no univariate association with class are not removed from the data prior to modeling.
* * Automatically outputs average feature importance bar-plots from feature importance/feature selection phase.
* * Since MultiSURF scales linearly with # of features and quadratically with # of instances, there is an option to select a random instance subset for MultiSURF scoring to reduce computational burden.
* Modeling
* * Includes 3 rule-based machine learning algorithms: ExSTraCS, XCS, and eLCS (to run optionally). These 'learning classifier systems' have been demonstrated to be able to detect complex associations while providing human interpretable models in the form of IF:THEN rule-sets. The ExSTraCS algorithm was developed by our research group to specifically handle the challenges of scalability, noise, and detection of epistasis and genetic heterogeneity in biomedical data mining.  
* * Utilizes the 'optuna' package to conduct automated Bayesian hyperparameter optimization during modeling (and optionally outputs plots summarizing the sweep).
* * We have sought to specify a comprehensive range of relevant hyperparameter options for all included ML algorithms.
* * Some ML algorithms that have a build in strategy to gather model feature importance estimates (i.e. LR,DT,RF,XGB,LGB,GB,eLCS,XCS,ExSTraCS) These can be used in place of permutation feature importance estimates by setting the parameter `use_uniform_fi` to 'False'.
* * All other algorithms rely on estimating feature importance using permutation feature importance.
* * All models are evaluated, reporting 16 classification metrics: Accuracy, Balanced Accuracy, F1 Score, Sensitivity(Recall), Specificity, Precision (PPV), True Positives (TP), True Negatives (TN), False Positives (FP), False Negatives (FN), Negative Predictive Value (NPV), Likeliehood Ratio + (LR+), Likeliehood Ratio - (LR-), ROC AUC, PRC AUC, and PRC APS.
* * All models are saved as 'pickle' files so that they can be loaded and reapplied in the future.
* Post-Analysis
* * Outputs ROC and PRC plots for each ML modeling algorithm displaying individual n-fold CV runs and average the average curve.
* * Outputs boxplots for each classification metric comparing ML modeling performance (across n-fold CV).
* * Outputs boxplots of feature importance estimation for each ML modeling algorithm (across n-fold CV).
* * Outputs our proposed 'composite feature importance plots' to examine feature importance estimate consistency (or lack of consistency) across all ML models (i.e. all algorithms)
* * Outputs summary ROC and PRC plots comparing average curves across all ML algorithms.
* * Collects run-time information on each phase of the pipeline and for the training of each ML algorithm model.
* * For each dataset, Kruskall-Wallis and subsequent pairwise Mann-Whitney U-Tests evaluates statistical significance of ML algorithm modeling performance differences for all metrics.
* * The same statistical tests (Kruskall-Wallis and Mann-Whitney U-Test) are conducted comparing datasets using the best performing modeling algorithm (for a given metric and dataset).
* * Outputs boxplots comparing performance of multiple datasets analyzed either across CV runs of a single algorithm and metric, or across average for each algorithm for a single metric.
* * A formatted PDF report is automatically generated giving a snapshot of all key pipeline results.
* * A script is included to apply all trained (and 'pickled') models to an external replication dataset to further evaluate model generalizability. This script (1) conducts an exploratory analysis of the new dataset, (2) uses the same scaling, imputation, and feature subsets determined from n-fold cv training, yielding 'n' versions of the replication dataset to be applied to the respective models, (3) applies and evaluates all models with these respective versions of the replication data, (4) outputs the same set of aforementioned boxplots, ROC, and PRC plots, and (5) automatically generates a new, formatted PDF report summarizing these applied results.

# Development notes 
Have ideas on how to improve this pipeline? We welcome suggestions, contributions, and collaborations.

## Change Log
The current version of STREAMLINE has the following additional features that are unique and noteworthy:
1. The ability to add new models by making a python file in `streamine/models/` based on the base model template.
2. The ability to run 7 different types of HPC clusters using `dask_jobqueue`
   as documented in its [documentation](https://jobqueue.dask.org/en/latest/api.html)
3. Ability to run the whole pipeline as a single command, which is now the primary method of operation.
4. Support for running using a configuration file instead of commandline parameters.

## History
The current version of STREAMLINE is based on our initial STREAMLINE project release Beta 0.2.5, and has since undergone major refactoring
STREAMLINE's codebase and functionalities have been reorganized and extended, along with the name rebranding. 
This STREAMLINE repository will be developed further in the future while the older version will be moved to a separate branch.

## Planned extensions/improvements

### Known issues
* Repair probable bugs in eLCS and XCS ML modeling algorithms (outside of STREAMLINE). Currently, we have intentionally set both to 'False' by default, so they will not run unless user explicitly turns them on)
* Set up STREAMLINE to be able to run (as an option) through all phases even if some CV model training runs have failed (as an option)
* Optuna currently prevents a guarantee of reproducibility of STREAMLINE when run in parallel, unless the user specifies `None` for the `timeout` parameter. This is explained in the Optuna documentation as an inherent result of running Optuna in parallel, since it is possible for a different optimal configuration to be found if a greater number of optimization trials are completed from one run to the next. We will consider alternative strategies for running STREAMLINE hyperparameter optimization as options in the future.
* Optuna generated visualization of hyper-parameter sweep results fails to operate correctly under certain situations (i.e. for GP most often, and for LR when using a version of Optuna other than 2.0.0)  It looks like Optuna developers intend to fix these issues in the future, and we will update STREAMLINE accordingly when they do.

### Logistical extensions
* Set up code to be run easily on cloud computing options such as AWS, Azure, or Google Cloud
* Set up option to use STREAMLINE within Docker - In Progress/TODO

### Capabilities extensions
* Support multiclass and quantitative endpoints
    * Will require significant extensions to most phases of the pipeline including exploratory analysis, CV partitioning, feature importance/selection, modeling, statistics analysis, and visualizations
* Shapley value calculation and visualizations
* Create ensemble model from all trained models which can then be evaluated on hold out replication data
* Expand available model visualization opportunities for model interpretation (i.e. Logistic Regression)
* Improve Catboost integration:
    * Allow it to use internal feature importance estimates as an option
    * Give it the list of features to be treated as categorical
* New code providing even more post-run data visualizations and customizations
* Clearly identify which algorithms can be run with missing values present, when user does not wish to apply `impute_data` (not yet fully tested)
* Create a smarter approach to hyper-parameter optimization: (1) avoid hyperparameter combinations that are invalid (i.e. as seen when using Logistic Regression), (2) intelligently exclude key hyperparameters known to improve overall performance as they get larger, and apply a user defined value for these in the final model training after all other hyperparameters have been optimized (i.e. evolutionary algorithms such as genetic programming and ExSTraCS almost always benefit from larger population sizes and learning cycles. Given that we know these parameters improve performance, including them in hyperparameter optimization only slows down the process with little informational gain)

### Algorithmic extensions
* Refinement of pre-configured ML algorithm hyperparameter options considered using Optuna
* Expanded feature importance estimation algorithm options and improved, more flexible feature selection strategy improving high-order feature interaction detection
* New rule-based machine learning algorithm (in development)

# Guidelines for Setting Parameters

## Reducing runtime
Conducting a more effective ML analysis typically demands a much larger amount of computing power and runtime. However, we provide general guidelines here for limiting overall runtime of a STREAMLINE experiment.
1. Run on a fewer number of datasets at once.
2. Run using fewer ML algorithms at once:
    * Naive Bayes, Logistic Regression, and Decision Trees are typically fastest.
    * Genetic Programming, eLCS, XCS, and ExSTraCS often take the longest (however other algorithms such as SVM, KNN, and ANN can take even longer when the number of instances is very large).
3. Run using a smaller number of `cv_partitions`.
4. Run without generating plots (i.e. `export_feature_correlations`, `export_univariate_plots`, `plot_PRC`, `plot_ROC`, `plot_FI_box`, `plot_metric_boxplots`).
5. In large datasets with missing values, set `multi_impute` to 'False'. This will apply simple mean imputation to numerical features instead.
6. Set `use_TURF` as 'False'. However we strongly recommend setting this to 'True' in feature spaces > 10,000 in order to avoid missing feature interactions during feature selection.
7. Set `TURF_pct` no lower than 0.5.  Setting at 0.5 is by far the fastest, but it will operate more effectively in very large feature spaces when set lower.
8. Set `instance_subset` at or below 2000 (speeds up multiSURF feature importance evaluation at potential expense of performance).
9. Set `max_features_to_keep` at or below 2000 and `filter_poor_features` = 'True' (this limits the maximum number of features that can be passed on to ML modeling).
10. Set `training_subsample` at or below 2000 (this limits the number of sample used to train particularly expensive ML modeling algorithms). However avoid setting this too low, or ML algorithms may not have enough training instances to effectively learn.
11. Set `n_trials` and/or timeout to lower values (this limits the time spent on hyperparameter optimization).
12. If using eLCS, XCS, or ExSTraCS, set `do_lcs_sweep` to 'False', `iterations` at or below 200000, and `N` at or below 2000.

## Improving Modeling Performance
* Generally speaking, the more computational time you are willing to spend on ML, the better the results. Doing the opposite of the above tips for reducing runtime, will likely improve performance.
* In certain situations, setting `feature_selection` to 'False', and relying on the ML algorithms alone to identify relevant features will yield better performance.  However, this may only be computationally practical when the total number of features in an original dataset is smaller (e.g. under 2000).
* Note that eLCS, XCS, and ExSTraCS are newer algorithm implementations developed by our research group.  As such, their algorithm performance may not yet be optimized in contrast to the other well established and widely utilized options. These learning classifier system (LCS) algorithms are unique however, in their ability to model very complex associations in data, while offering a largely interpretable model made up of simple, human readable IF:THEN rules. They have also been demonstrated to be able to tackle both complex feature interactions as well as heterogeneous patterns of association (i.e. different features are predictive in different subsets of the training data).
* In problems with no noise (i.e. datasets where it is possible to achieve 100% testing accuracy), LCS algorithms (i.e. eLCS, XCS, and ExSTraCS) perform better when `nu` is set larger than 1 (i.e. 5 or 10 recommended).  This applies significantly more pressure for individual rules to achieve perfect accuracy.  In noisy problems this may lead to significant overfitting.

## Other Guidelines
* SVM and ANN modeling should only be applied when data scaling is applied by the pipeline.
* Logistic Regression' baseline model feature importance estimation is determined by the exponential of the feature's coefficient. This should only be used if data scaling is applied by the pipeline.  Otherwise `use_uniform_FI` should be True.
* While the STREAMLINE includes `impute_data` as an option that can be turned off in `DataPreprocessing`, most algorithm implementations (all those standard in scikit-learn) cannot handle missing data values with the exception of eLCS, XCS, and ExSTraCS. In general, STREAMLINE is expected to fail with an errors if run on data with missing values, while `impute_data` is set to 'False'.