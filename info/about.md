#  About (FAQs)

## What level of computing skill is required for use?
STREAMLINE offers a variety of use options making it accessible to those with little or no coding experience as well as 
the seasoned programmer/data scientist. While there is currently no graphical user interface (GUI), 
the most naive user need only know how to navigate their PC file system, specify folder/file paths, un-zip a folder, 
and have or set up a Google Drive account. 
Those with a very basic knowledge of python and computer environments can apply 
STREAMLINE on Google Colab within the included jupyter notebook, and those 
with a bit more experience can run it serially by command line or in parallel 
(as is) on a compatible Linux computing clusters.

***
## What can it be used for?
STREAMLINE can be used as:
1. A tool to quickly run a rigorous ML data analysis over one or more datasets using one or more of the well-known or in-development modeling algorithms included
2. A framework to compare established scikit-learn compatible ML modeling algorithms to each other or to new algorithms
3. A baseline standard of comparison (i.e. positive control) with which to evaluate other AutoML tools that seek to optimize ML pipeline assembly as part of their methodology
4. A framework to quickly run an exploratory analysis and/or feature importance estimation/feature selection prior to using some other methodology for ML modeling
5. An educational example of how to integrate some of the many amazing Python-based data science tools currently available (in particular pandas, scipy, optuna, and scikit-learn).
6. A framework from which to create a new, expanded, adapted, or modified ML analysis pipeline
7. A framework to test new algorithms and identify less obvious bugs (i.e. those that don't prevent the algorithm from running to completion)

***
## What does STREAMLINE include?
The automated elements of STREAMLINE includes 
1) exploratory analysis
2) basic data cleaning
3) cross validation (CV) partitioning, scaling, imputation
4) filter-based feature importance estimation
5) collective feature selection
6) modeling with 'Optuna' hyperparameter optimization across 15 implemented ML algorithms 
7) testing evaluations with 15 classification metrics, and model feature importance estimation
8) automatic saving of all results, models, and publication-ready plots (including proposed composite feature importance plots)
9) non-parametric statistical comparisons across ML algorithms and analyzed datasets
10) automatically generated PDF summary reports.

The following 15 scikit-learn compatible ML modeling algorithms are currently included as options: 
Naive Bayes (NB), Logistic Regression (LR), Decision Tree (DT), Random Forest (RF), 
Gradient Boosting (GB), XGBoost (XGB), LGBoost (LGB), CatBoost (CGB), 
Support Vector Machine (SVM), Artificial Neural Network (ANN), 
K-Nearest Neighbors (k-NN), Genetic Programming (GP), 
Eductional Learning Classifier System (eLCS), 
'X' Classifier System (XCS), and 
Extended Supervised Tracking and Classifying System (ExSTraCS). 

Classification-relevant hyperparameter values and ranges have carefully 
selected for each and are pre-specified for the automated (Optuna-driven) 
automated hyperparameter sweep.

The automatically formatted PDF reports generated by STREAMLINE are intended 
to give a brief summary of pipeline settings and key results. 
A folder containing all results, statistical analyses publication-ready plots/figures, 
models, and other outputs is saved allowing users to carefully examine all aspects of 
analysis performance.  We have also included a variety of useful Jupyter Notebooks 
designed to operate on this output folder giving users quick paths to do even more 
with the pipeline output. 

Examples include: (1) Accessing prediction probabilities, 
(2) regenerating figures with custom tweaks, 
(3) trying out the effect of different prediction thresholds on selected 
models with an interactive slider, 
(4) re-evaluating models using a new prediction threshold, and 
(5) generating an interactive model feature importance ranking visualization across 
all ML algorithms. 
STREAMLINE is completely reproducible when the `timeout` parameter is set to `None`, 
ensuring training of the same models with the same performance whenever the same datasets, 
pipeline settings, and random seed are used. 
STREAMLINE output can vary somewhat (particularly when parallelized) 
since Optuna (for hyperparameter optimization) may not complete the same 
number of optimization trials within the user specified time limit on different 
computing resources. However, Optuna helps ensure STREAMLINE run completion 
within a reasonable time frame. STREAMLINE also outputs all CV training/testing 
datasets generated, along with relevant scaling and imputation objects so that 
users can easily run their own comparable ML analyses outside of STREAMLINE.

This pipeline does NOT automate the following elements, as they are still best 
completed by human experts: (1) feature engineering, or feature construction, 
(2) feature encoding (e.g. apply one-hot-encoding to categorical features, or 
numerically encode text-based feature values), (3) account for bias in data 
collection, or (4) anything beyond simple data cleaning (i.e. the pipeline 
only removes instances with no class label, or where all feature values are missing). 
We recommend users consider conducting these items, as needed, prior to applying STREAMLINE.

***
## How is STREAMLINE different from other AutoML tools?
Unlike most other AutoML tools, STREAMLINE was designed as a framework to rigorously apply 
and compare a variety of ML modeling algorithms and collectively learn from them as opposed 
to identifying a best performing model and/or attempting to optimize the analysis pipeline 
configuration itself. STREAMLINE adopts a fixed series of purposefully selected steps/phases 
in line with data science best practices. It seeks to automate all domain generalizable 
elements of an ML analysis pipeline with a specific focus on biomedical data mining challenges. 
This tool can be run or utilized in a number of ways to suite a variety experience levels and
levels of problem/data complexity.

***
## STREAMLINE Run Modes 
This multi-phase pipeline has been set up to run in one of four ways:

1. A 'Notebook' within Google Colaboratory [Almost Anyone]:
    * Advantages
      * No coding or PC environment experience needed
      * Computing can be performed directly on Google Cloud
      * One-click run of whole pipeline
    * Disadvantages:
      * Can only run pipeline serially
      * slowest of the run options
      * limited by google cloud computing allowances
    * Notes: Requires a Google and Google Drive account (free)

2. A Jupyter Notebook (included) [Basic Experience]:
    * Advantages:
      * Allows easy customizability of nearly all aspects of the pipeline with minimal coding/environment experience, 
      * offers in-notebook viewing of results
      * offers in-notebook documentation of the run phases
      * one-click run of whole pipeline
    * Disadvantages:
      * Can only run pipeline serially
      * slower runtime than from command-line
    * Notes: Requires Anaconda3, Python3, and several other minor Python package installations

3. Locally from the command line [Command-line Users]:
    * Advantages: 
      * Typically runs faster than within Jupyter Notebook
      * an easier more versatile option for those with command-line experience
    * Disadvantages: 
      * Can only run pipeline serially
      * command-line experience recommended
    * Notes: Requires Python3, and several other minor Python package installations

4. On HPC Clusters from command line (Currently only tested on SLURM Clusters) [Computing Cluster Users]:
    * Advantages: 
      * By far the fastest, most efficient way to run STREAMLINE
      * offers ability to run STREAMLINE over 7 types of HPC systems
    * Disadvantages: 
      * Experience with command-line recommended
      * access to a computing cluster required
    * Notes: Requires Python3, and several other minor Python package installations. Parallelization occurs within phases. Individual phases must be run in sequence.

Cluster runs of STREAMLINE were set up using `dask-jobqueue` and can support 7 types of clusters as described in the [dask documentation](https://jobqueue.dask.org/en/latest/api.html)
More setting may need to be done for non SLURM clusters.

## Implementation Highlights
* Overview
* * Pipeline includes reliable default run parameters that can be adjusted for further customization.
* * Easily compare ML performance between multiple target datasets (e.g. with different feature subsets)
* * Easily conduct an exploratory analysis including: (1) basic dataset characteristics: data dimensions, feature stats, missing value counts, and class balance, (2) detection of categorical vs. quantiative features, (3) feature correlation (with heatmap), and (4) univariate analyses with Chi-Square (categorical features), or Mann-Whitney U-Test (quantitative features).
* Preprocessing
* * Option to manually specify which features to treat as categorical vs. quantitative.
* * Option to manually specify features in loaded dataset to ignore in analysis.
* * Option to utilize 'group' cross validation partitioning: Case/control pairs or groups that have been matched based on one or more covariates will be kept together within CV data partitions.
* * Imputation is completed using mode imputation for categorical variables first, followed by MICE-based iterative imputation for quantitaive features. There is an option to use mean imputation for quantitative features when imputation computing cost is prohibitive in large datasets.
* Feature Importance and Selection
* * Data scaling, imputation, and feature selection are all conducted within respective CV partitions to prevent data leakage (i.e. testing data is not seen for any aspect of learning until final model evaluation).
* * The scaling, imputation, and feature selection data transformations (based only on the training data) are saved (i.e. 'pickled') so that they can be applied in the same way to testing partitions, and in the future to any replication data.
* * Collective feature selection is used: Both mutual information (proficient at detecting univariate associations) and MultiSURF (a Relief-based algorithm proficient at detecting both univariate and epistatic interactions) are run, and features are only removed from consideration if both algorithms fail to detect an informative signal (i.e. score > 0). This ensures that interacting features that may have no univariate association with class are not removed from the data prior to modeling.
* * Automatically outputs average feature importance bar-plots from feature importance/feature selection phase.
* * Since MultiSURF scales linearly with # of features and quadratically with # of instances, there is an option to select a random instance subset for MultiSURF scoring to reduce computational burden.
* Modeling
* * Includes 3 rule-based machine learning algorithms: ExSTraCS, XCS, and eLCS (to run optionally). These 'learning classifier systems' have been demonstrated to be able to detect complex associations while providing human interpretable models in the form of IF:THEN rule-sets. The ExSTraCS algorithm was developed by our research group to specifically handle the challenges of scalability, noise, and detection of epistasis and genetic heterogeneity in biomedical data mining.  
* * Utilizes the 'optuna' package to conduct automated Bayesian hyperparameter optimization during modeling (and optionally outputs plots summarizing the sweep).
* * We have sought to specify a comprehensive range of relevant hyperparameter options for all included ML algorithms.
* * Some ML algorithms that have a build in strategy to gather model feature importance estimates (i.e. LR,DT,RF,XGB,LGB,GB,eLCS,XCS,ExSTraCS) These can be used in place of permutation feature importance estimates by setting the parameter `use_uniform_fi` to 'False'.
* * All other algorithms rely on estimating feature importance using permutation feature importance.
* * All models are evaluated, reporting 16 classification metrics: Accuracy, Balanced Accuracy, F1 Score, Sensitivity(Recall), Specificity, Precision (PPV), True Positives (TP), True Negatives (TN), False Positives (FP), False Negatives (FN), Negative Predictive Value (NPV), Likeliehood Ratio + (LR+), Likeliehood Ratio - (LR-), ROC AUC, PRC AUC, and PRC APS.
* * All models are saved as 'pickle' files so that they can be loaded and reapplied in the future.
* Post-Analysis
* * Outputs ROC and PRC plots for each ML modeling algorithm displaying individual n-fold CV runs and average the average curve.
* * Outputs boxplots for each classification metric comparing ML modeling performance (across n-fold CV).
* * Outputs boxplots of feature importance estimation for each ML modeling algorithm (across n-fold CV).
* * Outputs our proposed 'composite feature importance plots' to examine feature importance estimate consistency (or lack of consistency) across all ML models (i.e. all algorithms)
* * Outputs summary ROC and PRC plots comparing average curves across all ML algorithms.
* * Collects run-time information on each phase of the pipeline and for the training of each ML algorithm model.
* * For each dataset, Kruskall-Wallis and subsequent pairwise Mann-Whitney U-Tests evaluates statistical significance of ML algorithm modeling performance differences for all metrics.
* * The same statistical tests (Kruskall-Wallis and Mann-Whitney U-Test) are conducted comparing datasets using the best performing modeling algorithm (for a given metric and dataset).
* * Outputs boxplots comparing performance of multiple datasets analyzed either across CV runs of a single algorithm and metric, or across average for each algorithm for a single metric.
* * A formatted PDF report is automatically generated giving a snapshot of all key pipeline results.
* * A script is included to apply all trained (and 'pickled') models to an external replication dataset to further evaluate model generalizability. This script (1) conducts an exploratory analysis of the new dataset, (2) uses the same scaling, imputation, and feature subsets determined from n-fold cv training, yielding 'n' versions of the replication dataset to be applied to the respective models, (3) applies and evaluates all models with these respective versions of the replication data, (4) outputs the same set of aforementioned boxplots, ROC, and PRC plots, and (5) automatically generates a new, formatted PDF report summarizing these applied results.
