============================= test session starts ==============================
platform darwin -- Python 3.8.15, pytest-7.2.1, pluggy-1.0.0 -- /Users/bandheyh/opt/anaconda3/envs/streamline/bin/python
cachedir: .pytest_cache
rootdir: /Users/bandheyh/Local/Cedars/UrbsLab/STREAMLINE_Dev
plugins: pycodestyle-2.3.1, anyio-3.5.0, profiling-1.7.0
collecting ... collected 64 items / 6 skipped

setup.py::PYCODESTYLE SKIPPED (previously passed pycodestyle checks)     [  1%]
docs/source/conf.py::PYCODESTYLE SKIPPED (previously passed pycodest...) [  3%]
streamline/__init__.py::PYCODESTYLE SKIPPED (previously passed pycod...) [  4%]
streamline/dataprep/__init__.py::PYCODESTYLE SKIPPED (previously pas...) [  6%]
streamline/dataprep/data_process.py::PYCODESTYLE SKIPPED (previously...) [  7%]
streamline/dataprep/eda_runner.py::PYCODESTYLE SKIPPED (previously p...) [  9%]
streamline/dataprep/exploratory_analysis.py::PYCODESTYLE SKIPPED (pr...) [ 10%]
streamline/dataprep/kfold_partitioning.py::PYCODESTYLE SKIPPED (prev...) [ 12%]
streamline/featurefns/__init__.py::PYCODESTYLE SKIPPED (previously p...) [ 14%]
streamline/featurefns/feature_runner.py::PYCODESTYLE SKIPPED (previo...) [ 15%]
streamline/featurefns/importance.py::PYCODESTYLE SKIPPED (previously...) [ 17%]
streamline/featurefns/selection.py::PYCODESTYLE SKIPPED (previously ...) [ 18%]
streamline/modeling/__init__.py::PYCODESTYLE SKIPPED (previously pas...) [ 20%]
streamline/modeling/basemodel.py::PYCODESTYLE SKIPPED (previously pa...) [ 21%]
streamline/modeling/modeljob.py::PYCODESTYLE SKIPPED (previously pas...) [ 23%]
streamline/modeling/modelrunner.py::PYCODESTYLE SKIPPED (previously ...) [ 25%]
streamline/modeling/parameters.py::PYCODESTYLE SKIPPED (previously p...) [ 26%]
streamline/modeling/utils.py::PYCODESTYLE SKIPPED (previously passed...) [ 28%]
streamline/models/__init__.py::PYCODESTYLE SKIPPED (previously passe...) [ 29%]
streamline/models/artificial_neural_network.py::PYCODESTYLE SKIPPED      [ 31%]
streamline/models/decision_tree.py::PYCODESTYLE SKIPPED (previously ...) [ 32%]
streamline/models/genetic_programming.py::PYCODESTYLE SKIPPED (previ...) [ 34%]
streamline/models/gradient_boosting.py::PYCODESTYLE SKIPPED (previou...) [ 35%]
streamline/models/learning_based.py::PYCODESTYLE SKIPPED (previously...) [ 37%]
streamline/models/linear_model.py::PYCODESTYLE SKIPPED (previously p...) [ 39%]
streamline/models/naive_bayes.py::PYCODESTYLE SKIPPED (previously pa...) [ 40%]
streamline/models/neighbouring.py::PYCODESTYLE SKIPPED (previously p...) [ 42%]
streamline/models/random_forest.py::PYCODESTYLE SKIPPED (previously ...) [ 43%]
streamline/models/support_vector_machine.py::PYCODESTYLE SKIPPED (pr...) [ 45%]
streamline/postanalysis/__init__.py::PYCODESTYLE SKIPPED (previously...) [ 46%]
streamline/postanalysis/statistics.py::PYCODESTYLE SKIPPED (previous...) [ 48%]
streamline/tests/__init__.py::PYCODESTYLE SKIPPED (previously passed...) [ 50%]
streamline/tests/test_dataprep.py::PYCODESTYLE SKIPPED (previously p...) [ 51%]
streamline/tests/test_dataprocess.py::PYCODESTYLE SKIPPED (previousl...) [ 53%]
streamline/tests/test_edarunner.py::PYCODESTYLE SKIPPED (previously ...) [ 54%]
streamline/tests/test_featurefns.py::PYCODESTYLE SKIPPED (previously...) [ 56%]
streamline/tests/test_kfold.py::PYCODESTYLE SKIPPED (previously pass...) [ 57%]
streamline/tests/test_model.py::PYCODESTYLE SKIPPED (previously pass...) [ 59%]
streamline/tests/test_model_runner.py::PYCODESTYLE SKIPPED (previous...) [ 60%]
streamline/tests/test_model_runner.py::test_setup FAILED                 [ 62%]
streamline/tests/test_model_runner.py::test_valid_model_runner[algorithms0-False] 
-------------------------------- live log call ---------------------------------
WARNING  root:test_model_runner.py:63 Running Modelling Phase
FAILED                                                                   [ 64%]
streamline/tests/test_model_runner.py::test_valid_model_runner[algorithms1-False] 
-------------------------------- live log call ---------------------------------
WARNING  root:test_model_runner.py:63 Running Modelling Phase
FAILED                                                                   [ 65%]
streamline/tests/test_model_runner.py::test_valid_model_runner[algorithms2-True] 
-------------------------------- live log call ---------------------------------
WARNING  root:test_model_runner.py:63 Running Modelling Phase
FAILED                                                                   [ 67%]
streamline/tests/test_model_runner.py::test_valid_model_runner[algorithms3-True] 
-------------------------------- live log call ---------------------------------
WARNING  root:test_model_runner.py:63 Running Modelling Phase
FAILED                                                                   [ 68%]
streamline/tests/test_model_runner.py::test_valid_model_runner[algorithms4-True] 
-------------------------------- live log call ---------------------------------
WARNING  root:test_model_runner.py:63 Running Modelling Phase
FAILED                                                                   [ 70%]
streamline/tests/test_model_runner.py::test_valid_model_runner[algorithms5-True] 
-------------------------------- live log call ---------------------------------
WARNING  root:test_model_runner.py:63 Running Modelling Phase
FAILED                                                                   [ 71%]
streamline/tests/test_model_runner.py::test_valid_model_runner[algorithms6-True] 
-------------------------------- live log call ---------------------------------
WARNING  root:test_model_runner.py:63 Running Modelling Phase
FAILED                                                                   [ 73%]
streamline/tests/test_model_runner.py::test_valid_model_runner[algorithms7-True] 
-------------------------------- live log call ---------------------------------
WARNING  root:test_model_runner.py:63 Running Modelling Phase
FAILED                                                                   [ 75%]
streamline/tests/test_model_runner.py::test_valid_model_runner[algorithms8-True] 
-------------------------------- live log call ---------------------------------
WARNING  root:test_model_runner.py:63 Running Modelling Phase
FAILED                                                                   [ 76%]
streamline/tests/test_model_runner.py::test_valid_model_runner[algorithms9-True] 
-------------------------------- live log call ---------------------------------
WARNING  root:test_model_runner.py:63 Running Modelling Phase
FAILED                                                                   [ 78%]
streamline/tests/test_model_runner.py::test_valid_model_runner[algorithms10-True] 
-------------------------------- live log call ---------------------------------
WARNING  root:test_model_runner.py:63 Running Modelling Phase
FAILED                                                                   [ 79%]
streamline/tests/test_model_runner.py::test_valid_model_runner[algorithms11-True] 
-------------------------------- live log call ---------------------------------
WARNING  root:test_model_runner.py:63 Running Modelling Phase
FAILED                                                                   [ 81%]
streamline/tests/test_model_runner.py::test_valid_model_runner[algorithms12-True] 
-------------------------------- live log call ---------------------------------
WARNING  root:test_model_runner.py:63 Running Modelling Phase
FAILED                                                                   [ 82%]
streamline/tests/test_model_runner.py::test_valid_model_runner[algorithms13-True] 
-------------------------------- live log call ---------------------------------
WARNING  root:test_model_runner.py:63 Running Modelling Phase
FAILED                                                                   [ 84%]
streamline/tests/test_model_runner.py::test_valid_model_runner[algorithms14-True] 
-------------------------------- live log call ---------------------------------
WARNING  root:test_model_runner.py:63 Running Modelling Phase
FAILED                                                                   [ 85%]
streamline/tests/test_model_runner.py::test_valid_model_runner[algorithms15-True] 
-------------------------------- live log call ---------------------------------
WARNING  root:test_model_runner.py:63 Running Modelling Phase
FAILED                                                                   [ 87%]
streamline/tests/test_model_runner.py::test_valid_model_runner[algorithms16-True] 
-------------------------------- live log call ---------------------------------
WARNING  root:test_model_runner.py:63 Running Modelling Phase
FAILED                                                                   [ 89%]
streamline/tests/test_zclean.py::PYCODESTYLE SKIPPED (previously pas...) [ 90%]
streamline/tests/test_zclean.py::test_stub FAILED                        [ 92%]
streamline/utils/__init__.py::PYCODESTYLE SKIPPED (previously passed...) [ 93%]
streamline/utils/dataset.py::PYCODESTYLE SKIPPED (previously passed ...) [ 95%]
streamline/utils/evaluation.py::PYCODESTYLE SKIPPED (previously pass...) [ 96%]
streamline/utils/job.py::PYCODESTYLE SKIPPED (previously passed pyco...) [ 98%]
streamline/utils/modelutils.py::PYCODESTYLE SKIPPED (previously pass...) [100%]

=================================== FAILURES ===================================
__________________________________ test_setup __________________________________

    def test_setup():
        if not os.path.exists(output_path):
            os.mkdir(output_path)
>       eda = EDARunner(dataset_path, output_path, experiment_name, exploration_list=None, plot_list=None,
                        class_label="Class", n_splits=5)

streamline/tests/test_model_runner.py:21: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
streamline/dataprep/eda_runner.py:107: in __init__
    raise e
streamline/dataprep/eda_runner.py:104: in __init__
    self.make_dir_tree()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <streamline.dataprep.eda_runner.EDARunner object at 0x7fd754039250>

    def make_dir_tree(self):
        """
        Checks existence of data folder path.
        Checks that experiment output folder does not already exist as well as validity of experiment_name parameter.
        Then generates initial output folder hierarchy.
        """
        # Check to make sure data_path exists and experiment name is valid & unique
        if not os.path.exists(self.data_path):
            raise Exception("Provided data_path does not exist")
        if os.path.exists(self.output_path + '/' + self.experiment_name):
>           raise Exception(
                "Error: A folder with the specified experiment name already exists at "
                "" + self.output_path + '/' + self.experiment_name + '. This path/folder name must be unique.')
E           Exception: Error: A folder with the specified experiment name already exists at ./tests//demo. This path/folder name must be unique.

streamline/dataprep/eda_runner.py:210: Exception
__________________ test_valid_model_runner[algorithms0-False] __________________

algorithms = ['NB'], run_parallel = False

    @pytest.mark.parametrize(
        ("algorithms", "run_parallel"),
        [
            (['NB'], False),
            (["LR"], False),
            # (["NB", "LR", "DT"], True),
            # (['CGB'], False),
            # (['LGB'], False),
            # (['XGB'], False),
            # (['GP'], False),
            # (['XCS'], True),
            # ([SUPPORTED_MODELS_SMALL[-1]], True),
    
        ]
        +
        [([algo], True) for algo in SUPPORTED_MODELS_SMALL]
    )
    def test_valid_model_runner(algorithms, run_parallel):
        start = time.time()
    
        logging.warning("Running Modelling Phase")
    
        optuna.logging.set_verbosity(optuna.logging.WARNING)
    
>       runner = ModelExperimentRunner(output_path, experiment_name, algorithms, save_plots=True)

streamline/tests/test_model_runner.py:67: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <streamline.modeling.modelrunner.ModelExperimentRunner object at 0x7fd72977e040>
output_path = './tests/', experiment_name = 'demo', algorithms = ['NB']
exclude = None, class_label = 'Class', instance_label = None
scoring_metric = 'balanced_accuracy', metric_direction = 'maximize'
training_subsample = 0, use_uniform_fi = True, n_trials = 200, timeout = 900
save_plots = True, do_lcs_sweep = False, lcs_nu = 1, lcs_n = 2000
lcs_iterations = 200000, lcs_timeout = 1200, random_state = None, n_jobs = None

    def __init__(self, output_path, experiment_name, algorithms=None, exclude=None, class_label="Class",
                 instance_label=None, scoring_metric='balanced_accuracy', metric_direction='maximize',
                 training_subsample=0, use_uniform_fi=True, n_trials=200,
                 timeout=900, save_plots=False, do_lcs_sweep=False, lcs_nu=1, lcs_n=2000, lcs_iterations=200000,
                 lcs_timeout=1200, random_state=None, n_jobs=None):
    
        """
        Args:
            output_path: path to output directory
            experiment_name: name of experiment (no spaces)
            algorithms: list of str of ML models to run
            scoring_metric: primary scikit-learn specified scoring metric used for hyperparameter optimization and \
                            permutation-based model feature importance evaluation, default='balanced_accuracy'
            metric_direction: direction to optimize the scoring metric in optuna, \
                              either 'maximize' or 'minimize', default='maximize'
            training_subsample: for long running algos (XGB,SVM,ANN,KNN), option to subsample training set \
                                (0 for no subsample, default=0)
            use_uniform_fi: overrides use of any available feature importance estimate methods from models, \
                            instead using permutation_importance uniformly, default=True
            n_trials: number of bayesian hyperparameter optimization trials using optuna \
                      (specify an integer or None) default=200
            timeout: seconds until hyperparameter sweep stops running new trials \
                    (Note: it may run longer to finish last trial started) \
                    If set to None, STREAMLINE is completely replicable, but will take longer to run \
                    default=900 i.e. 900 sec = 15 minutes default \
            save_plots: export optuna-generated hyperparameter sweep plots, default False
            do_lcs_sweep: do LCS hyper-param tuning or use below params, default=False
            lcs_nu: fixed LCS nu param (recommended range 1-10), set to larger value for data with \
                    less or no noise, default=1
            lcs_iterations: fixed LCS number of learning iterations param, default=200000
            lcs_n: fixed LCS rule population maximum size param, default=2000
            lcs_timeout: seconds until hyperparameter sweep stops for LCS algorithms, default=1200
    
        """
        # TODO: What does training subsample do
        self.cv_count = None
        self.dataset = None
        self.output_path = output_path
        self.experiment_name = experiment_name
        self.class_label = class_label
        self.instance_label = instance_label
    
        if algorithms is None:
            self.algorithms = SUPPORTED_MODELS
            if exclude is not None:
                for algorithm in exclude:
                    try:
                        self.algorithms.remove(algorithm)
                    except Exception:
                        logging.error("Unknown algorithm in exclude: " + str(algorithm))
        else:
            for algorithm in algorithms:
                assert is_supported_model(algorithm)
            self.algorithms = algorithms
    
        self.scoring_metric = scoring_metric
        self.metric_direction = metric_direction
        self.training_subsample = training_subsample
        self.uniform_fi = use_uniform_fi
        self.n_trials = n_trials
        self.timeout = timeout
        self.save_plots = save_plots
        self.do_lcs_sweep = do_lcs_sweep
        self.lcs_nu = lcs_nu
        self.lcs_n = lcs_n
        self.lcs_iterations = lcs_iterations
        self.lcs_timeout = lcs_timeout
    
        self.random_state = random_state
        self.n_jobs = n_jobs
    
        # Argument checks
        if not os.path.exists(self.output_path):
>           raise Exception("Output path must exist (from phase 1) before phase 4 can begin")
E           Exception: Output path must exist (from phase 1) before phase 4 can begin

streamline/modeling/modelrunner.py:90: Exception
------------------------------ Captured log call -------------------------------
WARNING  root:test_model_runner.py:63 Running Modelling Phase
__________________ test_valid_model_runner[algorithms1-False] __________________

algorithms = ['LR'], run_parallel = False

    @pytest.mark.parametrize(
        ("algorithms", "run_parallel"),
        [
            (['NB'], False),
            (["LR"], False),
            # (["NB", "LR", "DT"], True),
            # (['CGB'], False),
            # (['LGB'], False),
            # (['XGB'], False),
            # (['GP'], False),
            # (['XCS'], True),
            # ([SUPPORTED_MODELS_SMALL[-1]], True),
    
        ]
        +
        [([algo], True) for algo in SUPPORTED_MODELS_SMALL]
    )
    def test_valid_model_runner(algorithms, run_parallel):
        start = time.time()
    
        logging.warning("Running Modelling Phase")
    
        optuna.logging.set_verbosity(optuna.logging.WARNING)
    
>       runner = ModelExperimentRunner(output_path, experiment_name, algorithms, save_plots=True)

streamline/tests/test_model_runner.py:67: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <streamline.modeling.modelrunner.ModelExperimentRunner object at 0x7fd754039bb0>
output_path = './tests/', experiment_name = 'demo', algorithms = ['LR']
exclude = None, class_label = 'Class', instance_label = None
scoring_metric = 'balanced_accuracy', metric_direction = 'maximize'
training_subsample = 0, use_uniform_fi = True, n_trials = 200, timeout = 900
save_plots = True, do_lcs_sweep = False, lcs_nu = 1, lcs_n = 2000
lcs_iterations = 200000, lcs_timeout = 1200, random_state = None, n_jobs = None

    def __init__(self, output_path, experiment_name, algorithms=None, exclude=None, class_label="Class",
                 instance_label=None, scoring_metric='balanced_accuracy', metric_direction='maximize',
                 training_subsample=0, use_uniform_fi=True, n_trials=200,
                 timeout=900, save_plots=False, do_lcs_sweep=False, lcs_nu=1, lcs_n=2000, lcs_iterations=200000,
                 lcs_timeout=1200, random_state=None, n_jobs=None):
    
        """
        Args:
            output_path: path to output directory
            experiment_name: name of experiment (no spaces)
            algorithms: list of str of ML models to run
            scoring_metric: primary scikit-learn specified scoring metric used for hyperparameter optimization and \
                            permutation-based model feature importance evaluation, default='balanced_accuracy'
            metric_direction: direction to optimize the scoring metric in optuna, \
                              either 'maximize' or 'minimize', default='maximize'
            training_subsample: for long running algos (XGB,SVM,ANN,KNN), option to subsample training set \
                                (0 for no subsample, default=0)
            use_uniform_fi: overrides use of any available feature importance estimate methods from models, \
                            instead using permutation_importance uniformly, default=True
            n_trials: number of bayesian hyperparameter optimization trials using optuna \
                      (specify an integer or None) default=200
            timeout: seconds until hyperparameter sweep stops running new trials \
                    (Note: it may run longer to finish last trial started) \
                    If set to None, STREAMLINE is completely replicable, but will take longer to run \
                    default=900 i.e. 900 sec = 15 minutes default \
            save_plots: export optuna-generated hyperparameter sweep plots, default False
            do_lcs_sweep: do LCS hyper-param tuning or use below params, default=False
            lcs_nu: fixed LCS nu param (recommended range 1-10), set to larger value for data with \
                    less or no noise, default=1
            lcs_iterations: fixed LCS number of learning iterations param, default=200000
            lcs_n: fixed LCS rule population maximum size param, default=2000
            lcs_timeout: seconds until hyperparameter sweep stops for LCS algorithms, default=1200
    
        """
        # TODO: What does training subsample do
        self.cv_count = None
        self.dataset = None
        self.output_path = output_path
        self.experiment_name = experiment_name
        self.class_label = class_label
        self.instance_label = instance_label
    
        if algorithms is None:
            self.algorithms = SUPPORTED_MODELS
            if exclude is not None:
                for algorithm in exclude:
                    try:
                        self.algorithms.remove(algorithm)
                    except Exception:
                        logging.error("Unknown algorithm in exclude: " + str(algorithm))
        else:
            for algorithm in algorithms:
                assert is_supported_model(algorithm)
            self.algorithms = algorithms
    
        self.scoring_metric = scoring_metric
        self.metric_direction = metric_direction
        self.training_subsample = training_subsample
        self.uniform_fi = use_uniform_fi
        self.n_trials = n_trials
        self.timeout = timeout
        self.save_plots = save_plots
        self.do_lcs_sweep = do_lcs_sweep
        self.lcs_nu = lcs_nu
        self.lcs_n = lcs_n
        self.lcs_iterations = lcs_iterations
        self.lcs_timeout = lcs_timeout
    
        self.random_state = random_state
        self.n_jobs = n_jobs
    
        # Argument checks
        if not os.path.exists(self.output_path):
>           raise Exception("Output path must exist (from phase 1) before phase 4 can begin")
E           Exception: Output path must exist (from phase 1) before phase 4 can begin

streamline/modeling/modelrunner.py:90: Exception
------------------------------ Captured log call -------------------------------
WARNING  root:test_model_runner.py:63 Running Modelling Phase
__________________ test_valid_model_runner[algorithms2-True] ___________________

algorithms = ['NB'], run_parallel = True

    @pytest.mark.parametrize(
        ("algorithms", "run_parallel"),
        [
            (['NB'], False),
            (["LR"], False),
            # (["NB", "LR", "DT"], True),
            # (['CGB'], False),
            # (['LGB'], False),
            # (['XGB'], False),
            # (['GP'], False),
            # (['XCS'], True),
            # ([SUPPORTED_MODELS_SMALL[-1]], True),
    
        ]
        +
        [([algo], True) for algo in SUPPORTED_MODELS_SMALL]
    )
    def test_valid_model_runner(algorithms, run_parallel):
        start = time.time()
    
        logging.warning("Running Modelling Phase")
    
        optuna.logging.set_verbosity(optuna.logging.WARNING)
    
>       runner = ModelExperimentRunner(output_path, experiment_name, algorithms, save_plots=True)

streamline/tests/test_model_runner.py:67: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <streamline.modeling.modelrunner.ModelExperimentRunner object at 0x7fd729777790>
output_path = './tests/', experiment_name = 'demo', algorithms = ['NB']
exclude = None, class_label = 'Class', instance_label = None
scoring_metric = 'balanced_accuracy', metric_direction = 'maximize'
training_subsample = 0, use_uniform_fi = True, n_trials = 200, timeout = 900
save_plots = True, do_lcs_sweep = False, lcs_nu = 1, lcs_n = 2000
lcs_iterations = 200000, lcs_timeout = 1200, random_state = None, n_jobs = None

    def __init__(self, output_path, experiment_name, algorithms=None, exclude=None, class_label="Class",
                 instance_label=None, scoring_metric='balanced_accuracy', metric_direction='maximize',
                 training_subsample=0, use_uniform_fi=True, n_trials=200,
                 timeout=900, save_plots=False, do_lcs_sweep=False, lcs_nu=1, lcs_n=2000, lcs_iterations=200000,
                 lcs_timeout=1200, random_state=None, n_jobs=None):
    
        """
        Args:
            output_path: path to output directory
            experiment_name: name of experiment (no spaces)
            algorithms: list of str of ML models to run
            scoring_metric: primary scikit-learn specified scoring metric used for hyperparameter optimization and \
                            permutation-based model feature importance evaluation, default='balanced_accuracy'
            metric_direction: direction to optimize the scoring metric in optuna, \
                              either 'maximize' or 'minimize', default='maximize'
            training_subsample: for long running algos (XGB,SVM,ANN,KNN), option to subsample training set \
                                (0 for no subsample, default=0)
            use_uniform_fi: overrides use of any available feature importance estimate methods from models, \
                            instead using permutation_importance uniformly, default=True
            n_trials: number of bayesian hyperparameter optimization trials using optuna \
                      (specify an integer or None) default=200
            timeout: seconds until hyperparameter sweep stops running new trials \
                    (Note: it may run longer to finish last trial started) \
                    If set to None, STREAMLINE is completely replicable, but will take longer to run \
                    default=900 i.e. 900 sec = 15 minutes default \
            save_plots: export optuna-generated hyperparameter sweep plots, default False
            do_lcs_sweep: do LCS hyper-param tuning or use below params, default=False
            lcs_nu: fixed LCS nu param (recommended range 1-10), set to larger value for data with \
                    less or no noise, default=1
            lcs_iterations: fixed LCS number of learning iterations param, default=200000
            lcs_n: fixed LCS rule population maximum size param, default=2000
            lcs_timeout: seconds until hyperparameter sweep stops for LCS algorithms, default=1200
    
        """
        # TODO: What does training subsample do
        self.cv_count = None
        self.dataset = None
        self.output_path = output_path
        self.experiment_name = experiment_name
        self.class_label = class_label
        self.instance_label = instance_label
    
        if algorithms is None:
            self.algorithms = SUPPORTED_MODELS
            if exclude is not None:
                for algorithm in exclude:
                    try:
                        self.algorithms.remove(algorithm)
                    except Exception:
                        logging.error("Unknown algorithm in exclude: " + str(algorithm))
        else:
            for algorithm in algorithms:
                assert is_supported_model(algorithm)
            self.algorithms = algorithms
    
        self.scoring_metric = scoring_metric
        self.metric_direction = metric_direction
        self.training_subsample = training_subsample
        self.uniform_fi = use_uniform_fi
        self.n_trials = n_trials
        self.timeout = timeout
        self.save_plots = save_plots
        self.do_lcs_sweep = do_lcs_sweep
        self.lcs_nu = lcs_nu
        self.lcs_n = lcs_n
        self.lcs_iterations = lcs_iterations
        self.lcs_timeout = lcs_timeout
    
        self.random_state = random_state
        self.n_jobs = n_jobs
    
        # Argument checks
        if not os.path.exists(self.output_path):
>           raise Exception("Output path must exist (from phase 1) before phase 4 can begin")
E           Exception: Output path must exist (from phase 1) before phase 4 can begin

streamline/modeling/modelrunner.py:90: Exception
------------------------------ Captured log call -------------------------------
WARNING  root:test_model_runner.py:63 Running Modelling Phase
__________________ test_valid_model_runner[algorithms3-True] ___________________

algorithms = ['LR'], run_parallel = True

    @pytest.mark.parametrize(
        ("algorithms", "run_parallel"),
        [
            (['NB'], False),
            (["LR"], False),
            # (["NB", "LR", "DT"], True),
            # (['CGB'], False),
            # (['LGB'], False),
            # (['XGB'], False),
            # (['GP'], False),
            # (['XCS'], True),
            # ([SUPPORTED_MODELS_SMALL[-1]], True),
    
        ]
        +
        [([algo], True) for algo in SUPPORTED_MODELS_SMALL]
    )
    def test_valid_model_runner(algorithms, run_parallel):
        start = time.time()
    
        logging.warning("Running Modelling Phase")
    
        optuna.logging.set_verbosity(optuna.logging.WARNING)
    
>       runner = ModelExperimentRunner(output_path, experiment_name, algorithms, save_plots=True)

streamline/tests/test_model_runner.py:67: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <streamline.modeling.modelrunner.ModelExperimentRunner object at 0x7fd754086160>
output_path = './tests/', experiment_name = 'demo', algorithms = ['LR']
exclude = None, class_label = 'Class', instance_label = None
scoring_metric = 'balanced_accuracy', metric_direction = 'maximize'
training_subsample = 0, use_uniform_fi = True, n_trials = 200, timeout = 900
save_plots = True, do_lcs_sweep = False, lcs_nu = 1, lcs_n = 2000
lcs_iterations = 200000, lcs_timeout = 1200, random_state = None, n_jobs = None

    def __init__(self, output_path, experiment_name, algorithms=None, exclude=None, class_label="Class",
                 instance_label=None, scoring_metric='balanced_accuracy', metric_direction='maximize',
                 training_subsample=0, use_uniform_fi=True, n_trials=200,
                 timeout=900, save_plots=False, do_lcs_sweep=False, lcs_nu=1, lcs_n=2000, lcs_iterations=200000,
                 lcs_timeout=1200, random_state=None, n_jobs=None):
    
        """
        Args:
            output_path: path to output directory
            experiment_name: name of experiment (no spaces)
            algorithms: list of str of ML models to run
            scoring_metric: primary scikit-learn specified scoring metric used for hyperparameter optimization and \
                            permutation-based model feature importance evaluation, default='balanced_accuracy'
            metric_direction: direction to optimize the scoring metric in optuna, \
                              either 'maximize' or 'minimize', default='maximize'
            training_subsample: for long running algos (XGB,SVM,ANN,KNN), option to subsample training set \
                                (0 for no subsample, default=0)
            use_uniform_fi: overrides use of any available feature importance estimate methods from models, \
                            instead using permutation_importance uniformly, default=True
            n_trials: number of bayesian hyperparameter optimization trials using optuna \
                      (specify an integer or None) default=200
            timeout: seconds until hyperparameter sweep stops running new trials \
                    (Note: it may run longer to finish last trial started) \
                    If set to None, STREAMLINE is completely replicable, but will take longer to run \
                    default=900 i.e. 900 sec = 15 minutes default \
            save_plots: export optuna-generated hyperparameter sweep plots, default False
            do_lcs_sweep: do LCS hyper-param tuning or use below params, default=False
            lcs_nu: fixed LCS nu param (recommended range 1-10), set to larger value for data with \
                    less or no noise, default=1
            lcs_iterations: fixed LCS number of learning iterations param, default=200000
            lcs_n: fixed LCS rule population maximum size param, default=2000
            lcs_timeout: seconds until hyperparameter sweep stops for LCS algorithms, default=1200
    
        """
        # TODO: What does training subsample do
        self.cv_count = None
        self.dataset = None
        self.output_path = output_path
        self.experiment_name = experiment_name
        self.class_label = class_label
        self.instance_label = instance_label
    
        if algorithms is None:
            self.algorithms = SUPPORTED_MODELS
            if exclude is not None:
                for algorithm in exclude:
                    try:
                        self.algorithms.remove(algorithm)
                    except Exception:
                        logging.error("Unknown algorithm in exclude: " + str(algorithm))
        else:
            for algorithm in algorithms:
                assert is_supported_model(algorithm)
            self.algorithms = algorithms
    
        self.scoring_metric = scoring_metric
        self.metric_direction = metric_direction
        self.training_subsample = training_subsample
        self.uniform_fi = use_uniform_fi
        self.n_trials = n_trials
        self.timeout = timeout
        self.save_plots = save_plots
        self.do_lcs_sweep = do_lcs_sweep
        self.lcs_nu = lcs_nu
        self.lcs_n = lcs_n
        self.lcs_iterations = lcs_iterations
        self.lcs_timeout = lcs_timeout
    
        self.random_state = random_state
        self.n_jobs = n_jobs
    
        # Argument checks
        if not os.path.exists(self.output_path):
>           raise Exception("Output path must exist (from phase 1) before phase 4 can begin")
E           Exception: Output path must exist (from phase 1) before phase 4 can begin

streamline/modeling/modelrunner.py:90: Exception
------------------------------ Captured log call -------------------------------
WARNING  root:test_model_runner.py:63 Running Modelling Phase
__________________ test_valid_model_runner[algorithms4-True] ___________________

algorithms = ['DT'], run_parallel = True

    @pytest.mark.parametrize(
        ("algorithms", "run_parallel"),
        [
            (['NB'], False),
            (["LR"], False),
            # (["NB", "LR", "DT"], True),
            # (['CGB'], False),
            # (['LGB'], False),
            # (['XGB'], False),
            # (['GP'], False),
            # (['XCS'], True),
            # ([SUPPORTED_MODELS_SMALL[-1]], True),
    
        ]
        +
        [([algo], True) for algo in SUPPORTED_MODELS_SMALL]
    )
    def test_valid_model_runner(algorithms, run_parallel):
        start = time.time()
    
        logging.warning("Running Modelling Phase")
    
        optuna.logging.set_verbosity(optuna.logging.WARNING)
    
>       runner = ModelExperimentRunner(output_path, experiment_name, algorithms, save_plots=True)

streamline/tests/test_model_runner.py:67: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <streamline.modeling.modelrunner.ModelExperimentRunner object at 0x7fd72977c670>
output_path = './tests/', experiment_name = 'demo', algorithms = ['DT']
exclude = None, class_label = 'Class', instance_label = None
scoring_metric = 'balanced_accuracy', metric_direction = 'maximize'
training_subsample = 0, use_uniform_fi = True, n_trials = 200, timeout = 900
save_plots = True, do_lcs_sweep = False, lcs_nu = 1, lcs_n = 2000
lcs_iterations = 200000, lcs_timeout = 1200, random_state = None, n_jobs = None

    def __init__(self, output_path, experiment_name, algorithms=None, exclude=None, class_label="Class",
                 instance_label=None, scoring_metric='balanced_accuracy', metric_direction='maximize',
                 training_subsample=0, use_uniform_fi=True, n_trials=200,
                 timeout=900, save_plots=False, do_lcs_sweep=False, lcs_nu=1, lcs_n=2000, lcs_iterations=200000,
                 lcs_timeout=1200, random_state=None, n_jobs=None):
    
        """
        Args:
            output_path: path to output directory
            experiment_name: name of experiment (no spaces)
            algorithms: list of str of ML models to run
            scoring_metric: primary scikit-learn specified scoring metric used for hyperparameter optimization and \
                            permutation-based model feature importance evaluation, default='balanced_accuracy'
            metric_direction: direction to optimize the scoring metric in optuna, \
                              either 'maximize' or 'minimize', default='maximize'
            training_subsample: for long running algos (XGB,SVM,ANN,KNN), option to subsample training set \
                                (0 for no subsample, default=0)
            use_uniform_fi: overrides use of any available feature importance estimate methods from models, \
                            instead using permutation_importance uniformly, default=True
            n_trials: number of bayesian hyperparameter optimization trials using optuna \
                      (specify an integer or None) default=200
            timeout: seconds until hyperparameter sweep stops running new trials \
                    (Note: it may run longer to finish last trial started) \
                    If set to None, STREAMLINE is completely replicable, but will take longer to run \
                    default=900 i.e. 900 sec = 15 minutes default \
            save_plots: export optuna-generated hyperparameter sweep plots, default False
            do_lcs_sweep: do LCS hyper-param tuning or use below params, default=False
            lcs_nu: fixed LCS nu param (recommended range 1-10), set to larger value for data with \
                    less or no noise, default=1
            lcs_iterations: fixed LCS number of learning iterations param, default=200000
            lcs_n: fixed LCS rule population maximum size param, default=2000
            lcs_timeout: seconds until hyperparameter sweep stops for LCS algorithms, default=1200
    
        """
        # TODO: What does training subsample do
        self.cv_count = None
        self.dataset = None
        self.output_path = output_path
        self.experiment_name = experiment_name
        self.class_label = class_label
        self.instance_label = instance_label
    
        if algorithms is None:
            self.algorithms = SUPPORTED_MODELS
            if exclude is not None:
                for algorithm in exclude:
                    try:
                        self.algorithms.remove(algorithm)
                    except Exception:
                        logging.error("Unknown algorithm in exclude: " + str(algorithm))
        else:
            for algorithm in algorithms:
                assert is_supported_model(algorithm)
            self.algorithms = algorithms
    
        self.scoring_metric = scoring_metric
        self.metric_direction = metric_direction
        self.training_subsample = training_subsample
        self.uniform_fi = use_uniform_fi
        self.n_trials = n_trials
        self.timeout = timeout
        self.save_plots = save_plots
        self.do_lcs_sweep = do_lcs_sweep
        self.lcs_nu = lcs_nu
        self.lcs_n = lcs_n
        self.lcs_iterations = lcs_iterations
        self.lcs_timeout = lcs_timeout
    
        self.random_state = random_state
        self.n_jobs = n_jobs
    
        # Argument checks
        if not os.path.exists(self.output_path):
>           raise Exception("Output path must exist (from phase 1) before phase 4 can begin")
E           Exception: Output path must exist (from phase 1) before phase 4 can begin

streamline/modeling/modelrunner.py:90: Exception
------------------------------ Captured log call -------------------------------
WARNING  root:test_model_runner.py:63 Running Modelling Phase
__________________ test_valid_model_runner[algorithms5-True] ___________________

algorithms = ['RF'], run_parallel = True

    @pytest.mark.parametrize(
        ("algorithms", "run_parallel"),
        [
            (['NB'], False),
            (["LR"], False),
            # (["NB", "LR", "DT"], True),
            # (['CGB'], False),
            # (['LGB'], False),
            # (['XGB'], False),
            # (['GP'], False),
            # (['XCS'], True),
            # ([SUPPORTED_MODELS_SMALL[-1]], True),
    
        ]
        +
        [([algo], True) for algo in SUPPORTED_MODELS_SMALL]
    )
    def test_valid_model_runner(algorithms, run_parallel):
        start = time.time()
    
        logging.warning("Running Modelling Phase")
    
        optuna.logging.set_verbosity(optuna.logging.WARNING)
    
>       runner = ModelExperimentRunner(output_path, experiment_name, algorithms, save_plots=True)

streamline/tests/test_model_runner.py:67: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <streamline.modeling.modelrunner.ModelExperimentRunner object at 0x7fd754039e50>
output_path = './tests/', experiment_name = 'demo', algorithms = ['RF']
exclude = None, class_label = 'Class', instance_label = None
scoring_metric = 'balanced_accuracy', metric_direction = 'maximize'
training_subsample = 0, use_uniform_fi = True, n_trials = 200, timeout = 900
save_plots = True, do_lcs_sweep = False, lcs_nu = 1, lcs_n = 2000
lcs_iterations = 200000, lcs_timeout = 1200, random_state = None, n_jobs = None

    def __init__(self, output_path, experiment_name, algorithms=None, exclude=None, class_label="Class",
                 instance_label=None, scoring_metric='balanced_accuracy', metric_direction='maximize',
                 training_subsample=0, use_uniform_fi=True, n_trials=200,
                 timeout=900, save_plots=False, do_lcs_sweep=False, lcs_nu=1, lcs_n=2000, lcs_iterations=200000,
                 lcs_timeout=1200, random_state=None, n_jobs=None):
    
        """
        Args:
            output_path: path to output directory
            experiment_name: name of experiment (no spaces)
            algorithms: list of str of ML models to run
            scoring_metric: primary scikit-learn specified scoring metric used for hyperparameter optimization and \
                            permutation-based model feature importance evaluation, default='balanced_accuracy'
            metric_direction: direction to optimize the scoring metric in optuna, \
                              either 'maximize' or 'minimize', default='maximize'
            training_subsample: for long running algos (XGB,SVM,ANN,KNN), option to subsample training set \
                                (0 for no subsample, default=0)
            use_uniform_fi: overrides use of any available feature importance estimate methods from models, \
                            instead using permutation_importance uniformly, default=True
            n_trials: number of bayesian hyperparameter optimization trials using optuna \
                      (specify an integer or None) default=200
            timeout: seconds until hyperparameter sweep stops running new trials \
                    (Note: it may run longer to finish last trial started) \
                    If set to None, STREAMLINE is completely replicable, but will take longer to run \
                    default=900 i.e. 900 sec = 15 minutes default \
            save_plots: export optuna-generated hyperparameter sweep plots, default False
            do_lcs_sweep: do LCS hyper-param tuning or use below params, default=False
            lcs_nu: fixed LCS nu param (recommended range 1-10), set to larger value for data with \
                    less or no noise, default=1
            lcs_iterations: fixed LCS number of learning iterations param, default=200000
            lcs_n: fixed LCS rule population maximum size param, default=2000
            lcs_timeout: seconds until hyperparameter sweep stops for LCS algorithms, default=1200
    
        """
        # TODO: What does training subsample do
        self.cv_count = None
        self.dataset = None
        self.output_path = output_path
        self.experiment_name = experiment_name
        self.class_label = class_label
        self.instance_label = instance_label
    
        if algorithms is None:
            self.algorithms = SUPPORTED_MODELS
            if exclude is not None:
                for algorithm in exclude:
                    try:
                        self.algorithms.remove(algorithm)
                    except Exception:
                        logging.error("Unknown algorithm in exclude: " + str(algorithm))
        else:
            for algorithm in algorithms:
                assert is_supported_model(algorithm)
            self.algorithms = algorithms
    
        self.scoring_metric = scoring_metric
        self.metric_direction = metric_direction
        self.training_subsample = training_subsample
        self.uniform_fi = use_uniform_fi
        self.n_trials = n_trials
        self.timeout = timeout
        self.save_plots = save_plots
        self.do_lcs_sweep = do_lcs_sweep
        self.lcs_nu = lcs_nu
        self.lcs_n = lcs_n
        self.lcs_iterations = lcs_iterations
        self.lcs_timeout = lcs_timeout
    
        self.random_state = random_state
        self.n_jobs = n_jobs
    
        # Argument checks
        if not os.path.exists(self.output_path):
>           raise Exception("Output path must exist (from phase 1) before phase 4 can begin")
E           Exception: Output path must exist (from phase 1) before phase 4 can begin

streamline/modeling/modelrunner.py:90: Exception
------------------------------ Captured log call -------------------------------
WARNING  root:test_model_runner.py:63 Running Modelling Phase
__________________ test_valid_model_runner[algorithms6-True] ___________________

algorithms = ['GB'], run_parallel = True

    @pytest.mark.parametrize(
        ("algorithms", "run_parallel"),
        [
            (['NB'], False),
            (["LR"], False),
            # (["NB", "LR", "DT"], True),
            # (['CGB'], False),
            # (['LGB'], False),
            # (['XGB'], False),
            # (['GP'], False),
            # (['XCS'], True),
            # ([SUPPORTED_MODELS_SMALL[-1]], True),
    
        ]
        +
        [([algo], True) for algo in SUPPORTED_MODELS_SMALL]
    )
    def test_valid_model_runner(algorithms, run_parallel):
        start = time.time()
    
        logging.warning("Running Modelling Phase")
    
        optuna.logging.set_verbosity(optuna.logging.WARNING)
    
>       runner = ModelExperimentRunner(output_path, experiment_name, algorithms, save_plots=True)

streamline/tests/test_model_runner.py:67: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <streamline.modeling.modelrunner.ModelExperimentRunner object at 0x7fd72977c2e0>
output_path = './tests/', experiment_name = 'demo', algorithms = ['GB']
exclude = None, class_label = 'Class', instance_label = None
scoring_metric = 'balanced_accuracy', metric_direction = 'maximize'
training_subsample = 0, use_uniform_fi = True, n_trials = 200, timeout = 900
save_plots = True, do_lcs_sweep = False, lcs_nu = 1, lcs_n = 2000
lcs_iterations = 200000, lcs_timeout = 1200, random_state = None, n_jobs = None

    def __init__(self, output_path, experiment_name, algorithms=None, exclude=None, class_label="Class",
                 instance_label=None, scoring_metric='balanced_accuracy', metric_direction='maximize',
                 training_subsample=0, use_uniform_fi=True, n_trials=200,
                 timeout=900, save_plots=False, do_lcs_sweep=False, lcs_nu=1, lcs_n=2000, lcs_iterations=200000,
                 lcs_timeout=1200, random_state=None, n_jobs=None):
    
        """
        Args:
            output_path: path to output directory
            experiment_name: name of experiment (no spaces)
            algorithms: list of str of ML models to run
            scoring_metric: primary scikit-learn specified scoring metric used for hyperparameter optimization and \
                            permutation-based model feature importance evaluation, default='balanced_accuracy'
            metric_direction: direction to optimize the scoring metric in optuna, \
                              either 'maximize' or 'minimize', default='maximize'
            training_subsample: for long running algos (XGB,SVM,ANN,KNN), option to subsample training set \
                                (0 for no subsample, default=0)
            use_uniform_fi: overrides use of any available feature importance estimate methods from models, \
                            instead using permutation_importance uniformly, default=True
            n_trials: number of bayesian hyperparameter optimization trials using optuna \
                      (specify an integer or None) default=200
            timeout: seconds until hyperparameter sweep stops running new trials \
                    (Note: it may run longer to finish last trial started) \
                    If set to None, STREAMLINE is completely replicable, but will take longer to run \
                    default=900 i.e. 900 sec = 15 minutes default \
            save_plots: export optuna-generated hyperparameter sweep plots, default False
            do_lcs_sweep: do LCS hyper-param tuning or use below params, default=False
            lcs_nu: fixed LCS nu param (recommended range 1-10), set to larger value for data with \
                    less or no noise, default=1
            lcs_iterations: fixed LCS number of learning iterations param, default=200000
            lcs_n: fixed LCS rule population maximum size param, default=2000
            lcs_timeout: seconds until hyperparameter sweep stops for LCS algorithms, default=1200
    
        """
        # TODO: What does training subsample do
        self.cv_count = None
        self.dataset = None
        self.output_path = output_path
        self.experiment_name = experiment_name
        self.class_label = class_label
        self.instance_label = instance_label
    
        if algorithms is None:
            self.algorithms = SUPPORTED_MODELS
            if exclude is not None:
                for algorithm in exclude:
                    try:
                        self.algorithms.remove(algorithm)
                    except Exception:
                        logging.error("Unknown algorithm in exclude: " + str(algorithm))
        else:
            for algorithm in algorithms:
                assert is_supported_model(algorithm)
            self.algorithms = algorithms
    
        self.scoring_metric = scoring_metric
        self.metric_direction = metric_direction
        self.training_subsample = training_subsample
        self.uniform_fi = use_uniform_fi
        self.n_trials = n_trials
        self.timeout = timeout
        self.save_plots = save_plots
        self.do_lcs_sweep = do_lcs_sweep
        self.lcs_nu = lcs_nu
        self.lcs_n = lcs_n
        self.lcs_iterations = lcs_iterations
        self.lcs_timeout = lcs_timeout
    
        self.random_state = random_state
        self.n_jobs = n_jobs
    
        # Argument checks
        if not os.path.exists(self.output_path):
>           raise Exception("Output path must exist (from phase 1) before phase 4 can begin")
E           Exception: Output path must exist (from phase 1) before phase 4 can begin

streamline/modeling/modelrunner.py:90: Exception
------------------------------ Captured log call -------------------------------
WARNING  root:test_model_runner.py:63 Running Modelling Phase
__________________ test_valid_model_runner[algorithms7-True] ___________________

algorithms = ['XGB'], run_parallel = True

    @pytest.mark.parametrize(
        ("algorithms", "run_parallel"),
        [
            (['NB'], False),
            (["LR"], False),
            # (["NB", "LR", "DT"], True),
            # (['CGB'], False),
            # (['LGB'], False),
            # (['XGB'], False),
            # (['GP'], False),
            # (['XCS'], True),
            # ([SUPPORTED_MODELS_SMALL[-1]], True),
    
        ]
        +
        [([algo], True) for algo in SUPPORTED_MODELS_SMALL]
    )
    def test_valid_model_runner(algorithms, run_parallel):
        start = time.time()
    
        logging.warning("Running Modelling Phase")
    
        optuna.logging.set_verbosity(optuna.logging.WARNING)
    
>       runner = ModelExperimentRunner(output_path, experiment_name, algorithms, save_plots=True)

streamline/tests/test_model_runner.py:67: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <streamline.modeling.modelrunner.ModelExperimentRunner object at 0x7fd729777220>
output_path = './tests/', experiment_name = 'demo', algorithms = ['XGB']
exclude = None, class_label = 'Class', instance_label = None
scoring_metric = 'balanced_accuracy', metric_direction = 'maximize'
training_subsample = 0, use_uniform_fi = True, n_trials = 200, timeout = 900
save_plots = True, do_lcs_sweep = False, lcs_nu = 1, lcs_n = 2000
lcs_iterations = 200000, lcs_timeout = 1200, random_state = None, n_jobs = None

    def __init__(self, output_path, experiment_name, algorithms=None, exclude=None, class_label="Class",
                 instance_label=None, scoring_metric='balanced_accuracy', metric_direction='maximize',
                 training_subsample=0, use_uniform_fi=True, n_trials=200,
                 timeout=900, save_plots=False, do_lcs_sweep=False, lcs_nu=1, lcs_n=2000, lcs_iterations=200000,
                 lcs_timeout=1200, random_state=None, n_jobs=None):
    
        """
        Args:
            output_path: path to output directory
            experiment_name: name of experiment (no spaces)
            algorithms: list of str of ML models to run
            scoring_metric: primary scikit-learn specified scoring metric used for hyperparameter optimization and \
                            permutation-based model feature importance evaluation, default='balanced_accuracy'
            metric_direction: direction to optimize the scoring metric in optuna, \
                              either 'maximize' or 'minimize', default='maximize'
            training_subsample: for long running algos (XGB,SVM,ANN,KNN), option to subsample training set \
                                (0 for no subsample, default=0)
            use_uniform_fi: overrides use of any available feature importance estimate methods from models, \
                            instead using permutation_importance uniformly, default=True
            n_trials: number of bayesian hyperparameter optimization trials using optuna \
                      (specify an integer or None) default=200
            timeout: seconds until hyperparameter sweep stops running new trials \
                    (Note: it may run longer to finish last trial started) \
                    If set to None, STREAMLINE is completely replicable, but will take longer to run \
                    default=900 i.e. 900 sec = 15 minutes default \
            save_plots: export optuna-generated hyperparameter sweep plots, default False
            do_lcs_sweep: do LCS hyper-param tuning or use below params, default=False
            lcs_nu: fixed LCS nu param (recommended range 1-10), set to larger value for data with \
                    less or no noise, default=1
            lcs_iterations: fixed LCS number of learning iterations param, default=200000
            lcs_n: fixed LCS rule population maximum size param, default=2000
            lcs_timeout: seconds until hyperparameter sweep stops for LCS algorithms, default=1200
    
        """
        # TODO: What does training subsample do
        self.cv_count = None
        self.dataset = None
        self.output_path = output_path
        self.experiment_name = experiment_name
        self.class_label = class_label
        self.instance_label = instance_label
    
        if algorithms is None:
            self.algorithms = SUPPORTED_MODELS
            if exclude is not None:
                for algorithm in exclude:
                    try:
                        self.algorithms.remove(algorithm)
                    except Exception:
                        logging.error("Unknown algorithm in exclude: " + str(algorithm))
        else:
            for algorithm in algorithms:
                assert is_supported_model(algorithm)
            self.algorithms = algorithms
    
        self.scoring_metric = scoring_metric
        self.metric_direction = metric_direction
        self.training_subsample = training_subsample
        self.uniform_fi = use_uniform_fi
        self.n_trials = n_trials
        self.timeout = timeout
        self.save_plots = save_plots
        self.do_lcs_sweep = do_lcs_sweep
        self.lcs_nu = lcs_nu
        self.lcs_n = lcs_n
        self.lcs_iterations = lcs_iterations
        self.lcs_timeout = lcs_timeout
    
        self.random_state = random_state
        self.n_jobs = n_jobs
    
        # Argument checks
        if not os.path.exists(self.output_path):
>           raise Exception("Output path must exist (from phase 1) before phase 4 can begin")
E           Exception: Output path must exist (from phase 1) before phase 4 can begin

streamline/modeling/modelrunner.py:90: Exception
------------------------------ Captured log call -------------------------------
WARNING  root:test_model_runner.py:63 Running Modelling Phase
__________________ test_valid_model_runner[algorithms8-True] ___________________

algorithms = ['LGB'], run_parallel = True

    @pytest.mark.parametrize(
        ("algorithms", "run_parallel"),
        [
            (['NB'], False),
            (["LR"], False),
            # (["NB", "LR", "DT"], True),
            # (['CGB'], False),
            # (['LGB'], False),
            # (['XGB'], False),
            # (['GP'], False),
            # (['XCS'], True),
            # ([SUPPORTED_MODELS_SMALL[-1]], True),
    
        ]
        +
        [([algo], True) for algo in SUPPORTED_MODELS_SMALL]
    )
    def test_valid_model_runner(algorithms, run_parallel):
        start = time.time()
    
        logging.warning("Running Modelling Phase")
    
        optuna.logging.set_verbosity(optuna.logging.WARNING)
    
>       runner = ModelExperimentRunner(output_path, experiment_name, algorithms, save_plots=True)

streamline/tests/test_model_runner.py:67: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <streamline.modeling.modelrunner.ModelExperimentRunner object at 0x7fd7540d2fd0>
output_path = './tests/', experiment_name = 'demo', algorithms = ['LGB']
exclude = None, class_label = 'Class', instance_label = None
scoring_metric = 'balanced_accuracy', metric_direction = 'maximize'
training_subsample = 0, use_uniform_fi = True, n_trials = 200, timeout = 900
save_plots = True, do_lcs_sweep = False, lcs_nu = 1, lcs_n = 2000
lcs_iterations = 200000, lcs_timeout = 1200, random_state = None, n_jobs = None

    def __init__(self, output_path, experiment_name, algorithms=None, exclude=None, class_label="Class",
                 instance_label=None, scoring_metric='balanced_accuracy', metric_direction='maximize',
                 training_subsample=0, use_uniform_fi=True, n_trials=200,
                 timeout=900, save_plots=False, do_lcs_sweep=False, lcs_nu=1, lcs_n=2000, lcs_iterations=200000,
                 lcs_timeout=1200, random_state=None, n_jobs=None):
    
        """
        Args:
            output_path: path to output directory
            experiment_name: name of experiment (no spaces)
            algorithms: list of str of ML models to run
            scoring_metric: primary scikit-learn specified scoring metric used for hyperparameter optimization and \
                            permutation-based model feature importance evaluation, default='balanced_accuracy'
            metric_direction: direction to optimize the scoring metric in optuna, \
                              either 'maximize' or 'minimize', default='maximize'
            training_subsample: for long running algos (XGB,SVM,ANN,KNN), option to subsample training set \
                                (0 for no subsample, default=0)
            use_uniform_fi: overrides use of any available feature importance estimate methods from models, \
                            instead using permutation_importance uniformly, default=True
            n_trials: number of bayesian hyperparameter optimization trials using optuna \
                      (specify an integer or None) default=200
            timeout: seconds until hyperparameter sweep stops running new trials \
                    (Note: it may run longer to finish last trial started) \
                    If set to None, STREAMLINE is completely replicable, but will take longer to run \
                    default=900 i.e. 900 sec = 15 minutes default \
            save_plots: export optuna-generated hyperparameter sweep plots, default False
            do_lcs_sweep: do LCS hyper-param tuning or use below params, default=False
            lcs_nu: fixed LCS nu param (recommended range 1-10), set to larger value for data with \
                    less or no noise, default=1
            lcs_iterations: fixed LCS number of learning iterations param, default=200000
            lcs_n: fixed LCS rule population maximum size param, default=2000
            lcs_timeout: seconds until hyperparameter sweep stops for LCS algorithms, default=1200
    
        """
        # TODO: What does training subsample do
        self.cv_count = None
        self.dataset = None
        self.output_path = output_path
        self.experiment_name = experiment_name
        self.class_label = class_label
        self.instance_label = instance_label
    
        if algorithms is None:
            self.algorithms = SUPPORTED_MODELS
            if exclude is not None:
                for algorithm in exclude:
                    try:
                        self.algorithms.remove(algorithm)
                    except Exception:
                        logging.error("Unknown algorithm in exclude: " + str(algorithm))
        else:
            for algorithm in algorithms:
                assert is_supported_model(algorithm)
            self.algorithms = algorithms
    
        self.scoring_metric = scoring_metric
        self.metric_direction = metric_direction
        self.training_subsample = training_subsample
        self.uniform_fi = use_uniform_fi
        self.n_trials = n_trials
        self.timeout = timeout
        self.save_plots = save_plots
        self.do_lcs_sweep = do_lcs_sweep
        self.lcs_nu = lcs_nu
        self.lcs_n = lcs_n
        self.lcs_iterations = lcs_iterations
        self.lcs_timeout = lcs_timeout
    
        self.random_state = random_state
        self.n_jobs = n_jobs
    
        # Argument checks
        if not os.path.exists(self.output_path):
>           raise Exception("Output path must exist (from phase 1) before phase 4 can begin")
E           Exception: Output path must exist (from phase 1) before phase 4 can begin

streamline/modeling/modelrunner.py:90: Exception
------------------------------ Captured log call -------------------------------
WARNING  root:test_model_runner.py:63 Running Modelling Phase
__________________ test_valid_model_runner[algorithms9-True] ___________________

algorithms = ['CGB'], run_parallel = True

    @pytest.mark.parametrize(
        ("algorithms", "run_parallel"),
        [
            (['NB'], False),
            (["LR"], False),
            # (["NB", "LR", "DT"], True),
            # (['CGB'], False),
            # (['LGB'], False),
            # (['XGB'], False),
            # (['GP'], False),
            # (['XCS'], True),
            # ([SUPPORTED_MODELS_SMALL[-1]], True),
    
        ]
        +
        [([algo], True) for algo in SUPPORTED_MODELS_SMALL]
    )
    def test_valid_model_runner(algorithms, run_parallel):
        start = time.time()
    
        logging.warning("Running Modelling Phase")
    
        optuna.logging.set_verbosity(optuna.logging.WARNING)
    
>       runner = ModelExperimentRunner(output_path, experiment_name, algorithms, save_plots=True)

streamline/tests/test_model_runner.py:67: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <streamline.modeling.modelrunner.ModelExperimentRunner object at 0x7fd72977d850>
output_path = './tests/', experiment_name = 'demo', algorithms = ['CGB']
exclude = None, class_label = 'Class', instance_label = None
scoring_metric = 'balanced_accuracy', metric_direction = 'maximize'
training_subsample = 0, use_uniform_fi = True, n_trials = 200, timeout = 900
save_plots = True, do_lcs_sweep = False, lcs_nu = 1, lcs_n = 2000
lcs_iterations = 200000, lcs_timeout = 1200, random_state = None, n_jobs = None

    def __init__(self, output_path, experiment_name, algorithms=None, exclude=None, class_label="Class",
                 instance_label=None, scoring_metric='balanced_accuracy', metric_direction='maximize',
                 training_subsample=0, use_uniform_fi=True, n_trials=200,
                 timeout=900, save_plots=False, do_lcs_sweep=False, lcs_nu=1, lcs_n=2000, lcs_iterations=200000,
                 lcs_timeout=1200, random_state=None, n_jobs=None):
    
        """
        Args:
            output_path: path to output directory
            experiment_name: name of experiment (no spaces)
            algorithms: list of str of ML models to run
            scoring_metric: primary scikit-learn specified scoring metric used for hyperparameter optimization and \
                            permutation-based model feature importance evaluation, default='balanced_accuracy'
            metric_direction: direction to optimize the scoring metric in optuna, \
                              either 'maximize' or 'minimize', default='maximize'
            training_subsample: for long running algos (XGB,SVM,ANN,KNN), option to subsample training set \
                                (0 for no subsample, default=0)
            use_uniform_fi: overrides use of any available feature importance estimate methods from models, \
                            instead using permutation_importance uniformly, default=True
            n_trials: number of bayesian hyperparameter optimization trials using optuna \
                      (specify an integer or None) default=200
            timeout: seconds until hyperparameter sweep stops running new trials \
                    (Note: it may run longer to finish last trial started) \
                    If set to None, STREAMLINE is completely replicable, but will take longer to run \
                    default=900 i.e. 900 sec = 15 minutes default \
            save_plots: export optuna-generated hyperparameter sweep plots, default False
            do_lcs_sweep: do LCS hyper-param tuning or use below params, default=False
            lcs_nu: fixed LCS nu param (recommended range 1-10), set to larger value for data with \
                    less or no noise, default=1
            lcs_iterations: fixed LCS number of learning iterations param, default=200000
            lcs_n: fixed LCS rule population maximum size param, default=2000
            lcs_timeout: seconds until hyperparameter sweep stops for LCS algorithms, default=1200
    
        """
        # TODO: What does training subsample do
        self.cv_count = None
        self.dataset = None
        self.output_path = output_path
        self.experiment_name = experiment_name
        self.class_label = class_label
        self.instance_label = instance_label
    
        if algorithms is None:
            self.algorithms = SUPPORTED_MODELS
            if exclude is not None:
                for algorithm in exclude:
                    try:
                        self.algorithms.remove(algorithm)
                    except Exception:
                        logging.error("Unknown algorithm in exclude: " + str(algorithm))
        else:
            for algorithm in algorithms:
                assert is_supported_model(algorithm)
            self.algorithms = algorithms
    
        self.scoring_metric = scoring_metric
        self.metric_direction = metric_direction
        self.training_subsample = training_subsample
        self.uniform_fi = use_uniform_fi
        self.n_trials = n_trials
        self.timeout = timeout
        self.save_plots = save_plots
        self.do_lcs_sweep = do_lcs_sweep
        self.lcs_nu = lcs_nu
        self.lcs_n = lcs_n
        self.lcs_iterations = lcs_iterations
        self.lcs_timeout = lcs_timeout
    
        self.random_state = random_state
        self.n_jobs = n_jobs
    
        # Argument checks
        if not os.path.exists(self.output_path):
>           raise Exception("Output path must exist (from phase 1) before phase 4 can begin")
E           Exception: Output path must exist (from phase 1) before phase 4 can begin

streamline/modeling/modelrunner.py:90: Exception
------------------------------ Captured log call -------------------------------
WARNING  root:test_model_runner.py:63 Running Modelling Phase
__________________ test_valid_model_runner[algorithms10-True] __________________

algorithms = ['SVM'], run_parallel = True

    @pytest.mark.parametrize(
        ("algorithms", "run_parallel"),
        [
            (['NB'], False),
            (["LR"], False),
            # (["NB", "LR", "DT"], True),
            # (['CGB'], False),
            # (['LGB'], False),
            # (['XGB'], False),
            # (['GP'], False),
            # (['XCS'], True),
            # ([SUPPORTED_MODELS_SMALL[-1]], True),
    
        ]
        +
        [([algo], True) for algo in SUPPORTED_MODELS_SMALL]
    )
    def test_valid_model_runner(algorithms, run_parallel):
        start = time.time()
    
        logging.warning("Running Modelling Phase")
    
        optuna.logging.set_verbosity(optuna.logging.WARNING)
    
>       runner = ModelExperimentRunner(output_path, experiment_name, algorithms, save_plots=True)

streamline/tests/test_model_runner.py:67: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <streamline.modeling.modelrunner.ModelExperimentRunner object at 0x7fd7540573a0>
output_path = './tests/', experiment_name = 'demo', algorithms = ['SVM']
exclude = None, class_label = 'Class', instance_label = None
scoring_metric = 'balanced_accuracy', metric_direction = 'maximize'
training_subsample = 0, use_uniform_fi = True, n_trials = 200, timeout = 900
save_plots = True, do_lcs_sweep = False, lcs_nu = 1, lcs_n = 2000
lcs_iterations = 200000, lcs_timeout = 1200, random_state = None, n_jobs = None

    def __init__(self, output_path, experiment_name, algorithms=None, exclude=None, class_label="Class",
                 instance_label=None, scoring_metric='balanced_accuracy', metric_direction='maximize',
                 training_subsample=0, use_uniform_fi=True, n_trials=200,
                 timeout=900, save_plots=False, do_lcs_sweep=False, lcs_nu=1, lcs_n=2000, lcs_iterations=200000,
                 lcs_timeout=1200, random_state=None, n_jobs=None):
    
        """
        Args:
            output_path: path to output directory
            experiment_name: name of experiment (no spaces)
            algorithms: list of str of ML models to run
            scoring_metric: primary scikit-learn specified scoring metric used for hyperparameter optimization and \
                            permutation-based model feature importance evaluation, default='balanced_accuracy'
            metric_direction: direction to optimize the scoring metric in optuna, \
                              either 'maximize' or 'minimize', default='maximize'
            training_subsample: for long running algos (XGB,SVM,ANN,KNN), option to subsample training set \
                                (0 for no subsample, default=0)
            use_uniform_fi: overrides use of any available feature importance estimate methods from models, \
                            instead using permutation_importance uniformly, default=True
            n_trials: number of bayesian hyperparameter optimization trials using optuna \
                      (specify an integer or None) default=200
            timeout: seconds until hyperparameter sweep stops running new trials \
                    (Note: it may run longer to finish last trial started) \
                    If set to None, STREAMLINE is completely replicable, but will take longer to run \
                    default=900 i.e. 900 sec = 15 minutes default \
            save_plots: export optuna-generated hyperparameter sweep plots, default False
            do_lcs_sweep: do LCS hyper-param tuning or use below params, default=False
            lcs_nu: fixed LCS nu param (recommended range 1-10), set to larger value for data with \
                    less or no noise, default=1
            lcs_iterations: fixed LCS number of learning iterations param, default=200000
            lcs_n: fixed LCS rule population maximum size param, default=2000
            lcs_timeout: seconds until hyperparameter sweep stops for LCS algorithms, default=1200
    
        """
        # TODO: What does training subsample do
        self.cv_count = None
        self.dataset = None
        self.output_path = output_path
        self.experiment_name = experiment_name
        self.class_label = class_label
        self.instance_label = instance_label
    
        if algorithms is None:
            self.algorithms = SUPPORTED_MODELS
            if exclude is not None:
                for algorithm in exclude:
                    try:
                        self.algorithms.remove(algorithm)
                    except Exception:
                        logging.error("Unknown algorithm in exclude: " + str(algorithm))
        else:
            for algorithm in algorithms:
                assert is_supported_model(algorithm)
            self.algorithms = algorithms
    
        self.scoring_metric = scoring_metric
        self.metric_direction = metric_direction
        self.training_subsample = training_subsample
        self.uniform_fi = use_uniform_fi
        self.n_trials = n_trials
        self.timeout = timeout
        self.save_plots = save_plots
        self.do_lcs_sweep = do_lcs_sweep
        self.lcs_nu = lcs_nu
        self.lcs_n = lcs_n
        self.lcs_iterations = lcs_iterations
        self.lcs_timeout = lcs_timeout
    
        self.random_state = random_state
        self.n_jobs = n_jobs
    
        # Argument checks
        if not os.path.exists(self.output_path):
>           raise Exception("Output path must exist (from phase 1) before phase 4 can begin")
E           Exception: Output path must exist (from phase 1) before phase 4 can begin

streamline/modeling/modelrunner.py:90: Exception
------------------------------ Captured log call -------------------------------
WARNING  root:test_model_runner.py:63 Running Modelling Phase
__________________ test_valid_model_runner[algorithms11-True] __________________

algorithms = ['ANN'], run_parallel = True

    @pytest.mark.parametrize(
        ("algorithms", "run_parallel"),
        [
            (['NB'], False),
            (["LR"], False),
            # (["NB", "LR", "DT"], True),
            # (['CGB'], False),
            # (['LGB'], False),
            # (['XGB'], False),
            # (['GP'], False),
            # (['XCS'], True),
            # ([SUPPORTED_MODELS_SMALL[-1]], True),
    
        ]
        +
        [([algo], True) for algo in SUPPORTED_MODELS_SMALL]
    )
    def test_valid_model_runner(algorithms, run_parallel):
        start = time.time()
    
        logging.warning("Running Modelling Phase")
    
        optuna.logging.set_verbosity(optuna.logging.WARNING)
    
>       runner = ModelExperimentRunner(output_path, experiment_name, algorithms, save_plots=True)

streamline/tests/test_model_runner.py:67: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <streamline.modeling.modelrunner.ModelExperimentRunner object at 0x7fd754096ac0>
output_path = './tests/', experiment_name = 'demo', algorithms = ['ANN']
exclude = None, class_label = 'Class', instance_label = None
scoring_metric = 'balanced_accuracy', metric_direction = 'maximize'
training_subsample = 0, use_uniform_fi = True, n_trials = 200, timeout = 900
save_plots = True, do_lcs_sweep = False, lcs_nu = 1, lcs_n = 2000
lcs_iterations = 200000, lcs_timeout = 1200, random_state = None, n_jobs = None

    def __init__(self, output_path, experiment_name, algorithms=None, exclude=None, class_label="Class",
                 instance_label=None, scoring_metric='balanced_accuracy', metric_direction='maximize',
                 training_subsample=0, use_uniform_fi=True, n_trials=200,
                 timeout=900, save_plots=False, do_lcs_sweep=False, lcs_nu=1, lcs_n=2000, lcs_iterations=200000,
                 lcs_timeout=1200, random_state=None, n_jobs=None):
    
        """
        Args:
            output_path: path to output directory
            experiment_name: name of experiment (no spaces)
            algorithms: list of str of ML models to run
            scoring_metric: primary scikit-learn specified scoring metric used for hyperparameter optimization and \
                            permutation-based model feature importance evaluation, default='balanced_accuracy'
            metric_direction: direction to optimize the scoring metric in optuna, \
                              either 'maximize' or 'minimize', default='maximize'
            training_subsample: for long running algos (XGB,SVM,ANN,KNN), option to subsample training set \
                                (0 for no subsample, default=0)
            use_uniform_fi: overrides use of any available feature importance estimate methods from models, \
                            instead using permutation_importance uniformly, default=True
            n_trials: number of bayesian hyperparameter optimization trials using optuna \
                      (specify an integer or None) default=200
            timeout: seconds until hyperparameter sweep stops running new trials \
                    (Note: it may run longer to finish last trial started) \
                    If set to None, STREAMLINE is completely replicable, but will take longer to run \
                    default=900 i.e. 900 sec = 15 minutes default \
            save_plots: export optuna-generated hyperparameter sweep plots, default False
            do_lcs_sweep: do LCS hyper-param tuning or use below params, default=False
            lcs_nu: fixed LCS nu param (recommended range 1-10), set to larger value for data with \
                    less or no noise, default=1
            lcs_iterations: fixed LCS number of learning iterations param, default=200000
            lcs_n: fixed LCS rule population maximum size param, default=2000
            lcs_timeout: seconds until hyperparameter sweep stops for LCS algorithms, default=1200
    
        """
        # TODO: What does training subsample do
        self.cv_count = None
        self.dataset = None
        self.output_path = output_path
        self.experiment_name = experiment_name
        self.class_label = class_label
        self.instance_label = instance_label
    
        if algorithms is None:
            self.algorithms = SUPPORTED_MODELS
            if exclude is not None:
                for algorithm in exclude:
                    try:
                        self.algorithms.remove(algorithm)
                    except Exception:
                        logging.error("Unknown algorithm in exclude: " + str(algorithm))
        else:
            for algorithm in algorithms:
                assert is_supported_model(algorithm)
            self.algorithms = algorithms
    
        self.scoring_metric = scoring_metric
        self.metric_direction = metric_direction
        self.training_subsample = training_subsample
        self.uniform_fi = use_uniform_fi
        self.n_trials = n_trials
        self.timeout = timeout
        self.save_plots = save_plots
        self.do_lcs_sweep = do_lcs_sweep
        self.lcs_nu = lcs_nu
        self.lcs_n = lcs_n
        self.lcs_iterations = lcs_iterations
        self.lcs_timeout = lcs_timeout
    
        self.random_state = random_state
        self.n_jobs = n_jobs
    
        # Argument checks
        if not os.path.exists(self.output_path):
>           raise Exception("Output path must exist (from phase 1) before phase 4 can begin")
E           Exception: Output path must exist (from phase 1) before phase 4 can begin

streamline/modeling/modelrunner.py:90: Exception
------------------------------ Captured log call -------------------------------
WARNING  root:test_model_runner.py:63 Running Modelling Phase
__________________ test_valid_model_runner[algorithms12-True] __________________

algorithms = ['KNN'], run_parallel = True

    @pytest.mark.parametrize(
        ("algorithms", "run_parallel"),
        [
            (['NB'], False),
            (["LR"], False),
            # (["NB", "LR", "DT"], True),
            # (['CGB'], False),
            # (['LGB'], False),
            # (['XGB'], False),
            # (['GP'], False),
            # (['XCS'], True),
            # ([SUPPORTED_MODELS_SMALL[-1]], True),
    
        ]
        +
        [([algo], True) for algo in SUPPORTED_MODELS_SMALL]
    )
    def test_valid_model_runner(algorithms, run_parallel):
        start = time.time()
    
        logging.warning("Running Modelling Phase")
    
        optuna.logging.set_verbosity(optuna.logging.WARNING)
    
>       runner = ModelExperimentRunner(output_path, experiment_name, algorithms, save_plots=True)

streamline/tests/test_model_runner.py:67: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <streamline.modeling.modelrunner.ModelExperimentRunner object at 0x7fd72977ea90>
output_path = './tests/', experiment_name = 'demo', algorithms = ['KNN']
exclude = None, class_label = 'Class', instance_label = None
scoring_metric = 'balanced_accuracy', metric_direction = 'maximize'
training_subsample = 0, use_uniform_fi = True, n_trials = 200, timeout = 900
save_plots = True, do_lcs_sweep = False, lcs_nu = 1, lcs_n = 2000
lcs_iterations = 200000, lcs_timeout = 1200, random_state = None, n_jobs = None

    def __init__(self, output_path, experiment_name, algorithms=None, exclude=None, class_label="Class",
                 instance_label=None, scoring_metric='balanced_accuracy', metric_direction='maximize',
                 training_subsample=0, use_uniform_fi=True, n_trials=200,
                 timeout=900, save_plots=False, do_lcs_sweep=False, lcs_nu=1, lcs_n=2000, lcs_iterations=200000,
                 lcs_timeout=1200, random_state=None, n_jobs=None):
    
        """
        Args:
            output_path: path to output directory
            experiment_name: name of experiment (no spaces)
            algorithms: list of str of ML models to run
            scoring_metric: primary scikit-learn specified scoring metric used for hyperparameter optimization and \
                            permutation-based model feature importance evaluation, default='balanced_accuracy'
            metric_direction: direction to optimize the scoring metric in optuna, \
                              either 'maximize' or 'minimize', default='maximize'
            training_subsample: for long running algos (XGB,SVM,ANN,KNN), option to subsample training set \
                                (0 for no subsample, default=0)
            use_uniform_fi: overrides use of any available feature importance estimate methods from models, \
                            instead using permutation_importance uniformly, default=True
            n_trials: number of bayesian hyperparameter optimization trials using optuna \
                      (specify an integer or None) default=200
            timeout: seconds until hyperparameter sweep stops running new trials \
                    (Note: it may run longer to finish last trial started) \
                    If set to None, STREAMLINE is completely replicable, but will take longer to run \
                    default=900 i.e. 900 sec = 15 minutes default \
            save_plots: export optuna-generated hyperparameter sweep plots, default False
            do_lcs_sweep: do LCS hyper-param tuning or use below params, default=False
            lcs_nu: fixed LCS nu param (recommended range 1-10), set to larger value for data with \
                    less or no noise, default=1
            lcs_iterations: fixed LCS number of learning iterations param, default=200000
            lcs_n: fixed LCS rule population maximum size param, default=2000
            lcs_timeout: seconds until hyperparameter sweep stops for LCS algorithms, default=1200
    
        """
        # TODO: What does training subsample do
        self.cv_count = None
        self.dataset = None
        self.output_path = output_path
        self.experiment_name = experiment_name
        self.class_label = class_label
        self.instance_label = instance_label
    
        if algorithms is None:
            self.algorithms = SUPPORTED_MODELS
            if exclude is not None:
                for algorithm in exclude:
                    try:
                        self.algorithms.remove(algorithm)
                    except Exception:
                        logging.error("Unknown algorithm in exclude: " + str(algorithm))
        else:
            for algorithm in algorithms:
                assert is_supported_model(algorithm)
            self.algorithms = algorithms
    
        self.scoring_metric = scoring_metric
        self.metric_direction = metric_direction
        self.training_subsample = training_subsample
        self.uniform_fi = use_uniform_fi
        self.n_trials = n_trials
        self.timeout = timeout
        self.save_plots = save_plots
        self.do_lcs_sweep = do_lcs_sweep
        self.lcs_nu = lcs_nu
        self.lcs_n = lcs_n
        self.lcs_iterations = lcs_iterations
        self.lcs_timeout = lcs_timeout
    
        self.random_state = random_state
        self.n_jobs = n_jobs
    
        # Argument checks
        if not os.path.exists(self.output_path):
>           raise Exception("Output path must exist (from phase 1) before phase 4 can begin")
E           Exception: Output path must exist (from phase 1) before phase 4 can begin

streamline/modeling/modelrunner.py:90: Exception
------------------------------ Captured log call -------------------------------
WARNING  root:test_model_runner.py:63 Running Modelling Phase
__________________ test_valid_model_runner[algorithms13-True] __________________

algorithms = ['GP'], run_parallel = True

    @pytest.mark.parametrize(
        ("algorithms", "run_parallel"),
        [
            (['NB'], False),
            (["LR"], False),
            # (["NB", "LR", "DT"], True),
            # (['CGB'], False),
            # (['LGB'], False),
            # (['XGB'], False),
            # (['GP'], False),
            # (['XCS'], True),
            # ([SUPPORTED_MODELS_SMALL[-1]], True),
    
        ]
        +
        [([algo], True) for algo in SUPPORTED_MODELS_SMALL]
    )
    def test_valid_model_runner(algorithms, run_parallel):
        start = time.time()
    
        logging.warning("Running Modelling Phase")
    
        optuna.logging.set_verbosity(optuna.logging.WARNING)
    
>       runner = ModelExperimentRunner(output_path, experiment_name, algorithms, save_plots=True)

streamline/tests/test_model_runner.py:67: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <streamline.modeling.modelrunner.ModelExperimentRunner object at 0x7fd7540df700>
output_path = './tests/', experiment_name = 'demo', algorithms = ['GP']
exclude = None, class_label = 'Class', instance_label = None
scoring_metric = 'balanced_accuracy', metric_direction = 'maximize'
training_subsample = 0, use_uniform_fi = True, n_trials = 200, timeout = 900
save_plots = True, do_lcs_sweep = False, lcs_nu = 1, lcs_n = 2000
lcs_iterations = 200000, lcs_timeout = 1200, random_state = None, n_jobs = None

    def __init__(self, output_path, experiment_name, algorithms=None, exclude=None, class_label="Class",
                 instance_label=None, scoring_metric='balanced_accuracy', metric_direction='maximize',
                 training_subsample=0, use_uniform_fi=True, n_trials=200,
                 timeout=900, save_plots=False, do_lcs_sweep=False, lcs_nu=1, lcs_n=2000, lcs_iterations=200000,
                 lcs_timeout=1200, random_state=None, n_jobs=None):
    
        """
        Args:
            output_path: path to output directory
            experiment_name: name of experiment (no spaces)
            algorithms: list of str of ML models to run
            scoring_metric: primary scikit-learn specified scoring metric used for hyperparameter optimization and \
                            permutation-based model feature importance evaluation, default='balanced_accuracy'
            metric_direction: direction to optimize the scoring metric in optuna, \
                              either 'maximize' or 'minimize', default='maximize'
            training_subsample: for long running algos (XGB,SVM,ANN,KNN), option to subsample training set \
                                (0 for no subsample, default=0)
            use_uniform_fi: overrides use of any available feature importance estimate methods from models, \
                            instead using permutation_importance uniformly, default=True
            n_trials: number of bayesian hyperparameter optimization trials using optuna \
                      (specify an integer or None) default=200
            timeout: seconds until hyperparameter sweep stops running new trials \
                    (Note: it may run longer to finish last trial started) \
                    If set to None, STREAMLINE is completely replicable, but will take longer to run \
                    default=900 i.e. 900 sec = 15 minutes default \
            save_plots: export optuna-generated hyperparameter sweep plots, default False
            do_lcs_sweep: do LCS hyper-param tuning or use below params, default=False
            lcs_nu: fixed LCS nu param (recommended range 1-10), set to larger value for data with \
                    less or no noise, default=1
            lcs_iterations: fixed LCS number of learning iterations param, default=200000
            lcs_n: fixed LCS rule population maximum size param, default=2000
            lcs_timeout: seconds until hyperparameter sweep stops for LCS algorithms, default=1200
    
        """
        # TODO: What does training subsample do
        self.cv_count = None
        self.dataset = None
        self.output_path = output_path
        self.experiment_name = experiment_name
        self.class_label = class_label
        self.instance_label = instance_label
    
        if algorithms is None:
            self.algorithms = SUPPORTED_MODELS
            if exclude is not None:
                for algorithm in exclude:
                    try:
                        self.algorithms.remove(algorithm)
                    except Exception:
                        logging.error("Unknown algorithm in exclude: " + str(algorithm))
        else:
            for algorithm in algorithms:
                assert is_supported_model(algorithm)
            self.algorithms = algorithms
    
        self.scoring_metric = scoring_metric
        self.metric_direction = metric_direction
        self.training_subsample = training_subsample
        self.uniform_fi = use_uniform_fi
        self.n_trials = n_trials
        self.timeout = timeout
        self.save_plots = save_plots
        self.do_lcs_sweep = do_lcs_sweep
        self.lcs_nu = lcs_nu
        self.lcs_n = lcs_n
        self.lcs_iterations = lcs_iterations
        self.lcs_timeout = lcs_timeout
    
        self.random_state = random_state
        self.n_jobs = n_jobs
    
        # Argument checks
        if not os.path.exists(self.output_path):
>           raise Exception("Output path must exist (from phase 1) before phase 4 can begin")
E           Exception: Output path must exist (from phase 1) before phase 4 can begin

streamline/modeling/modelrunner.py:90: Exception
------------------------------ Captured log call -------------------------------
WARNING  root:test_model_runner.py:63 Running Modelling Phase
__________________ test_valid_model_runner[algorithms14-True] __________________

algorithms = ['eLCS'], run_parallel = True

    @pytest.mark.parametrize(
        ("algorithms", "run_parallel"),
        [
            (['NB'], False),
            (["LR"], False),
            # (["NB", "LR", "DT"], True),
            # (['CGB'], False),
            # (['LGB'], False),
            # (['XGB'], False),
            # (['GP'], False),
            # (['XCS'], True),
            # ([SUPPORTED_MODELS_SMALL[-1]], True),
    
        ]
        +
        [([algo], True) for algo in SUPPORTED_MODELS_SMALL]
    )
    def test_valid_model_runner(algorithms, run_parallel):
        start = time.time()
    
        logging.warning("Running Modelling Phase")
    
        optuna.logging.set_verbosity(optuna.logging.WARNING)
    
>       runner = ModelExperimentRunner(output_path, experiment_name, algorithms, save_plots=True)

streamline/tests/test_model_runner.py:67: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <streamline.modeling.modelrunner.ModelExperimentRunner object at 0x7fd7540fee50>
output_path = './tests/', experiment_name = 'demo', algorithms = ['eLCS']
exclude = None, class_label = 'Class', instance_label = None
scoring_metric = 'balanced_accuracy', metric_direction = 'maximize'
training_subsample = 0, use_uniform_fi = True, n_trials = 200, timeout = 900
save_plots = True, do_lcs_sweep = False, lcs_nu = 1, lcs_n = 2000
lcs_iterations = 200000, lcs_timeout = 1200, random_state = None, n_jobs = None

    def __init__(self, output_path, experiment_name, algorithms=None, exclude=None, class_label="Class",
                 instance_label=None, scoring_metric='balanced_accuracy', metric_direction='maximize',
                 training_subsample=0, use_uniform_fi=True, n_trials=200,
                 timeout=900, save_plots=False, do_lcs_sweep=False, lcs_nu=1, lcs_n=2000, lcs_iterations=200000,
                 lcs_timeout=1200, random_state=None, n_jobs=None):
    
        """
        Args:
            output_path: path to output directory
            experiment_name: name of experiment (no spaces)
            algorithms: list of str of ML models to run
            scoring_metric: primary scikit-learn specified scoring metric used for hyperparameter optimization and \
                            permutation-based model feature importance evaluation, default='balanced_accuracy'
            metric_direction: direction to optimize the scoring metric in optuna, \
                              either 'maximize' or 'minimize', default='maximize'
            training_subsample: for long running algos (XGB,SVM,ANN,KNN), option to subsample training set \
                                (0 for no subsample, default=0)
            use_uniform_fi: overrides use of any available feature importance estimate methods from models, \
                            instead using permutation_importance uniformly, default=True
            n_trials: number of bayesian hyperparameter optimization trials using optuna \
                      (specify an integer or None) default=200
            timeout: seconds until hyperparameter sweep stops running new trials \
                    (Note: it may run longer to finish last trial started) \
                    If set to None, STREAMLINE is completely replicable, but will take longer to run \
                    default=900 i.e. 900 sec = 15 minutes default \
            save_plots: export optuna-generated hyperparameter sweep plots, default False
            do_lcs_sweep: do LCS hyper-param tuning or use below params, default=False
            lcs_nu: fixed LCS nu param (recommended range 1-10), set to larger value for data with \
                    less or no noise, default=1
            lcs_iterations: fixed LCS number of learning iterations param, default=200000
            lcs_n: fixed LCS rule population maximum size param, default=2000
            lcs_timeout: seconds until hyperparameter sweep stops for LCS algorithms, default=1200
    
        """
        # TODO: What does training subsample do
        self.cv_count = None
        self.dataset = None
        self.output_path = output_path
        self.experiment_name = experiment_name
        self.class_label = class_label
        self.instance_label = instance_label
    
        if algorithms is None:
            self.algorithms = SUPPORTED_MODELS
            if exclude is not None:
                for algorithm in exclude:
                    try:
                        self.algorithms.remove(algorithm)
                    except Exception:
                        logging.error("Unknown algorithm in exclude: " + str(algorithm))
        else:
            for algorithm in algorithms:
                assert is_supported_model(algorithm)
            self.algorithms = algorithms
    
        self.scoring_metric = scoring_metric
        self.metric_direction = metric_direction
        self.training_subsample = training_subsample
        self.uniform_fi = use_uniform_fi
        self.n_trials = n_trials
        self.timeout = timeout
        self.save_plots = save_plots
        self.do_lcs_sweep = do_lcs_sweep
        self.lcs_nu = lcs_nu
        self.lcs_n = lcs_n
        self.lcs_iterations = lcs_iterations
        self.lcs_timeout = lcs_timeout
    
        self.random_state = random_state
        self.n_jobs = n_jobs
    
        # Argument checks
        if not os.path.exists(self.output_path):
>           raise Exception("Output path must exist (from phase 1) before phase 4 can begin")
E           Exception: Output path must exist (from phase 1) before phase 4 can begin

streamline/modeling/modelrunner.py:90: Exception
------------------------------ Captured log call -------------------------------
WARNING  root:test_model_runner.py:63 Running Modelling Phase
__________________ test_valid_model_runner[algorithms15-True] __________________

algorithms = ['XCS'], run_parallel = True

    @pytest.mark.parametrize(
        ("algorithms", "run_parallel"),
        [
            (['NB'], False),
            (["LR"], False),
            # (["NB", "LR", "DT"], True),
            # (['CGB'], False),
            # (['LGB'], False),
            # (['XGB'], False),
            # (['GP'], False),
            # (['XCS'], True),
            # ([SUPPORTED_MODELS_SMALL[-1]], True),
    
        ]
        +
        [([algo], True) for algo in SUPPORTED_MODELS_SMALL]
    )
    def test_valid_model_runner(algorithms, run_parallel):
        start = time.time()
    
        logging.warning("Running Modelling Phase")
    
        optuna.logging.set_verbosity(optuna.logging.WARNING)
    
>       runner = ModelExperimentRunner(output_path, experiment_name, algorithms, save_plots=True)

streamline/tests/test_model_runner.py:67: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <streamline.modeling.modelrunner.ModelExperimentRunner object at 0x7fd7540d2760>
output_path = './tests/', experiment_name = 'demo', algorithms = ['XCS']
exclude = None, class_label = 'Class', instance_label = None
scoring_metric = 'balanced_accuracy', metric_direction = 'maximize'
training_subsample = 0, use_uniform_fi = True, n_trials = 200, timeout = 900
save_plots = True, do_lcs_sweep = False, lcs_nu = 1, lcs_n = 2000
lcs_iterations = 200000, lcs_timeout = 1200, random_state = None, n_jobs = None

    def __init__(self, output_path, experiment_name, algorithms=None, exclude=None, class_label="Class",
                 instance_label=None, scoring_metric='balanced_accuracy', metric_direction='maximize',
                 training_subsample=0, use_uniform_fi=True, n_trials=200,
                 timeout=900, save_plots=False, do_lcs_sweep=False, lcs_nu=1, lcs_n=2000, lcs_iterations=200000,
                 lcs_timeout=1200, random_state=None, n_jobs=None):
    
        """
        Args:
            output_path: path to output directory
            experiment_name: name of experiment (no spaces)
            algorithms: list of str of ML models to run
            scoring_metric: primary scikit-learn specified scoring metric used for hyperparameter optimization and \
                            permutation-based model feature importance evaluation, default='balanced_accuracy'
            metric_direction: direction to optimize the scoring metric in optuna, \
                              either 'maximize' or 'minimize', default='maximize'
            training_subsample: for long running algos (XGB,SVM,ANN,KNN), option to subsample training set \
                                (0 for no subsample, default=0)
            use_uniform_fi: overrides use of any available feature importance estimate methods from models, \
                            instead using permutation_importance uniformly, default=True
            n_trials: number of bayesian hyperparameter optimization trials using optuna \
                      (specify an integer or None) default=200
            timeout: seconds until hyperparameter sweep stops running new trials \
                    (Note: it may run longer to finish last trial started) \
                    If set to None, STREAMLINE is completely replicable, but will take longer to run \
                    default=900 i.e. 900 sec = 15 minutes default \
            save_plots: export optuna-generated hyperparameter sweep plots, default False
            do_lcs_sweep: do LCS hyper-param tuning or use below params, default=False
            lcs_nu: fixed LCS nu param (recommended range 1-10), set to larger value for data with \
                    less or no noise, default=1
            lcs_iterations: fixed LCS number of learning iterations param, default=200000
            lcs_n: fixed LCS rule population maximum size param, default=2000
            lcs_timeout: seconds until hyperparameter sweep stops for LCS algorithms, default=1200
    
        """
        # TODO: What does training subsample do
        self.cv_count = None
        self.dataset = None
        self.output_path = output_path
        self.experiment_name = experiment_name
        self.class_label = class_label
        self.instance_label = instance_label
    
        if algorithms is None:
            self.algorithms = SUPPORTED_MODELS
            if exclude is not None:
                for algorithm in exclude:
                    try:
                        self.algorithms.remove(algorithm)
                    except Exception:
                        logging.error("Unknown algorithm in exclude: " + str(algorithm))
        else:
            for algorithm in algorithms:
                assert is_supported_model(algorithm)
            self.algorithms = algorithms
    
        self.scoring_metric = scoring_metric
        self.metric_direction = metric_direction
        self.training_subsample = training_subsample
        self.uniform_fi = use_uniform_fi
        self.n_trials = n_trials
        self.timeout = timeout
        self.save_plots = save_plots
        self.do_lcs_sweep = do_lcs_sweep
        self.lcs_nu = lcs_nu
        self.lcs_n = lcs_n
        self.lcs_iterations = lcs_iterations
        self.lcs_timeout = lcs_timeout
    
        self.random_state = random_state
        self.n_jobs = n_jobs
    
        # Argument checks
        if not os.path.exists(self.output_path):
>           raise Exception("Output path must exist (from phase 1) before phase 4 can begin")
E           Exception: Output path must exist (from phase 1) before phase 4 can begin

streamline/modeling/modelrunner.py:90: Exception
------------------------------ Captured log call -------------------------------
WARNING  root:test_model_runner.py:63 Running Modelling Phase
__________________ test_valid_model_runner[algorithms16-True] __________________

algorithms = ['ExSTraCS'], run_parallel = True

    @pytest.mark.parametrize(
        ("algorithms", "run_parallel"),
        [
            (['NB'], False),
            (["LR"], False),
            # (["NB", "LR", "DT"], True),
            # (['CGB'], False),
            # (['LGB'], False),
            # (['XGB'], False),
            # (['GP'], False),
            # (['XCS'], True),
            # ([SUPPORTED_MODELS_SMALL[-1]], True),
    
        ]
        +
        [([algo], True) for algo in SUPPORTED_MODELS_SMALL]
    )
    def test_valid_model_runner(algorithms, run_parallel):
        start = time.time()
    
        logging.warning("Running Modelling Phase")
    
        optuna.logging.set_verbosity(optuna.logging.WARNING)
    
>       runner = ModelExperimentRunner(output_path, experiment_name, algorithms, save_plots=True)

streamline/tests/test_model_runner.py:67: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <streamline.modeling.modelrunner.ModelExperimentRunner object at 0x7fd754057f40>
output_path = './tests/', experiment_name = 'demo', algorithms = ['ExSTraCS']
exclude = None, class_label = 'Class', instance_label = None
scoring_metric = 'balanced_accuracy', metric_direction = 'maximize'
training_subsample = 0, use_uniform_fi = True, n_trials = 200, timeout = 900
save_plots = True, do_lcs_sweep = False, lcs_nu = 1, lcs_n = 2000
lcs_iterations = 200000, lcs_timeout = 1200, random_state = None, n_jobs = None

    def __init__(self, output_path, experiment_name, algorithms=None, exclude=None, class_label="Class",
                 instance_label=None, scoring_metric='balanced_accuracy', metric_direction='maximize',
                 training_subsample=0, use_uniform_fi=True, n_trials=200,
                 timeout=900, save_plots=False, do_lcs_sweep=False, lcs_nu=1, lcs_n=2000, lcs_iterations=200000,
                 lcs_timeout=1200, random_state=None, n_jobs=None):
    
        """
        Args:
            output_path: path to output directory
            experiment_name: name of experiment (no spaces)
            algorithms: list of str of ML models to run
            scoring_metric: primary scikit-learn specified scoring metric used for hyperparameter optimization and \
                            permutation-based model feature importance evaluation, default='balanced_accuracy'
            metric_direction: direction to optimize the scoring metric in optuna, \
                              either 'maximize' or 'minimize', default='maximize'
            training_subsample: for long running algos (XGB,SVM,ANN,KNN), option to subsample training set \
                                (0 for no subsample, default=0)
            use_uniform_fi: overrides use of any available feature importance estimate methods from models, \
                            instead using permutation_importance uniformly, default=True
            n_trials: number of bayesian hyperparameter optimization trials using optuna \
                      (specify an integer or None) default=200
            timeout: seconds until hyperparameter sweep stops running new trials \
                    (Note: it may run longer to finish last trial started) \
                    If set to None, STREAMLINE is completely replicable, but will take longer to run \
                    default=900 i.e. 900 sec = 15 minutes default \
            save_plots: export optuna-generated hyperparameter sweep plots, default False
            do_lcs_sweep: do LCS hyper-param tuning or use below params, default=False
            lcs_nu: fixed LCS nu param (recommended range 1-10), set to larger value for data with \
                    less or no noise, default=1
            lcs_iterations: fixed LCS number of learning iterations param, default=200000
            lcs_n: fixed LCS rule population maximum size param, default=2000
            lcs_timeout: seconds until hyperparameter sweep stops for LCS algorithms, default=1200
    
        """
        # TODO: What does training subsample do
        self.cv_count = None
        self.dataset = None
        self.output_path = output_path
        self.experiment_name = experiment_name
        self.class_label = class_label
        self.instance_label = instance_label
    
        if algorithms is None:
            self.algorithms = SUPPORTED_MODELS
            if exclude is not None:
                for algorithm in exclude:
                    try:
                        self.algorithms.remove(algorithm)
                    except Exception:
                        logging.error("Unknown algorithm in exclude: " + str(algorithm))
        else:
            for algorithm in algorithms:
                assert is_supported_model(algorithm)
            self.algorithms = algorithms
    
        self.scoring_metric = scoring_metric
        self.metric_direction = metric_direction
        self.training_subsample = training_subsample
        self.uniform_fi = use_uniform_fi
        self.n_trials = n_trials
        self.timeout = timeout
        self.save_plots = save_plots
        self.do_lcs_sweep = do_lcs_sweep
        self.lcs_nu = lcs_nu
        self.lcs_n = lcs_n
        self.lcs_iterations = lcs_iterations
        self.lcs_timeout = lcs_timeout
    
        self.random_state = random_state
        self.n_jobs = n_jobs
    
        # Argument checks
        if not os.path.exists(self.output_path):
>           raise Exception("Output path must exist (from phase 1) before phase 4 can begin")
E           Exception: Output path must exist (from phase 1) before phase 4 can begin

streamline/modeling/modelrunner.py:90: Exception
------------------------------ Captured log call -------------------------------
WARNING  root:test_model_runner.py:63 Running Modelling Phase
__________________________________ test_stub ___________________________________

    def test_stub():
        if not DEBUG:
>           shutil.rmtree('./tests/')

streamline/tests/test_zclean.py:7: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../../../opt/anaconda3/envs/streamline/lib/python3.8/shutil.py:709: in rmtree
    onerror(os.lstat, path, sys.exc_info())
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

path = './tests/', ignore_errors = False
onerror = <function rmtree.<locals>.onerror at 0x7fd7540daf70>

    def rmtree(path, ignore_errors=False, onerror=None):
        """Recursively delete a directory tree.
    
        If ignore_errors is set, errors are ignored; otherwise, if onerror
        is set, it is called to handle the error with arguments (func,
        path, exc_info) where func is platform and implementation dependent;
        path is the argument to that function that caused it to fail; and
        exc_info is a tuple returned by sys.exc_info().  If ignore_errors
        is false and onerror is None, an exception is raised.
    
        """
        sys.audit("shutil.rmtree", path)
        if ignore_errors:
            def onerror(*args):
                pass
        elif onerror is None:
            def onerror(*args):
                raise
        if _use_fd_functions:
            # While the unsafe rmtree works fine on bytes, the fd based does not.
            if isinstance(path, bytes):
                path = os.fsdecode(path)
            # Note: To guard against symlink races, we use the standard
            # lstat()/open()/fstat() trick.
            try:
>               orig_st = os.lstat(path)
E               FileNotFoundError: [Errno 2] No such file or directory: './tests/'

../../../../opt/anaconda3/envs/streamline/lib/python3.8/shutil.py:707: FileNotFoundError
=============================== warnings summary ===============================
../../../../opt/anaconda3/envs/streamline/lib/python3.8/site-packages/jupyter_client/connect.py:27
  /Users/bandheyh/opt/anaconda3/envs/streamline/lib/python3.8/site-packages/jupyter_client/connect.py:27: DeprecationWarning: Jupyter is migrating its paths to use standard platformdirs
  given by the platformdirs library.  To remove this warning and
  see the appropriate new directories, set the environment variable
  `JUPYTER_PLATFORM_DIRS=1` and then run `jupyter --paths`.
  The use of platformdirs will be the default in `jupyter_core` v6
    from jupyter_core.paths import jupyter_data_dir

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
=========================== short test summary info ============================
FAILED streamline/tests/test_model_runner.py::test_setup - Exception: Error: ...
FAILED streamline/tests/test_model_runner.py::test_valid_model_runner[algorithms0-False]
FAILED streamline/tests/test_model_runner.py::test_valid_model_runner[algorithms1-False]
FAILED streamline/tests/test_model_runner.py::test_valid_model_runner[algorithms2-True]
FAILED streamline/tests/test_model_runner.py::test_valid_model_runner[algorithms3-True]
FAILED streamline/tests/test_model_runner.py::test_valid_model_runner[algorithms4-True]
FAILED streamline/tests/test_model_runner.py::test_valid_model_runner[algorithms5-True]
FAILED streamline/tests/test_model_runner.py::test_valid_model_runner[algorithms6-True]
FAILED streamline/tests/test_model_runner.py::test_valid_model_runner[algorithms7-True]
FAILED streamline/tests/test_model_runner.py::test_valid_model_runner[algorithms8-True]
FAILED streamline/tests/test_model_runner.py::test_valid_model_runner[algorithms9-True]
FAILED streamline/tests/test_model_runner.py::test_valid_model_runner[algorithms10-True]
FAILED streamline/tests/test_model_runner.py::test_valid_model_runner[algorithms11-True]
FAILED streamline/tests/test_model_runner.py::test_valid_model_runner[algorithms12-True]
FAILED streamline/tests/test_model_runner.py::test_valid_model_runner[algorithms13-True]
FAILED streamline/tests/test_model_runner.py::test_valid_model_runner[algorithms14-True]
FAILED streamline/tests/test_model_runner.py::test_valid_model_runner[algorithms15-True]
FAILED streamline/tests/test_model_runner.py::test_valid_model_runner[algorithms16-True]
FAILED streamline/tests/test_zclean.py::test_stub - FileNotFoundError: [Errno...
================== 19 failed, 51 skipped, 1 warning in 2.58s ===================
============================= test session starts ==============================
platform darwin -- Python 3.8.15, pytest-7.2.1, pluggy-1.0.0 -- /Users/bandheyh/opt/anaconda3/envs/streamline/bin/python
cachedir: .pytest_cache
rootdir: /Users/bandheyh/Local/Cedars/UrbsLab/STREAMLINE_Dev
plugins: pycodestyle-2.3.1, anyio-3.5.0, profiling-1.7.0
collecting ... collected 64 items / 6 skipped

setup.py::PYCODESTYLE SKIPPED (previously passed pycodestyle checks)     [  1%]
docs/source/conf.py::PYCODESTYLE SKIPPED (previously passed pycodest...) [  3%]
streamline/__init__.py::PYCODESTYLE SKIPPED (previously passed pycod...) [  4%]
streamline/dataprep/__init__.py::PYCODESTYLE SKIPPED (previously pas...) [  6%]
streamline/dataprep/data_process.py::PYCODESTYLE SKIPPED (previously...) [  7%]
streamline/dataprep/eda_runner.py::PYCODESTYLE SKIPPED (previously p...) [  9%]
streamline/dataprep/exploratory_analysis.py::PYCODESTYLE SKIPPED (pr...) [ 10%]
streamline/dataprep/kfold_partitioning.py::PYCODESTYLE SKIPPED (prev...) [ 12%]
streamline/featurefns/__init__.py::PYCODESTYLE SKIPPED (previously p...) [ 14%]
streamline/featurefns/feature_runner.py::PYCODESTYLE SKIPPED (previo...) [ 15%]
streamline/featurefns/importance.py::PYCODESTYLE SKIPPED (previously...) [ 17%]
streamline/featurefns/selection.py::PYCODESTYLE SKIPPED (previously ...) [ 18%]
streamline/modeling/__init__.py::PYCODESTYLE SKIPPED (previously pas...) [ 20%]
streamline/modeling/basemodel.py::PYCODESTYLE SKIPPED (previously pa...) [ 21%]
streamline/modeling/modeljob.py::PYCODESTYLE SKIPPED (previously pas...) [ 23%]
streamline/modeling/modelrunner.py::PYCODESTYLE SKIPPED (previously ...) [ 25%]
streamline/modeling/parameters.py::PYCODESTYLE SKIPPED (previously p...) [ 26%]
streamline/modeling/utils.py::PYCODESTYLE SKIPPED (previously passed...) [ 28%]
streamline/models/__init__.py::PYCODESTYLE SKIPPED (previously passe...) [ 29%]
streamline/models/artificial_neural_network.py::PYCODESTYLE SKIPPED      [ 31%]
streamline/models/decision_tree.py::PYCODESTYLE SKIPPED (previously ...) [ 32%]
streamline/models/genetic_programming.py::PYCODESTYLE SKIPPED (previ...) [ 34%]
streamline/models/gradient_boosting.py::PYCODESTYLE SKIPPED (previou...) [ 35%]
streamline/models/learning_based.py::PYCODESTYLE SKIPPED (previously...) [ 37%]
streamline/models/linear_model.py::PYCODESTYLE SKIPPED (previously p...) [ 39%]
streamline/models/naive_bayes.py::PYCODESTYLE SKIPPED (previously pa...) [ 40%]
streamline/models/neighbouring.py::PYCODESTYLE SKIPPED (previously p...) [ 42%]
streamline/models/random_forest.py::PYCODESTYLE SKIPPED (previously ...) [ 43%]
streamline/models/support_vector_machine.py::PYCODESTYLE SKIPPED (pr...) [ 45%]
streamline/postanalysis/__init__.py::PYCODESTYLE SKIPPED (previously...) [ 46%]
streamline/postanalysis/statistics.py::PYCODESTYLE SKIPPED (previous...) [ 48%]
streamline/tests/__init__.py::PYCODESTYLE SKIPPED (previously passed...) [ 50%]
streamline/tests/test_dataprep.py::PYCODESTYLE SKIPPED (previously p...) [ 51%]
streamline/tests/test_dataprocess.py::PYCODESTYLE SKIPPED (previousl...) [ 53%]
streamline/tests/test_edarunner.py::PYCODESTYLE SKIPPED (previously ...) [ 54%]
streamline/tests/test_featurefns.py::PYCODESTYLE SKIPPED (previously...) [ 56%]
streamline/tests/test_kfold.py::PYCODESTYLE SKIPPED (previously pass...) [ 57%]
streamline/tests/test_model.py::PYCODESTYLE SKIPPED (previously pass...) [ 59%]
streamline/tests/test_model_runner.py::PYCODESTYLE SKIPPED (previous...) [ 60%]
streamline/tests/test_model_runner.py::test_setup PASSED                 [ 62%]
streamline/tests/test_model_runner.py::test_valid_model_runner[algorithms0-False] 
-------------------------------- live log call ---------------------------------
WARNING  root:test_model_runner.py:63 Running Modelling Phase
WARNING  root:test_model_runner.py:74 Modelling Step with ['NB'], Time running serially: 1.4583301544189453
PASSED                                                                   [ 64%]
streamline/tests/test_model_runner.py::test_valid_model_runner[algorithms1-False] 
-------------------------------- live log call ---------------------------------
WARNING  root:test_model_runner.py:63 Running Modelling Phase
WARNING  root:test_model_runner.py:74 Modelling Step with ['LR'], Time running serially: 50.123366355895996
PASSED                                                                   [ 65%]
streamline/tests/test_model_runner.py::test_valid_model_runner[algorithms2-True] 
-------------------------------- live log call ---------------------------------
WARNING  root:test_model_runner.py:63 Running Modelling Phase
WARNING  root:test_model_runner.py:74 Modelling Step with ['NB'], Time running parallely: 5.634066104888916
PASSED                                                                   [ 67%]
streamline/tests/test_model_runner.py::test_valid_model_runner[algorithms3-True] 
-------------------------------- live log call ---------------------------------
WARNING  root:test_model_runner.py:63 Running Modelling Phase
WARNING  root:test_model_runner.py:74 Modelling Step with ['LR'], Time running parallely: 24.700824975967407
PASSED                                                                   [ 68%]
streamline/tests/test_model_runner.py::test_valid_model_runner[algorithms4-True] 
-------------------------------- live log call ---------------------------------
WARNING  root:test_model_runner.py:63 Running Modelling Phase
WARNING  root:test_model_runner.py:74 Modelling Step with ['DT'], Time running parallely: 20.252896070480347
PASSED                                                                   [ 70%]
streamline/tests/test_model_runner.py::test_valid_model_runner[algorithms5-True] 
-------------------------------- live log call ---------------------------------
WARNING  root:test_model_runner.py:63 Running Modelling Phase
WARNING  root:test_model_runner.py:74 Modelling Step with ['RF'], Time running parallely: 726.1978576183319
PASSED                                                                   [ 71%]
streamline/tests/test_model_runner.py::test_valid_model_runner[algorithms6-True] 
-------------------------------- live log call ---------------------------------
WARNING  root:test_model_runner.py:63 Running Modelling Phase
WARNING  root:test_model_runner.py:74 Modelling Step with ['GB'], Time running parallely: 272.3604428768158
PASSED                                                                   [ 73%]
streamline/tests/test_model_runner.py::test_valid_model_runner[algorithms7-True] 
-------------------------------- live log call ---------------------------------
WARNING  root:test_model_runner.py:63 Running Modelling Phase
WARNING  root:test_model_runner.py:74 Modelling Step with ['XGB'], Time running parallely: 148.6641640663147
PASSED                                                                   [ 75%]
streamline/tests/test_model_runner.py::test_valid_model_runner[algorithms8-True] 
-------------------------------- live log call ---------------------------------
WARNING  root:test_model_runner.py:63 Running Modelling Phase
WARNING  root:test_model_runner.py:74 Modelling Step with ['LGB'], Time running parallely: 959.0102689266205
PASSED                                                                   [ 76%]
streamline/tests/test_model_runner.py::test_valid_model_runner[algorithms9-True] 
-------------------------------- live log call ---------------------------------
WARNING  root:test_model_runner.py:63 Running Modelling Phase
WARNING  root:test_model_runner.py:74 Modelling Step with ['CGB'], Time running parallely: 913.1056849956512
PASSED                                                                   [ 78%]
streamline/tests/test_model_runner.py::test_valid_model_runner[algorithms10-True] 
-------------------------------- live log call ---------------------------------
WARNING  root:test_model_runner.py:63 Running Modelling Phase
WARNING  root:test_model_runner.py:74 Modelling Step with ['SVM'], Time running parallely: 37.290095806121826
PASSED                                                                   [ 79%]
streamline/tests/test_model_runner.py::test_valid_model_runner[algorithms11-True] 
-------------------------------- live log call ---------------------------------
WARNING  root:test_model_runner.py:63 Running Modelling Phase
WARNING  root:test_model_runner.py:74 Modelling Step with ['ANN'], Time running parallely: 920.4645540714264
PASSED                                                                   [ 81%]
streamline/tests/test_model_runner.py::test_valid_model_runner[algorithms12-True] 
-------------------------------- live log call ---------------------------------
WARNING  root:test_model_runner.py:63 Running Modelling Phase
WARNING  root:test_model_runner.py:74 Modelling Step with ['KNN'], Time running parallely: 38.365131855010986
PASSED                                                                   [ 82%]
streamline/tests/test_model_runner.py::test_valid_model_runner[algorithms13-True] 
-------------------------------- live log call ---------------------------------
WARNING  root:test_model_runner.py:63 Running Modelling Phase
WARNING  root:test_model_runner.py:74 Modelling Step with ['GP'], Time running parallely: 1251.9255049228668
PASSED                                                                   [ 84%]
streamline/tests/test_model_runner.py::test_valid_model_runner[algorithms14-True] 
-------------------------------- live log call ---------------------------------
WARNING  root:test_model_runner.py:63 Running Modelling Phase
WARNING  root:test_model_runner.py:74 Modelling Step with ['eLCS'], Time running parallely: 1143.6145389080048
PASSED                                                                   [ 85%]
streamline/tests/test_model_runner.py::test_valid_model_runner[algorithms15-True] 
-------------------------------- live log call ---------------------------------
WARNING  root:test_model_runner.py:63 Running Modelling Phase
WARNING  root:test_model_runner.py:74 Modelling Step with ['XCS'], Time running parallely: 1147.8058848381042
PASSED                                                                   [ 87%]
streamline/tests/test_model_runner.py::test_valid_model_runner[algorithms16-True] 
-------------------------------- live log call ---------------------------------
WARNING  root:test_model_runner.py:63 Running Modelling Phase
WARNING  root:test_model_runner.py:74 Modelling Step with ['ExSTraCS'], Time running parallely: 2157.958104133606
PASSED                                                                   [ 89%]
streamline/tests/test_zclean.py::PYCODESTYLE SKIPPED (previously pas...) [ 90%]
streamline/tests/test_zclean.py::test_stub PASSED                        [ 92%]
streamline/utils/__init__.py::PYCODESTYLE SKIPPED (previously passed...) [ 93%]
streamline/utils/dataset.py::PYCODESTYLE SKIPPED (previously passed ...) [ 95%]
streamline/utils/evaluation.py::PYCODESTYLE SKIPPED (previously pass...) [ 96%]
streamline/utils/job.py::PYCODESTYLE SKIPPED (previously passed pyco...) [ 98%]
streamline/utils/modelutils.py::PYCODESTYLE SKIPPED (previously pass...) [100%]

=============================== warnings summary ===============================
../../../../opt/anaconda3/envs/streamline/lib/python3.8/site-packages/jupyter_client/connect.py:27
  /Users/bandheyh/opt/anaconda3/envs/streamline/lib/python3.8/site-packages/jupyter_client/connect.py:27: DeprecationWarning: Jupyter is migrating its paths to use standard platformdirs
  given by the platformdirs library.  To remove this warning and
  see the appropriate new directories, set the environment variable
  `JUPYTER_PLATFORM_DIRS=1` and then run `jupyter --paths`.
  The use of platformdirs will be the default in `jupyter_core` v6
    from jupyter_core.paths import jupyter_data_dir

streamline/tests/test_model_runner.py: 2000 warnings
  /Users/bandheyh/Local/Cedars/UrbsLab/STREAMLINE_Dev/streamline/models/linear_model.py:20: FutureWarning:
  
  suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.

streamline/tests/test_model_runner.py::test_valid_model_runner[algorithms1-False]
streamline/tests/test_model_runner.py::test_valid_model_runner[algorithms1-False]
  /Users/bandheyh/opt/anaconda3/envs/streamline/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning:
  
  The max_iter was reached which means the coef_ did not converge

streamline/tests/test_model_runner.py::test_valid_model_runner[algorithms1-False]
  /Users/bandheyh/opt/anaconda3/envs/streamline/lib/python3.8/site-packages/sklearn/utils/optimize.py:210: ConvergenceWarning:
  
  newton-cg failed to converge. Increase the number of iterations.

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
========== 19 passed, 51 skipped, 2004 warnings in 9832.15s (2:43:52) ==========
