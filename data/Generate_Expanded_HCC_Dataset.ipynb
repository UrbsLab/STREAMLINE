{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMS+w45V9IPVM3kAXv7oYci"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Create simulated replication data for original HCC dataset"],"metadata":{"id":"OfstZAEElnPB"}},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","\n","# Specify the path to your CSV dataset\n","csv_file = \"/content/hcc-data_example.csv\"\n","\n","column_to_exclude = 'InstanceID'\n","\n","# Specify the proportion of instances to replace (between 0 and 1)\n","replace_proportion = 0.3\n","\n","# Find the last occurrence of the directory separator '/'\n","last_separator_index = csv_file.rfind('/')\n","\n","# Extract the file path\n","file_path = csv_file[:last_separator_index] if last_separator_index != -1 else ''\n","\n","# Find the extension separator '.'\n","extension_separator_index = csv_file.rfind('.')\n","\n","# Extract the file name without the extension\n","file_name = csv_file[last_separator_index + 1 : extension_separator_index] if last_separator_index != -1 else csv_file[:extension_separator_index]\n","\n","# Print the extracted file path and file name\n","print(\"File Path:\", file_path)\n","print(\"File Name:\", file_name)\n","\n","# Load the CSV dataset\n","data = pd.read_csv(csv_file)\n","\n","# Select random instances to replace\n","replace_indices = np.random.choice(len(data), size=int(len(data) * replace_proportion), replace=False)\n","\n","# Iterate over the selected indices and replace feature values\n","for index in replace_indices:\n","    # Get the values of the current instance\n","    instance_values = data.iloc[index, :]\n","\n","    # Iterate over the features\n","    for feature in instance_values.index:\n","        # Check if the current feature is the one to exclude\n","        if feature == column_to_exclude:\n","            # Change the value of the excluded feature\n","            data.at[index, feature] = str(instance_values[feature]) + \"_random\"\n","        else:\n","            # Compute the distribution of the feature values in the rest of the dataset\n","            feature_distribution = data[data.index != index][feature]\n","\n","            # Generate a new feature value that resembles the rest of the dataset\n","            new_value = np.random.choice(feature_distribution)\n","\n","            # Assign the new feature value to the current instance\n","            data.at[index, feature] = new_value\n","\n","# Save the updated dataset with the simulated features\n","output_file = file_path+'/'+file_name+'_rep.csv'\n","data.to_csv(output_file, index=False)\n","print(f\"Updated dataset saved to {output_file}.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"haDZyxwslmgw","executionInfo":{"status":"ok","timestamp":1684477405468,"user_tz":420,"elapsed":1255,"user":{"displayName":"ryan urbanowicz","userId":"09525282221743790591"}},"outputId":"3b84377c-2ba2-4c55-e194-9bc113f2877c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["File Path: /content\n","File Name: hcc-data_example\n","Updated dataset saved to /content/hcc-data_example_rep.csv.\n"]}]},{"cell_type":"markdown","source":["# Create Custom HCC dataset for STREAMLINE Testing"],"metadata":{"id":"JeDRAh5ijI8c"}},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import random\n","\n","class_label = 'Class'\n","instance_ID_label = 'InstanceID'\n","features_to_remove = ['Gender','Age at diagnosis']\n","cat_feature_list = [3,5]\n","miss_feature_list = [0.5,0.7]\n","corr_feature_list = [-1.0,0.9,1.0]\n","num_nolabel_instances = 2\n","miss_instance_list = [0.7,0.8]\n","\n","# Specify the path to your existing CSV dataset\n","csv_file = \"/content/hcc-data_example.csv\"\n","\n","def remove_features(data, features_to_remove):\n","    return data.drop(features_to_remove, axis=1)\n","\n","def generate_categorical_feature(num_categories, num_rows):\n","    categories = [f\"Category {i+1}\" for i in range(num_categories)]\n","    return np.random.choice(categories, size=num_rows)\n","\n","def generate_categorical_feature_numerical_encode(num_categories, num_rows):\n","    categories = np.arange(1, num_categories + 1)\n","    return np.random.choice(categories, size=num_rows)\n","\n","def generate_quantitative_feature(num_rows, missing_percentage):\n","    data = np.random.rand(num_rows)\n","    missing_mask = np.random.choice([False, True], size=num_rows, p=[1-missing_percentage, missing_percentage])\n","    data[missing_mask] = np.nan\n","    return data\n","\n","def generate_correlated_values(feature1, correlation):\n","    # Generate the second feature correlated with the first feature\n","    feature2 = correlation * feature1 + np.random.normal(0, np.sqrt(1 - correlation**2), len(feature1))\n","    return feature2\n","\n","# Find the last occurrence of the directory separator '/'\n","last_separator_index = csv_file.rfind('/')\n","\n","# Extract the file path\n","file_path = csv_file[:last_separator_index] if last_separator_index != -1 else ''\n","\n","# Find the extension separator '.'\n","extension_separator_index = csv_file.rfind('.')\n","\n","# Extract the file name without the extension\n","file_name = csv_file[last_separator_index + 1 : extension_separator_index] if last_separator_index != -1 else csv_file[:extension_separator_index]\n","\n","# Print the extracted file path and file name\n","print(\"File Path:\", file_path)\n","print(\"File Name:\", file_name)\n","\n","# Load the existing CSV dataset\n","data = pd.read_csv(csv_file)\n","\n","# Remove specified features from original dataset\n","data = remove_features(data, features_to_remove)\n","\n","# Generate random instances resembling existing dataset and add to the dataset that have missing class label\n","for _ in range(num_nolabel_instances):\n","    random_instance = data.sample(n=1, replace=True)\n","    random_instance[class_label] = np.nan\n","    random_instance[instance_ID_label] = 'no_class'\n","    data = pd.concat([data, random_instance], ignore_index=True)\n","\n","# Generate random instances resembling existing dataset and add to the dataset that have missing class label\n","for miss in miss_instance_list:\n","    random_instance = data.sample(n=1, replace=True)\n","    num_features = len(data.columns)\n","    num_missing_values = int(num_features * miss)\n","    random_features = np.random.choice(data.columns, size=num_missing_values, replace=False)\n","    random_instance[random_features] = np.nan\n","    random_instance[instance_ID_label] = 'miss_'+str(miss)\n","    # Randomly choose a value of 0 or 1\n","    value = random.choice([0, 1])\n","    random_instance[class_label] = value\n","    data = pd.concat([data, random_instance], ignore_index=True)\n","\n","# Simulate the categorical feature and add it to the dataset\n","num_rows = len(data)\n","for cat in cat_feature_list:\n","    simulated_categorical_feature = generate_categorical_feature_numerical_encode(cat, num_rows)\n","    data['Sim_Cat_'+str(cat)] = simulated_categorical_feature\n","\n","# Simulate the quantitative feature and add it to the dataset\n","for miss in miss_feature_list:\n","    simulated_quant_feature = generate_quantitative_feature(num_rows, miss)\n","    data['Sim_Miss_'+str(miss)] = simulated_quant_feature\n","\n","# Simulate the correlated variables and add them to the dataset\n","for corr in corr_feature_list:\n","    # Generate a new random quantitative feature\n","    new_feature = np.random.rand(len(data))\n","\n","    # Generate the correlated feature based on the new feature\n","    correlated_feature = generate_correlated_values(new_feature, corr)\n","\n","    # Add the new features to the dataset\n","    data['Sim_Cor_'+str(corr)+'_A'] = new_feature\n","    data['Sim_Cor_'+str(corr)+'_B'] = correlated_feature\n","\n","# Save the updated dataset with the simulated features\n","output_file = file_path+'/'+file_name+'_custom.csv'\n","data.to_csv(output_file, index=False)\n","print(f\"Updated dataset saved to {output_file}.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pBMJv6IpJQeb","executionInfo":{"status":"ok","timestamp":1684477405468,"user_tz":420,"elapsed":3,"user":{"displayName":"ryan urbanowicz","userId":"09525282221743790591"}},"outputId":"27be38ca-c17b-4a1a-baac-a65cdb7808e6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["File Path: /content\n","File Name: hcc-data_example\n","Updated dataset saved to /content/hcc-data_example_custom.csv.\n"]}]},{"cell_type":"markdown","source":["# Create replication dataset for the custom HCC dataset."],"metadata":{"id":"d3ibZorIqrMn"}},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","\n","# Specify the path to your CSV dataset\n","csv_file = \"/content/hcc-data_example_custom.csv\"\n","\n","column_to_exclude = 'InstanceID'\n","\n","# Specify the proportion of instances to replace (between 0 and 1)\n","replace_proportion = 0.3\n","\n","# Find the last occurrence of the directory separator '/'\n","last_separator_index = csv_file.rfind('/')\n","\n","# Extract the file path\n","file_path = csv_file[:last_separator_index] if last_separator_index != -1 else ''\n","\n","# Find the extension separator '.'\n","extension_separator_index = csv_file.rfind('.')\n","\n","# Extract the file name without the extension\n","file_name = csv_file[last_separator_index + 1 : extension_separator_index] if last_separator_index != -1 else csv_file[:extension_separator_index]\n","\n","# Print the extracted file path and file name\n","print(\"File Path:\", file_path)\n","print(\"File Name:\", file_name)\n","\n","# Load the CSV dataset\n","data = pd.read_csv(csv_file)\n","\n","# Select random instances to replace\n","replace_indices = np.random.choice(len(data), size=int(len(data) * replace_proportion), replace=False)\n","\n","# Iterate over the selected indices and replace feature values\n","for index in replace_indices:\n","    # Get the values of the current instance\n","    instance_values = data.iloc[index, :]\n","\n","    # Iterate over the features\n","    for feature in instance_values.index:\n","        # Check if the current feature is the one to exclude\n","        if feature == column_to_exclude:\n","            # Change the value of the excluded feature\n","            data.at[index, feature] = str(instance_values[feature]) + \"_random\"\n","        else:\n","            # Compute the distribution of the feature values in the rest of the dataset\n","            feature_distribution = data[data.index != index][feature]\n","\n","            # Generate a new feature value that resembles the rest of the dataset\n","            new_value = np.random.choice(feature_distribution)\n","\n","            # Assign the new feature value to the current instance\n","            data.at[index, feature] = new_value\n","\n","# Save the updated dataset with the simulated features\n","output_file = file_path+'/'+file_name+'_rep.csv'\n","data.to_csv(output_file, index=False)\n","print(f\"Updated dataset saved to {output_file}.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"X0jmek1-Nqua","executionInfo":{"status":"ok","timestamp":1684477407195,"user_tz":420,"elapsed":1728,"user":{"displayName":"ryan urbanowicz","userId":"09525282221743790591"}},"outputId":"3f9eb404-5c7e-4e58-c69e-6529005ae1e6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["File Path: /content\n","File Name: hcc-data_example_custom\n","Updated dataset saved to /content/hcc-data_example_custom_rep.csv.\n"]}]},{"cell_type":"markdown","source":["# Create Custom HCC dataset for STREAMLINE Testing (with some text variables)"],"metadata":{"id":"f8vlWxqWdPUm"}},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import random\n","\n","class_label = 'Class'\n","instance_ID_label = 'InstanceID'\n","features_to_remove = ['Gender','Age at diagnosis']\n","cat_feature_list = [3,5]\n","cat_feature_list_text = [3,4]\n","miss_feature_list = [0.5,0.7]\n","corr_feature_list = [-1.0,0.9,1.0]\n","num_nolabel_instances = 2\n","miss_instance_list = [0.7,0.8]\n","\n","# Specify the path to your existing CSV dataset\n","csv_file = \"/content/hcc-data_example.csv\"\n","\n","def remove_features(data, features_to_remove):\n","    return data.drop(features_to_remove, axis=1)\n","\n","def generate_categorical_feature(num_categories, num_rows):\n","    categories = [f\"Category {i+1}\" for i in range(num_categories)]\n","    return np.random.choice(categories, size=num_rows)\n","\n","def generate_categorical_feature_numerical_encode(num_categories, num_rows):\n","    categories = np.arange(1, num_categories + 1)\n","    return np.random.choice(categories, size=num_rows)\n","\n","def generate_quantitative_feature(num_rows, missing_percentage):\n","    data = np.random.rand(num_rows)\n","    missing_mask = np.random.choice([False, True], size=num_rows, p=[1-missing_percentage, missing_percentage])\n","    data[missing_mask] = np.nan\n","    return data\n","\n","def generate_correlated_values(feature1, correlation):\n","    # Generate the second feature correlated with the first feature\n","    feature2 = correlation * feature1 + np.random.normal(0, np.sqrt(1 - correlation**2), len(feature1))\n","    return feature2\n","\n","# Find the last occurrence of the directory separator '/'\n","last_separator_index = csv_file.rfind('/')\n","\n","# Extract the file path\n","file_path = csv_file[:last_separator_index] if last_separator_index != -1 else ''\n","\n","# Find the extension separator '.'\n","extension_separator_index = csv_file.rfind('.')\n","\n","# Extract the file name without the extension\n","file_name = csv_file[last_separator_index + 1 : extension_separator_index] if last_separator_index != -1 else csv_file[:extension_separator_index]\n","\n","# Print the extracted file path and file name\n","print(\"File Path:\", file_path)\n","print(\"File Name:\", file_name)\n","\n","# Load the existing CSV dataset\n","data = pd.read_csv(csv_file)\n","\n","# Remove specified features from original dataset\n","data = remove_features(data, features_to_remove)\n","\n","# Generate random instances resembling existing dataset and add to the dataset that have missing class label\n","for _ in range(num_nolabel_instances):\n","    random_instance = data.sample(n=1, replace=True)\n","    random_instance[class_label] = np.nan\n","    random_instance[instance_ID_label] = 'no_class'\n","    data = pd.concat([data, random_instance], ignore_index=True)\n","\n","# Generate random instances resembling existing dataset and add to the dataset that have missing class label\n","for miss in miss_instance_list:\n","    random_instance = data.sample(n=1, replace=True)\n","    num_features = len(data.columns)\n","    num_missing_values = int(num_features * miss)\n","    random_features = np.random.choice(data.columns, size=num_missing_values, replace=False)\n","    random_instance[random_features] = np.nan\n","    random_instance[instance_ID_label] = 'miss_'+str(miss)\n","    # Randomly choose a value of 0 or 1\n","    value = random.choice([0, 1])\n","    random_instance[class_label] = value\n","    data = pd.concat([data, random_instance], ignore_index=True)\n","\n","# Simulate the categorical feature and add it to the dataset\n","num_rows = len(data)\n","for cat in cat_feature_list:\n","    simulated_categorical_feature = generate_categorical_feature_numerical_encode(cat, num_rows)\n","    data['Sim_Cat_'+str(cat)] = simulated_categorical_feature\n","\n","# Simulate the text-based categorical feature and add it to the dataset\n","num_rows = len(data)\n","for cat in cat_feature_list_text:\n","    simulated_categorical_feature = generate_categorical_feature(cat, num_rows)\n","    data['Sim_Text_Cat_'+str(cat)] = simulated_categorical_feature\n","\n","# Simulate the quantitative feature and add it to the dataset\n","for miss in miss_feature_list:\n","    simulated_quant_feature = generate_quantitative_feature(num_rows, miss)\n","    data['Sim_Miss_'+str(miss)] = simulated_quant_feature\n","\n","# Simulate the correlated variables and add them to the dataset\n","for corr in corr_feature_list:\n","    # Generate a new random quantitative feature\n","    new_feature = np.random.rand(len(data))\n","\n","    # Generate the correlated feature based on the new feature\n","    correlated_feature = generate_correlated_values(new_feature, corr)\n","\n","    # Add the new features to the dataset\n","    data['Sim_Cor_'+str(corr)+'_A'] = new_feature\n","    data['Sim_Cor_'+str(corr)+'_B'] = correlated_feature\n","\n","# Save the updated dataset with the simulated features\n","output_file = file_path+'/'+file_name+'_custom.csv'\n","data.to_csv(output_file, index=False)\n","print(f\"Updated dataset saved to {output_file}.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fau-KvVEdPi9","executionInfo":{"status":"ok","timestamp":1684507587943,"user_tz":420,"elapsed":2853,"user":{"displayName":"ryan urbanowicz","userId":"09525282221743790591"}},"outputId":"0a4714f7-4ef5-4f0b-e716-fd4a60f289fc"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["File Path: /content\n","File Name: hcc-data_example\n","Updated dataset saved to /content/hcc-data_example_custom.csv.\n"]}]},{"cell_type":"markdown","source":["# Create replication dataset for the custom HCC dataset (with some text variables)\n"],"metadata":{"id":"zf8yWymwdPrh"}},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","\n","# Specify the path to your CSV dataset\n","csv_file = \"/content/hcc-data_example_custom.csv\"\n","\n","column_to_exclude = 'InstanceID'\n","\n","# Specify the proportion of instances to replace (between 0 and 1)\n","replace_proportion = 0.3\n","\n","# Find the last occurrence of the directory separator '/'\n","last_separator_index = csv_file.rfind('/')\n","\n","# Extract the file path\n","file_path = csv_file[:last_separator_index] if last_separator_index != -1 else ''\n","\n","# Find the extension separator '.'\n","extension_separator_index = csv_file.rfind('.')\n","\n","# Extract the file name without the extension\n","file_name = csv_file[last_separator_index + 1 : extension_separator_index] if last_separator_index != -1 else csv_file[:extension_separator_index]\n","\n","# Print the extracted file path and file name\n","print(\"File Path:\", file_path)\n","print(\"File Name:\", file_name)\n","\n","# Load the CSV dataset\n","data = pd.read_csv(csv_file)\n","\n","# Select random instances to replace\n","replace_indices = np.random.choice(len(data), size=int(len(data) * replace_proportion), replace=False)\n","\n","# Iterate over the selected indices and replace feature values\n","for index in replace_indices:\n","    # Get the values of the current instance\n","    instance_values = data.iloc[index, :]\n","\n","    # Iterate over the features\n","    for feature in instance_values.index:\n","        # Check if the current feature is the one to exclude\n","        if feature == column_to_exclude:\n","            # Change the value of the excluded feature\n","            data.at[index, feature] = str(instance_values[feature]) + \"_random\"\n","        else:\n","            # Compute the distribution of the feature values in the rest of the dataset\n","            feature_distribution = data[data.index != index][feature]\n","\n","            # Generate a new feature value that resembles the rest of the dataset\n","            new_value = np.random.choice(feature_distribution)\n","\n","            # Assign the new feature value to the current instance\n","            data.at[index, feature] = new_value\n","\n","# Save the updated dataset with the simulated features\n","output_file = file_path+'/'+file_name+'_rep.csv'\n","data.to_csv(output_file, index=False)\n","print(f\"Updated dataset saved to {output_file}.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eS2x07BbdP2G","executionInfo":{"status":"ok","timestamp":1684507614444,"user_tz":420,"elapsed":2513,"user":{"displayName":"ryan urbanowicz","userId":"09525282221743790591"}},"outputId":"869f264c-2020-426b-85d2-ffa48a4f95a6"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["File Path: /content\n","File Name: hcc-data_example_custom\n","Updated dataset saved to /content/hcc-data_example_custom_rep.csv.\n"]}]}]}