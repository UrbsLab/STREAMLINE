{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNjrGbasy1zXK4+yGy2/IkZ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Create simulated replication data for original HCC dataset"],"metadata":{"id":"OfstZAEElnPB"}},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","\n","random_seed = 42\n","np.random.seed(seed=random_seed)\n","\n","# Specify the path to your CSV dataset\n","csv_file = \"/content/hcc-data_example.csv\"\n","\n","column_to_exclude = 'InstanceID'\n","\n","# Specify the proportion of instances to replace (between 0 and 1)\n","replace_proportion = 0.3\n","\n","# Find the last occurrence of the directory separator '/'\n","last_separator_index = csv_file.rfind('/')\n","\n","# Extract the file path\n","file_path = csv_file[:last_separator_index] if last_separator_index != -1 else ''\n","\n","# Find the extension separator '.'\n","extension_separator_index = csv_file.rfind('.')\n","\n","# Extract the file name without the extension\n","file_name = csv_file[last_separator_index + 1 : extension_separator_index] if last_separator_index != -1 else csv_file[:extension_separator_index]\n","\n","# Print the extracted file path and file name\n","print(\"File Path:\", file_path)\n","print(\"File Name:\", file_name)\n","\n","# Load the CSV dataset\n","data = pd.read_csv(csv_file)\n","\n","# Select random instances to replace\n","replace_indices = np.random.choice(len(data), size=int(len(data) * replace_proportion), replace=False)\n","\n","# Iterate over the selected indices and replace feature values\n","for index in replace_indices:\n","    # Get the values of the current instance\n","    instance_values = data.iloc[index, :]\n","\n","    # Iterate over the features\n","    for feature in instance_values.index:\n","        # Check if the current feature is the one to exclude\n","        if feature == column_to_exclude:\n","            # Change the value of the excluded feature\n","            data.at[index, feature] = str(instance_values[feature]) + \"_random\"\n","        else:\n","            # Compute the distribution of the feature values in the rest of the dataset\n","            feature_distribution = data[data.index != index][feature]\n","\n","            # Generate a new feature value that resembles the rest of the dataset\n","            new_value = np.random.choice(feature_distribution)\n","\n","            # Assign the new feature value to the current instance\n","            data.at[index, feature] = new_value\n","\n","# Save the updated dataset with the simulated features\n","output_file = file_path+'/'+file_name+'_rep.csv'\n","data.to_csv(output_file, index=False)\n","print(f\"Updated dataset saved to {output_file}.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"haDZyxwslmgw","executionInfo":{"status":"ok","timestamp":1685747659893,"user_tz":420,"elapsed":1400,"user":{"displayName":"ryan urbanowicz","userId":"09525282221743790591"}},"outputId":"9f4f0db3-a0c0-47a5-87b1-73662e4688fe"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["File Path: /content\n","File Name: hcc-data_example\n","Updated dataset saved to /content/hcc-data_example_rep.csv.\n"]}]},{"cell_type":"markdown","source":["# Create Custom HCC dataset for STREAMLINE Testing (with some text variables)\n","With randomly simulated instances and features to test specific data challenges that may be encountered.\n","* Instances with a missing class label\n","* Instances with a high percent of missingness\n","* Simulate numerically encoded categorical features (with 2, 3,or 4 state values)\n","* Simulate text-value categorical features (with 2, 3,or 4 state values)\n","* Simulate quantiative features with high missingness\n","* Simulate pairs of features with high correlations between them\n","  * Both positive and negative correlations"],"metadata":{"id":"f8vlWxqWdPUm"}},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","\n","random_seed = 42\n","np.random.seed(seed=random_seed)\n","\n","class_label = 'Class'\n","instance_ID_label = 'InstanceID'\n","features_to_remove = ['Gender','Age at diagnosis']\n","cat_feature_list = [2,3,4]\n","cat_feature_list_text = [2,3,4]\n","miss_feature_list = [0.6,0.7]\n","corr_feature_list = [-1.0,0.9,1.0]\n","num_nolabel_instances = 2\n","miss_instance_list = [0.7,0.8]\n","\n","# Specify the path to your existing CSV dataset\n","csv_file = \"/content/hcc-data_example.csv\"\n","\n","def remove_features(data, features_to_remove):\n","    return data.drop(features_to_remove, axis=1)\n","\n","def generate_categorical_feature(num_categories, num_rows):\n","    categories = [f\"Category {i+1}\" for i in range(num_categories)]\n","    return np.random.choice(categories, size=num_rows)\n","\n","def generate_categorical_feature_numerical_encode(num_categories, num_rows):\n","    categories = np.arange(1, num_categories + 1)\n","    return np.random.choice(categories, size=num_rows)\n","\n","def generate_quantitative_feature(num_rows, missing_percentage):\n","    data = np.random.rand(num_rows)\n","    missing_mask = np.random.choice([False, True], size=num_rows, p=[1-missing_percentage, missing_percentage])\n","    data[missing_mask] = np.nan\n","    return data\n","\n","def generate_correlated_values(feature1, correlation):\n","    # Generate the second feature correlated with the first feature\n","    feature2 = correlation * feature1 + np.random.normal(0, np.sqrt(1 - correlation**2), len(feature1))\n","    return feature2\n","\n","# Find the last occurrence of the directory separator '/'\n","last_separator_index = csv_file.rfind('/')\n","\n","# Extract the file path\n","file_path = csv_file[:last_separator_index] if last_separator_index != -1 else ''\n","\n","# Find the extension separator '.'\n","extension_separator_index = csv_file.rfind('.')\n","\n","# Extract the file name without the extension\n","file_name = csv_file[last_separator_index + 1 : extension_separator_index] if last_separator_index != -1 else csv_file[:extension_separator_index]\n","\n","# Print the extracted file path and file name\n","print(\"File Path:\", file_path)\n","print(\"File Name:\", file_name)\n","\n","# Load the existing CSV dataset\n","data = pd.read_csv(csv_file)\n","\n","# Remove specified features from original dataset\n","data = remove_features(data, features_to_remove)\n","\n","# Generate random instances resembling existing dataset and add to the dataset that have missing class label\n","i = 0\n","for _ in range(num_nolabel_instances):\n","    random_instance = data.sample(n=1, replace=True)\n","    random_instance[class_label] = np.nan\n","    random_instance[instance_ID_label] = 'no_class_'+str(i)\n","    data = pd.concat([data, random_instance], ignore_index=True)\n","    i += 1\n","\n","# Generate random instances resembling existing dataset but have some percentage of missingness\n","i = 0\n","for miss in miss_instance_list:\n","    random_instance = data.sample(n=1, replace=True)\n","    num_features = len(data.columns)\n","    num_missing_values = int(num_features * miss)\n","    random_features = np.random.choice(data.columns, size=num_missing_values, replace=False)\n","    random_instance[random_features] = np.nan\n","    random_instance[instance_ID_label] = 'miss_'+str(i)+'_'+str(miss)\n","    # Randomly choose a value of 0 or 1\n","    value = np.random.choice([0, 1])\n","    random_instance[class_label] = value\n","    data = pd.concat([data, random_instance], ignore_index=True)\n","    i += 1\n","\n","# Simulate the categorical feature and add it to the dataset\n","num_rows = len(data)\n","for cat in cat_feature_list:\n","    simulated_categorical_feature = generate_categorical_feature_numerical_encode(cat, num_rows)\n","    data['Sim_Cat_'+str(cat)] = simulated_categorical_feature\n","\n","# Simulate the text-based categorical feature and add it to the dataset\n","num_rows = len(data)\n","for cat in cat_feature_list_text:\n","    simulated_categorical_feature = generate_categorical_feature(cat, num_rows)\n","    data['Sim_Text_Cat_'+str(cat)] = simulated_categorical_feature\n","\n","# Simulate the quantitative feature and add it to the dataset\n","for miss in miss_feature_list:\n","    simulated_quant_feature = generate_quantitative_feature(num_rows, miss)\n","    data['Sim_Miss_'+str(miss)] = simulated_quant_feature\n","\n","# Simulate the correlated variables and add them to the dataset\n","for corr in corr_feature_list:\n","    # Generate a new random quantitative feature\n","    new_feature = np.random.rand(len(data))\n","\n","    # Generate the correlated feature based on the new feature\n","    correlated_feature = generate_correlated_values(new_feature, corr)\n","\n","    # Add the new features to the dataset\n","    data['Sim_Cor_'+str(corr)+'_A'] = new_feature\n","    data['Sim_Cor_'+str(corr)+'_B'] = correlated_feature\n","\n","# Save the updated dataset with the simulated features\n","output_file = file_path+'/'+file_name+'_custom.csv'\n","data.to_csv(output_file, index=False)\n","print(f\"Updated dataset saved to {output_file}.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fau-KvVEdPi9","executionInfo":{"status":"ok","timestamp":1685747659893,"user_tz":420,"elapsed":2,"user":{"displayName":"ryan urbanowicz","userId":"09525282221743790591"}},"outputId":"36c91551-821f-49a2-8480-592d31dd5dd5"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["File Path: /content\n","File Name: hcc-data_example\n","Updated dataset saved to /content/hcc-data_example_custom.csv.\n"]}]},{"cell_type":"markdown","source":["# Create replication dataset for the custom HCC dataset\n","\n"],"metadata":{"id":"zf8yWymwdPrh"}},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","\n","random_seed = 42\n","np.random.seed(seed=random_seed)\n","\n","# Specify the path to your CSV dataset\n","csv_file = \"/content/hcc-data_example_custom.csv\"\n","\n","column_to_exclude = 'InstanceID'\n","\n","num_sim_instances = 4 #Number of made up instances generated in original HCC dataset (at end of the dataset) -these will not be randomized to preserve them as examples of data challenges\n","\n","# Specify the proportion of instances to replace (between 0 and 1)\n","replace_proportion = 0.3\n","\n","# Find the last occurrence of the directory separator '/'\n","last_separator_index = csv_file.rfind('/')\n","\n","# Extract the file path\n","file_path = csv_file[:last_separator_index] if last_separator_index != -1 else ''\n","\n","# Find the extension separator '.'\n","extension_separator_index = csv_file.rfind('.')\n","\n","# Extract the file name without the extension\n","file_name = csv_file[last_separator_index + 1 : extension_separator_index] if last_separator_index != -1 else csv_file[:extension_separator_index]\n","\n","# Print the extracted file path and file name\n","print(\"File Path:\", file_path)\n","print(\"File Name:\", file_name)\n","\n","# Load the CSV dataset\n","data = pd.read_csv(csv_file)\n","\n","# Select random instances to replace\n","replace_indices = np.random.choice(len(data)-num_sim_instances, size=int(len(data) * replace_proportion), replace=False)\n","\n","# Iterate over the selected indices and replace feature values\n","for index in replace_indices:\n","    # Get the values of the current instance\n","    instance_values = data.iloc[index, :]\n","\n","    # Iterate over the features\n","    for feature in instance_values.index:\n","        # Check if the current feature is the one to exclude\n","        if feature == column_to_exclude:\n","            # Change the value of the excluded feature\n","            data.at[index, feature] = str(instance_values[feature]) + \"_random\"\n","        else:\n","            # Compute the distribution of the feature values in the rest of the dataset\n","            feature_distribution = data[data.index != index][feature]\n","\n","            # Generate a new feature value that resembles the rest of the dataset\n","            new_value = np.random.choice(feature_distribution)\n","\n","            # Assign the new feature value to the current instance\n","            data.at[index, feature] = new_value\n","\n","# Generate random instance that includes a new categorical value in binary text categorical feature\n","random_instance = data.sample(n=1, replace=True)\n","# Randomly choose a value of 0 or 1\n","value = np.random.choice([0, 1])\n","random_instance[class_label] = value\n","random_instance[instance_ID_label] = 'new_val_cat_text_binary'\n","random_instance['Sim_Text_Cat_2'] = 'Category 5' # assign new value \n","data = pd.concat([data, random_instance], ignore_index=True)\n","\n","# Generate random instance that includes a new categorical value in 3-value text categorical feature\n","random_instance = data.sample(n=1, replace=True)\n","# Randomly choose a value of 0 or 1\n","value = np.random.choice([0, 1])\n","random_instance[class_label] = value\n","random_instance[instance_ID_label] = 'new_val_cat_text_3'\n","random_instance['Sim_Text_Cat_3'] = 'Category 7' # assign new value \n","data = pd.concat([data, random_instance], ignore_index=True)\n","\n","# Generate random instance that includes a new categorical value in binary categorical feature\n","random_instance = data.sample(n=1, replace=True)\n","# Randomly choose a value of 0 or 1\n","value = np.random.choice([0, 1])\n","random_instance[class_label] = value\n","random_instance[instance_ID_label] = 'new_val_cat_binary'\n","random_instance['Sim_Cat_2'] = 7 # assign new value \n","data = pd.concat([data, random_instance], ignore_index=True)\n","\n","# Generate random instance that includes a new categorical value in 3-value categorical feature\n","random_instance = data.sample(n=1, replace=True)\n","# Randomly choose a value of 0 or 1\n","value = np.random.choice([0, 1])\n","random_instance[class_label] = value\n","random_instance[instance_ID_label] = 'new_val_cat_3'\n","random_instance['Sim_Cat_3'] = 16 # assign new value \n","data = pd.concat([data, random_instance], ignore_index=True)\n","\n","# Save the updated dataset with the simulated features\n","output_file = file_path+'/'+file_name+'_rep.csv'\n","data.to_csv(output_file, index=False)\n","print(f\"Updated dataset saved to {output_file}.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eS2x07BbdP2G","executionInfo":{"status":"ok","timestamp":1685747660644,"user_tz":420,"elapsed":752,"user":{"displayName":"ryan urbanowicz","userId":"09525282221743790591"}},"outputId":"30b50590-e9a1-4af9-877f-6ac3a520c44b"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["File Path: /content\n","File Name: hcc-data_example_custom\n","Updated dataset saved to /content/hcc-data_example_custom_rep.csv.\n"]}]}]}