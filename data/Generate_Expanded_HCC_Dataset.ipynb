{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OfstZAEElnPB"
   },
   "source": [
    "# Create simulated replication data for original HCC dataset\n",
    "Takes the original HCC dataset and then for a user defined proportion of instances, generates new values for each feature and class outcome based on the distribution of values for that respective column in the data. This effectively adds some noise to the dataset so that the replication data is different from the original HCC dataset for the demonstration of the replication phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6531,
     "status": "ok",
     "timestamp": 1691164531262,
     "user": {
      "displayName": "ryan urbanowicz",
      "userId": "09525282221743790591"
     },
     "user_tz": 420
    },
    "id": "haDZyxwslmgw",
    "outputId": "0b5ac11c-c3f2-4a5c-a453-1a6f20d798c4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File Path: ./DemoData\n",
      "File Name: hcc_data\n",
      "Updated dataset saved to ./OtherData/hcc_data_rep.csv.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "random_seed = 42\n",
    "np.random.seed(seed=random_seed)\n",
    "\n",
    "# Specify the path to your CSV dataset\n",
    "csv_file = \"./DemoData/hcc_data.csv\"\n",
    "\n",
    "column_to_exclude = 'InstanceID'\n",
    "\n",
    "# Specify the proportion of instances to replace (between 0 and 1)\n",
    "replace_proportion = 0.3\n",
    "\n",
    "# Find the last occurrence of the directory separator '/'\n",
    "last_separator_index = csv_file.rfind('/')\n",
    "\n",
    "# Extract the file path\n",
    "file_path = csv_file[:last_separator_index] if last_separator_index != -1 else ''\n",
    "\n",
    "# Find the extension separator '.'\n",
    "extension_separator_index = csv_file.rfind('.')\n",
    "\n",
    "# Extract the file name without the extension\n",
    "file_name = csv_file[last_separator_index + 1 : extension_separator_index] if last_separator_index != -1 else csv_file[:extension_separator_index]\n",
    "\n",
    "# Print the extracted file path and file name\n",
    "print(\"File Path:\", file_path)\n",
    "print(\"File Name:\", file_name)\n",
    "\n",
    "# Load the CSV dataset\n",
    "data = pd.read_csv(csv_file)\n",
    "\n",
    "# Select random instances to replace\n",
    "replace_indices = np.random.choice(len(data), size=int(len(data) * replace_proportion), replace=False)\n",
    "\n",
    "# Iterate over the selected indices and replace feature values\n",
    "for index in replace_indices:\n",
    "    # Get the values of the current instance\n",
    "    instance_values = data.iloc[index, :]\n",
    "\n",
    "    # Iterate over the features\n",
    "    for feature in instance_values.index:\n",
    "        # Check if the current feature is the one to exclude\n",
    "        if feature == column_to_exclude:\n",
    "            # Change the value of the excluded feature\n",
    "            data.at[index, feature] = str(instance_values[feature]) + \"_random\"\n",
    "        else:\n",
    "            # Compute the distribution of the feature values in the rest of the dataset\n",
    "            feature_distribution = data[data.index != index][feature]\n",
    "\n",
    "            # Generate a new feature value that resembles the rest of the dataset\n",
    "            new_value = np.random.choice(feature_distribution)\n",
    "\n",
    "            # Assign the new feature value to the current instance\n",
    "            data.at[index, feature] = new_value\n",
    "\n",
    "# Save the updated dataset with the simulated features\n",
    "output_file = './OtherData/'+file_name+'_rep.csv'\n",
    "data.to_csv(output_file, index=False)\n",
    "print(f\"Updated dataset saved to {output_file}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f8vlWxqWdPUm"
   },
   "source": [
    "# Create Custom HCC dataset for STREAMLINE Testing (with some text variables)\n",
    "Starting with the original HCC dataset, we make custom modifications to that dataset to explicitly test different data challenges and edge case scenarios that might exist in user loaded data.\n",
    "\n",
    "These include:\n",
    "* Removal of covariate features (i.e. gender and age at diagnosis)\n",
    "* Instances with a missing class label\n",
    "* Instances with a high percent of missingness\n",
    "* Simulate numerically encoded categorical features (with 2, 3,or 4 state values)\n",
    "* Simulate text-value categorical features (with 2, 3,or 4 state values)\n",
    "* Simulate quantiative features with high missingness\n",
    "* Simulate pairs of features with high correlations between them\n",
    "  * Both positive and negative correlations\n",
    "* Add invariant features with (1) all one value, (2) all missing values, or (3) a mix of one value and missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 292,
     "status": "ok",
     "timestamp": 1691164531552,
     "user": {
      "displayName": "ryan urbanowicz",
      "userId": "09525282221743790591"
     },
     "user_tz": 420
    },
    "id": "fau-KvVEdPi9",
    "outputId": "83317cd7-3e0e-4698-90c0-d333097c9374"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File Path: ./DemoData\n",
      "File Name: hcc_data\n",
      "Updated dataset saved to ./DemoData/hcc_data_custom.csv.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "random_seed = 42\n",
    "np.random.seed(seed=random_seed)\n",
    "\n",
    "class_label = 'Class'\n",
    "instance_ID_label = 'InstanceID'\n",
    "features_to_remove = ['Gender','Age at diagnosis']\n",
    "cat_feature_list = [2,3,4]\n",
    "cat_feature_list_text = [2,3,4]\n",
    "miss_feature_list = [0.6,0.7]\n",
    "corr_feature_list = [-1.0,0.9,1.0]\n",
    "num_nolabel_instances = 2\n",
    "miss_instance_list = [0.7,0.8]\n",
    "\n",
    "# Specify the path to your existing CSV dataset\n",
    "csv_file = \"./DemoData/hcc_data.csv\"\n",
    "\n",
    "def remove_features(data, features_to_remove):\n",
    "    return data.drop(features_to_remove, axis=1)\n",
    "\n",
    "def generate_categorical_feature(num_categories, num_rows):\n",
    "    categories = [f\"Category {i+1}\" for i in range(num_categories)]\n",
    "    return np.random.choice(categories, size=num_rows)\n",
    "\n",
    "def generate_categorical_feature_numerical_encode(num_categories, num_rows):\n",
    "    categories = np.arange(1, num_categories + 1)\n",
    "    return np.random.choice(categories, size=num_rows)\n",
    "\n",
    "def generate_quantitative_feature(num_rows, missing_percentage):\n",
    "    data = np.random.rand(num_rows)\n",
    "    missing_mask = np.random.choice([False, True], size=num_rows, p=[1-missing_percentage, missing_percentage])\n",
    "    data[missing_mask] = np.nan\n",
    "    return data\n",
    "\n",
    "def generate_correlated_values(feature1, correlation):\n",
    "    # Generate the second feature correlated with the first feature\n",
    "    feature2 = correlation * feature1 + np.random.normal(0, np.sqrt(1 - correlation**2), len(feature1))\n",
    "    return feature2\n",
    "\n",
    "# Find the last occurrence of the directory separator '/'\n",
    "last_separator_index = csv_file.rfind('/')\n",
    "\n",
    "# Extract the file path\n",
    "file_path = csv_file[:last_separator_index] if last_separator_index != -1 else ''\n",
    "\n",
    "# Find the extension separator '.'\n",
    "extension_separator_index = csv_file.rfind('.')\n",
    "\n",
    "# Extract the file name without the extension\n",
    "file_name = csv_file[last_separator_index + 1 : extension_separator_index] if last_separator_index != -1 else csv_file[:extension_separator_index]\n",
    "\n",
    "# Print the extracted file path and file name\n",
    "print(\"File Path:\", file_path)\n",
    "print(\"File Name:\", file_name)\n",
    "\n",
    "# Load the existing CSV dataset\n",
    "data = pd.read_csv(csv_file)\n",
    "\n",
    "# Remove specified features from original dataset\n",
    "data = remove_features(data, features_to_remove)\n",
    "\n",
    "# Generate random instances resembling existing dataset and add to the dataset that have missing class label\n",
    "i = 0\n",
    "for _ in range(num_nolabel_instances):\n",
    "    random_instance = data.sample(n=1, replace=True)\n",
    "    random_instance[class_label] = np.nan\n",
    "    random_instance[instance_ID_label] = 'no_class_'+str(i)\n",
    "    data = pd.concat([data, random_instance], ignore_index=True)\n",
    "    i += 1\n",
    "\n",
    "# Generate random instances resembling existing dataset but have some percentage of missingness\n",
    "i = 0\n",
    "for miss in miss_instance_list:\n",
    "    random_instance = data.sample(n=1, replace=True)\n",
    "    num_features = len(data.columns)\n",
    "    num_missing_values = int(num_features * miss)\n",
    "    random_features = np.random.choice(data.columns, size=num_missing_values, replace=False)\n",
    "    random_instance[random_features] = np.nan\n",
    "    random_instance[instance_ID_label] = 'miss_'+str(i)+'_'+str(miss)\n",
    "    # Randomly choose a value of 0 or 1\n",
    "    value = np.random.choice([0, 1])\n",
    "    random_instance[class_label] = value\n",
    "    data = pd.concat([data, random_instance], ignore_index=True)\n",
    "    i += 1\n",
    "\n",
    "# Simulate the categorical feature and add it to the dataset\n",
    "num_rows = len(data)\n",
    "for cat in cat_feature_list:\n",
    "    simulated_categorical_feature = generate_categorical_feature_numerical_encode(cat, num_rows)\n",
    "    data['Sim_Cat_'+str(cat)] = simulated_categorical_feature\n",
    "\n",
    "# Simulate the text-based categorical feature and add it to the dataset\n",
    "num_rows = len(data)\n",
    "for cat in cat_feature_list_text:\n",
    "    simulated_categorical_feature = generate_categorical_feature(cat, num_rows)\n",
    "    data['Sim_Text_Cat_'+str(cat)] = simulated_categorical_feature\n",
    "\n",
    "# Simulate the quantitative feature and add it to the dataset\n",
    "for miss in miss_feature_list:\n",
    "    simulated_quant_feature = generate_quantitative_feature(num_rows, miss)\n",
    "    data['Sim_Miss_'+str(miss)] = simulated_quant_feature\n",
    "\n",
    "# Simulate the correlated variables and add them to the dataset\n",
    "for corr in corr_feature_list:\n",
    "    # Generate a new random quantitative feature\n",
    "    new_feature = np.random.rand(len(data))\n",
    "\n",
    "    # Generate the correlated feature based on the new feature\n",
    "    correlated_feature = generate_correlated_values(new_feature, corr)\n",
    "\n",
    "    # Add the new features to the dataset\n",
    "    data['Sim_Cor_'+str(corr)+'_A'] = new_feature\n",
    "    data['Sim_Cor_'+str(corr)+'_B'] = correlated_feature\n",
    "\n",
    "#Simulate invariant features\n",
    "data['Invariant_Val'] = 42\n",
    "data['Invariant_NA'] = np.nan\n",
    "num_missing = int(len(data) * 0.1)\n",
    "new_column_values = [43] * (len(data) - num_missing) + [np.nan] * num_missing\n",
    "np.random.shuffle(new_column_values)\n",
    "data['Invariant_Val_NA'] = new_column_values\n",
    "\n",
    "# Shuffle the array to randomize the placement of NaN values\n",
    "np.random.shuffle(new_column_values)\n",
    "\n",
    "# Save the updated dataset with the simulated features\n",
    "output_file = './DemoData/'+file_name+'_custom.csv'\n",
    "data.to_csv(output_file, index=False)\n",
    "print(f\"Updated dataset saved to {output_file}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zf8yWymwdPrh"
   },
   "source": [
    "# Create replication dataset for the custom HCC dataset\n",
    "Takes the original 'Custom HCC dataset' (generated above) and then for a user defined proportion of instances, generates new values for each feature and class outcome based on the distribution of values for that respective column in the data. This effectively adds some noise to the dataset so that the replication data is different from the original HCC dataset for the demonstration of the replication phase.\n",
    "\n",
    "Furthermore, we make some additional custom modifications to this dataset to test aspects of the STREAMLINE replication phase. These include:\n",
    "\n",
    "* A random instance that includes a new categorical value for a binary text categorical feature\n",
    "* A random instance that includes a new categorical value for a 3-value text categorical feature\n",
    "* A random instance that includes a new categorical value for a binary categorical feature\n",
    "*  A random instance that includes a new categorical value in 3-value categorical feature\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6250,
     "status": "ok",
     "timestamp": 1691164537800,
     "user": {
      "displayName": "ryan urbanowicz",
      "userId": "09525282221743790591"
     },
     "user_tz": 420
    },
    "id": "eS2x07BbdP2G",
    "outputId": "ce4e87ea-dcc0-45ba-ce09-52f269c5b02e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File Path: ./DemoData\n",
      "File Name: hcc_data_custom\n",
      "Updated dataset saved to ./DemoRepData/hcc_data_custom_rep.csv.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "random_seed = 42\n",
    "np.random.seed(seed=random_seed)\n",
    "\n",
    "# Specify the path to your CSV dataset\n",
    "csv_file = \"./DemoData/hcc_data_custom.csv\"\n",
    "\n",
    "column_to_exclude = 'InstanceID'\n",
    "\n",
    "num_sim_instances = 4 #Number of made up instances generated in original HCC dataset (at end of the dataset) -these will not be randomized to preserve them as examples of data challenges\n",
    "\n",
    "# Specify the proportion of instances to replace (between 0 and 1)\n",
    "replace_proportion = 0.3\n",
    "\n",
    "# Find the last occurrence of the directory separator '/'\n",
    "last_separator_index = csv_file.rfind('/')\n",
    "\n",
    "# Extract the file path\n",
    "file_path = csv_file[:last_separator_index] if last_separator_index != -1 else ''\n",
    "\n",
    "# Find the extension separator '.'\n",
    "extension_separator_index = csv_file.rfind('.')\n",
    "\n",
    "# Extract the file name without the extension\n",
    "file_name = csv_file[last_separator_index + 1 : extension_separator_index] if last_separator_index != -1 else csv_file[:extension_separator_index]\n",
    "\n",
    "# Print the extracted file path and file name\n",
    "print(\"File Path:\", file_path)\n",
    "print(\"File Name:\", file_name)\n",
    "\n",
    "# Load the CSV dataset\n",
    "data = pd.read_csv(csv_file)\n",
    "\n",
    "# Select random instances to replace\n",
    "replace_indices = np.random.choice(len(data)-num_sim_instances, size=int(len(data) * replace_proportion), replace=False)\n",
    "\n",
    "# Iterate over the selected indices and replace feature values\n",
    "for index in replace_indices:\n",
    "    # Get the values of the current instance\n",
    "    instance_values = data.iloc[index, :]\n",
    "\n",
    "    # Iterate over the features\n",
    "    for feature in instance_values.index:\n",
    "        # Check if the current feature is the one to exclude\n",
    "        if feature == column_to_exclude:\n",
    "            # Change the value of the excluded feature\n",
    "            data.at[index, feature] = str(instance_values[feature]) + \"_random\"\n",
    "        else:\n",
    "            # Compute the distribution of the feature values in the rest of the dataset\n",
    "            feature_distribution = data[data.index != index][feature]\n",
    "\n",
    "            # Generate a new feature value that resembles the rest of the dataset\n",
    "            new_value = np.random.choice(feature_distribution)\n",
    "\n",
    "            # Assign the new feature value to the current instance\n",
    "            data.at[index, feature] = new_value\n",
    "\n",
    "# Generate random instance that includes a new categorical value in binary text categorical feature\n",
    "random_instance = data.sample(n=1, replace=True)\n",
    "# Randomly choose a value of 0 or 1\n",
    "value = np.random.choice([0, 1])\n",
    "random_instance[class_label] = value\n",
    "random_instance[instance_ID_label] = 'new_val_cat_text_binary'\n",
    "random_instance['Sim_Text_Cat_2'] = 'Category 5' # assign new value\n",
    "data = pd.concat([data, random_instance], ignore_index=True)\n",
    "\n",
    "# Generate random instance that includes a new categorical value in 3-value text categorical feature\n",
    "random_instance = data.sample(n=1, replace=True)\n",
    "# Randomly choose a value of 0 or 1\n",
    "value = np.random.choice([0, 1])\n",
    "random_instance[class_label] = value\n",
    "random_instance[instance_ID_label] = 'new_val_cat_text_3'\n",
    "random_instance['Sim_Text_Cat_3'] = 'Category 7' # assign new value\n",
    "data = pd.concat([data, random_instance], ignore_index=True)\n",
    "\n",
    "# Generate random instance that includes a new categorical value in binary categorical feature\n",
    "random_instance = data.sample(n=1, replace=True)\n",
    "# Randomly choose a value of 0 or 1\n",
    "value = np.random.choice([0, 1])\n",
    "random_instance[class_label] = value\n",
    "random_instance[instance_ID_label] = 'new_val_cat_binary'\n",
    "random_instance['Sim_Cat_2'] = 7 # assign new value\n",
    "data = pd.concat([data, random_instance], ignore_index=True)\n",
    "\n",
    "# Generate random instance that includes a new categorical value in 3-value categorical feature\n",
    "random_instance = data.sample(n=1, replace=True)\n",
    "# Randomly choose a value of 0 or 1\n",
    "value = np.random.choice([0, 1])\n",
    "random_instance[class_label] = value\n",
    "random_instance[instance_ID_label] = 'new_val_cat_3'\n",
    "random_instance['Sim_Cat_3'] = 16 # assign new value\n",
    "data = pd.concat([data, random_instance], ignore_index=True)\n",
    "\n",
    "#Add exceptions to the invariant data columns\n",
    "data.at[1, 'Invariant_Val'] = 41\n",
    "data.at[2, 'Invariant_NA'] = 41\n",
    "data.at[3, 'Invariant_Val_NA'] = 41\n",
    "\n",
    "# Save the updated dataset with the simulated features\n",
    "output_file = './DemoRepData/'+file_name+'_rep.csv'\n",
    "data.to_csv(output_file, index=False)\n",
    "print(f\"Updated dataset saved to {output_file}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNtiZnKEe8on4QGhDImr0Um",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
